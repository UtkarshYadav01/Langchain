{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion- Documentloaders\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T17:50:06.761899Z",
     "start_time": "2025-11-17T17:50:06.741884Z"
    }
   },
   "source": [
    "## Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader('speech.txt')\n",
    "loader\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1fb9ec8df90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T14:09:55.807344Z",
     "start_time": "2025-11-17T14:09:55.795347Z"
    }
   },
   "source": [
    "text_documents=loader.load()\n",
    "text_documents"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T14:10:05.503157Z",
     "start_time": "2025-11-17T14:10:04.000622Z"
    }
   },
   "source": [
    "## Reading a PDf File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('attention.pdf')\n",
    "docs=loader.load()\n",
    "docs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-08-03T00:07:29+00:00', 'author': '', 'keywords': '', 'moddate': '2023-08-03T00:07:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T14:12:20.536835Z",
     "start_time": "2025-11-17T14:12:20.516167Z"
    }
   },
   "source": [
    "type(docs[0])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T17:33:50.569251Z",
     "start_time": "2025-11-17T17:33:50.211173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Reading a Microsoft Word File\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "loader = Docx2txtLoader(\"sample-rfp.docx\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sample-rfp.docx'}, page_content='_\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\tREQUEST FOR PROPOSALS      \\n\\n(RFP)\\n\\n\\t\\t\\n\\nISSUE DATE:\\t\\t\\tAugust 3, 2018\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTITLE:\\t\\t\\t\\t\\t Administrative Services and Fully Insured Health Benefits Plans\\n\\n\\n\\nNumber: \\t\\tOHB19-01\\n\\n\\n\\nISSUING AGENCY:\\t\\tCommonwealth of Virginia\\n\\nDepartment of Human Resource Management\\n\\nJames Monroe Building, 13th Floor\\n\\n101 North 14th Street\\n\\nRichmond, Virginia 23219\\n\\n\\n\\nPERIOD OF CONTRACT:\\tFrom July 1, 2019 through June 30, 2024, with five one-year renewal options as described within.\\n\\n\\n\\nSealed proposals for furnishing services described herein will be received subject to the conditions cited herein until 2:00 p.m., September 18, 2018.\\n\\n\\n\\nAll Inquiries Must Be In Writing (the cut-off date for all questions is September 1, 2018) and Should Be Directed To:\\n\\n\\n\\nMr. Richard Whitfield\\n\\nDepartment of Human Resource Management\\n\\nJames Monroe Building, 13th Floor\\n\\n101 North 14th Street\\n\\nRichmond, Virginia 23219\\n\\ne-mail:  richard.whitfield@dhrm.virginia.gov\\n\\n\\n\\nSEND ALL PROPOSALS DIRECTLY TO THE ISSUING AGENCY ADDRESS SHOWN ABOVE.\\n\\n\\n\\nNote:  This public body does not discriminate against faith-based organizations in accordance with the Code of Virginia, § 2.2-4343.1 or against a bidder or Offeror because of race, religion, color, sex, national origin, age, disability, or any other basis prohibited by state law relating to discrimination in employment.\\n\\n\\n\\nIn compliance with this Request for Proposals, and to all the conditions imposed therein and hereby incorporated by reference, the undersigned offers and agrees to furnish materials and services in accordance with the attached signed proposal or as mutually agreed upon by subsequent negotiation.\\n\\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\\tName and Address of Firm:\\n\\n\\t____________________________________________\\tDate:   ________________________________________\\n\\n\\t____________________________________________\\tBy:      ________________________________________\\n\\n\\t____________________________________________\\t              (Signature In Ink)\\n\\n\\t__________________________Zip Code:___________\\tName:  ________________________________________\\n\\n\\teVA Vendor ID or DUNS #:______________________\\t              (Please Print)\\n\\n\\tFax Number: (___)  ____________________________\\tTitle:  _________________________________________\\n\\n\\tE-mail Address: _______________________________\\tTelephone Number:  (___)_________________________\\n\\n\\n\\nPREPROPOSAL CONFERENCE: A Mandatory preproposal conference will be held on Monday, August 20, 2018 at 10:30 a.m. at the James Monroe Building, Conference Rooms C, D and E, 1st Floor, 101 North 14th Street, Richmond, Virginia.  Any vendor arriving to the conference rooms after 10:30 a.m. will not be admitted (a picture ID is required, and arrival 30 minutes in advance of the meeting is recommended to allow for parking). \\n\\nTABLE OF CONTENTS\\n\\n\\n\\n\\t\\t\\n\\n\\t\\t1.0\\tINTRODUCTION ……………………………………………………………………..\\t3\\n\\n\\t\\t\\n\\n\\t\\t2.0\\tSPECIFICATIONS, TASKS, AND MANDATORY QUALIFICATIONS …….........\\t6\\n\\n\\t\\t\\n\\n\\t\\t3.0\\tSTANDARDS OF PERFORMANCE ……………………………………………….\\t30\\n\\n\\t\\t\\n\\n\\t\\t.4.0\\tREPORTS AND DELIVERABLES …………………………………………............\\t33\\n\\n\\t\\t\\n\\n\\t\\t5.0\\tPROCUREMENT PROCEDURES …………………………………………...........\\t36\\n\\n\\t\\t\\n\\n\\t\\t6.0\\tFORMS OF RESPONSE AND CRITERIA …………………………………..........\\t39\\n\\n\\t\\t\\n\\n\\t\\t7.0\\tGENERAL TERMS AND CONDITIONS ……………………………………..........\\t47\\n\\n\\t\\t\\n\\n\\t\\t8.0\\tSPECIAL TERMS AND CONDITIONS ……………………………………………\\t56\\n\\n\\t\\t\\n\\n\\t\\tEXIBIT ONE – HIPAA PRIVACY BUSINESS ASSOCIATE AGREEMENT ……...........\\t68\\n\\n\\t\\t\\n\\n\\t\\tEXHIBIT TWO – SMALL BUSINESS SUBCONTRACTING PLAN ……………………\\t75\\n\\n\\t\\t\\n\\n\\t\\tLIST OF APPENDICES ……………………………………………………………………\\t77\\n\\n\\t\\t\\n\\n\\t\\t\\tLIST OF ATTACHMENTS …………………………………………………………………\\t90\\n\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nINTRODUCTION\\n\\n\\n\\n1.1\\tPURPOSE\\n\\n\\n\\nThe purpose of this Request for Proposals (RFP) is to secure an administrator(s) for the statewide Commonwealth of Virginia employee health benefits program, The Local Choice (TLC) health benefits program, and the Line of Duty Act (LODA) health benefits plans.  These self-insured programs anticipate relatively few benefit changes to the current plan designs. Successful offerors will be expected to incorporate changes to plan design and program components at any time, including at initial implementation, as required by law or requested by the Department of Human Resource Management (the Department).  Statewide refers to any eligible employee or former employee, regardless of location.  This requires nationwide and, in some cases, international access to coverage.  \\n\\n\\n\\nThe RFP contains six (6) separate components for which individual submissions may be made. These components are:\\n\\n\\n\\nStatewide Preferred Provider Organization (PPO) and High Deductible Health Plan (HDHP) Medical/Surgical, Behavioral Health (to include Employee Assistance Plan (EAP)), Vision, and Hearing administrative services for the state employee, TLC, and LODA plans;\\n\\nStatewide Consumer-Driven Health Plan (CDHP) Medical/Surgical, Behavioral Health (to include EAP), Vision, and Hearing administrative services for the state employee plan;\\n\\nStatewide PPO, HDHP, and CDHP Prescription Drug  administrative services for the state employee, TLC, and LODA plans;\\n\\nStatewide PPO, HDHP, and CDHP Dental administrative services for the state employee, TLC, and LODA plans;\\n\\nFully-insured regional plans for state employee and TLC programs; and\\n\\nSection 125 Flexible Spending Account administration for state employees.\\n\\n \\n\\nAny submission must address all elements of a given component. \\n\\n\\n\\nThe TLC program is an optional health benefits program administered by the Department for political subdivisions of the Commonwealth.  The LODA Health Benefits Plans cover eligible public safety employees/volunteers permanently injured or killed in the line of duty and/or their eligible family members. (The LODA Plans are not exempt from ERISA.)  Please note that Flexible Benefits will not be included in the TLC program or the LODA plans.  \\n\\n\\n\\nThe objectives of the programs include providing better than average benefits administered in a very cost-effective manner with excellent service to enrollees, so that state agencies and participating local jurisdictions can recruit and retain high quality employees.  \\n\\n\\n\\n\\n1.2\\tBACKGROUND\\n\\n\\n\\nThe Department is the authorized agent of the Governor in administering the state employee health benefits program.  The program is delivered through approximately 219 state agencies to some 100,000 active full-time and part-time employees, 10,000 retirees not eligible for Medicare, and 500 extended coverage (COBRA) enrollees, and to the dependents of these enrollees.  Agencies distribute program materials to active employees, assist employees in applying for coverage or changes in coverage according to rules developed by the Department, payroll-deduct employee premiums (with some exceptions), post eligibility information onto the Benefits Eligibility System (BES), and otherwise assist employees in accessing the program’s benefits.  Participants who do not receive pay from which to deduct premiums (e.g., some retirees and COBRA participants) will be billed by the selected Offeror for the Medical/Surgical product.  \\n\\n\\n\\nThe Department also has the responsibility for administering a health benefits program, The Local Choice (TLC), which is offered to localities statewide as a replacement option to other health benefits program choices.  Any local government, school district, or political subdivision may join this program.  Presently there are 440 member groups covering approximately 45,000 employees, retirees and their 32,000 covered dependents.  The Department offers a choice of benefit designs to TLC member groups.  Currently, the choices of plans include PPO, HDHP and HMO plans using a variety of coinsurance, co-payments, and deductibles.\\n\\n \\n\\nThe Department also administers a health benefits program, the LODA Health Benefits Plans, which is described in Section 1.1 above. Presently there are 1,100 participants and their 1,400 covered family members in two (2) non-Medicare LODA plans, one based on current and the other based on former employment. Both of these plans are PPOs. \\n\\n\\n\\nThe Department has developed plans and programs with the advice of consultants, vendors, employees and others, and has delivered benefits through Contractors, either insurers or third party administrators.  The coverages currently available for the state employee and LODA plans may be found on the state employees’ web site: www.dhrm.virginia.gov., The coverages currently available for TLC plans may be found on The Local Choice web site: www.thelocalchoice.virginia.gov.   \\n\\n\\n\\n1.3\\tGENERAL DESCRIPTION\\n\\n\\n\\nThe Department currently offers three statewide self-insured plans for state employees, a PPO (COVA Care), an HDHP (COVA HDHP), and a CDHP (COVA HealthAware). The Department also currently offers a regional fully insured HMO to state employees within the applicable service area.  \\n\\n\\n\\nThe TLC program currently offers four self-insured plans designed around a PPO called Key Advantage.  These include Key Advantage Expanded, Key Advantage 250, Key Advantage 500 and Key Advantage 1000. In addition, a self-insured HDHP and a regional fully-insured HMO are available.  It is anticipated that there will always be a degree of choice in TLC to better meet the needs of the different groups and to ensure the program remains competitive in the marketplace. \\n\\n\\n\\nThe Department wishes to receive offers for the statewide plans on an Administrative Services Only (ASO), self-insured basis. It wishes to receive offers for less than statewide plans on a fully insured basis. However, the Department is under no obligation to implement such fully insured plans.\\n\\n\\t\\n\\nThis RFP is divided into sections, such as this numbered Section 1.0, Introduction.  A section is one of the principal divisions of this RFP.  Within these sections, numbered subsections are the second principal division and normally contain the number of the section in which they are located, such as subsection numbered 1.3.  \\n\\n\\n\\nIt is imperative that Offerors respond to all applicable requirements and complete all applicable schedules and exhibits described in the Form of Response, Section 6.  Any Offeror confusion about which sections/subsections and/or paragraphs may be applicable to a potential Offeror should be clarified no later than the mandatory pre-proposal conference.\\n\\n\\n\\nThis RFP embraces coverage for Medical/Surgical, Behavioral Health including EAP, Dental, Pharmacy, and Flexible Benefits.  For scoring requirements concerning all plans, refer to the Section 6.7 of this RFP.  \\n\\n\\n\\nThis RFP also does not address coverage for Medicare Retiree benefits.   Benefits for the Medicare Retiree Program, including the Medicare Part D benefit, will be procured at a later date for an effective date on or after January 1, 2020, but not later than January 1, 2022.\\n\\n\\n\\n\\t1.4\\tAPPENDICES \\n\\n\\n\\nAppendix 1 is the current standard contract.  Appendix 2 contains selected enrollment, cost, workload, demographic and utilization data for state employees.  Appendix 3 contains a link to websites for current plan information.  Appendix 4 gives information regarding the number of enrollees of TLC local employees covered under non-HMO contracts and current LODA Enrollment.   Appendix 5 contains a description of the state employee eligibility, enrollment and billing system.  Appendix 6 contains a description of the eligibility, enrollment and billing procedures and group renewal process for TLC.  Appendix 7 contains a link to a website providing the EDI payment procedures that are used for the state employee group. Appendix 8 contains selected information about the Flexible Spending Accounts. Appendix 9 contains a proposal checklist.\\n\\n\\n\\nATTACHMENTS\\n\\n\\n\\nAttachment 1 contains a link to benefit descriptions.  Attachment 2 contains critical instructions for the cost schedules and technical questionnaires that must be submitted with a proposal.  In electronic form (see 1.8 below), it also contains claim and eligibility data necessary to prepare a proposal.   Attachment 3 provides report formats.  \\n\\n\\n\\n\\nEXHIBITS \\n\\n\\n\\nExhibit One contains a sample HIPAA Privacy Business Associate Agreement (see subsection 8.23).  Exhibit Two contains the Small Business Participation report that is required to be submitted under paragraph 6.7. \\n\\n\\n\\n\\t1.8 \\tELECTRONIC DATA FILES AND RESPONSE FORMS \\n\\n\\t\\n\\nFiles containing claims, enrollment data and the Attachment 2 schedules that you will need to prepare and submit with a proposal are available in electronic form. To obtain these files, please send email to Brian Dwyer (brian.dwyer@aon.com) with a copy to Richard Whitfield (richard.whitfield@dhrm.virginia.gov) requesting credentials and instructions necessary to download the files from a secure site.\\xa0 \\n\\n\\n\\nPlease note that these files are proprietary and available only to vendors of the services requested by this RFP.\\n\\n\\n\\n\\n\\n\\t2.0   \\tSPECIFICATIONS, TASKS, AND MANDATORY QUALIFICATIONS\\n\\n\\n\\n2.1\\tSTATEWIDE PLANS        \\n\\n\\n\\nThe Department offers a statewide benefit plan for the state employees program,  TLC plans, and the LODA plans.  They are all provided on a self-insured basis.  The plans that are currently offered are described on the web sites provided in Section 1.2 above and encompass a variety of plan designs.  The Department will continue self-insured arrangements and the Offeror(s) must have the ability to administer multiple plans.   The Offeror(s) must be able to assist the Department in changing plan designs during the term of this contract at the request of the Department and/or as required by legislation.\\n\\n\\n\\n\\t2.2\\tLESS THAN STATEWIDE PLANS:  HEALTH MAINTENANCE ORGANIZATIONS (HMOS) AND PPOS\\n\\n\\t\\n\\nAn Offeror may submit a proposal for a less than statewide plan under these conditions:\\n\\n\\n\\n\\t2.2.1 \\tThe plan has a managed care network (HMO; PPO).\\n\\n\\t\\n\\n\\t2.2.2 \\tThe plan is licensed and the proposal covers a contiguous service area.\\n\\n\\t\\n\\n\\t2.2.3 \\tOnly fully insured options will be considered.\\n\\n\\n\\nThe benefit design for a less than statewide plan is up to the Offeror, but should represent a distinctive choice when compared to the statewide PPO option. More than one option may be proposed. \\n\\n\\n\\n2.3\\tPLAN PROVIDER NETWORK\\n\\n\\n\\n\\t2.3.1\\tThe statewide Offeror(s) must offer a statewide network of providers who are expert, practiced, and appropriately credentialed.  The number of providers should permit employees to access the network for services within the standards described in subsection 2.4. In addition, there must be provider access, as demonstrated through geo access reports, for participants who live or travel outside of the state, including abroad.  \\n\\n\\n\\n\\t 2.3.2 \\tThe Offeror(s) must: \\n\\n\\t\\n\\n\\tensure that providers continue to meet the Offeror’s criteria,\\n\\n\\t\\n\\n\\tensure that sufficient liability insurance is maintained,\\n\\n\\t\\n\\n\\tensure that provider contracts continue to remain in force,\\n\\n\\t\\n\\n\\tensure that utilization of services is monitored continually, \\n\\n\\n\\n\\tensure that sufficient (in the Department’s judgment) numbers of credentialed providers are available, \\n\\n\\n\\n\\tencourage providers to support and utilize electronic health records, \\n\\n\\n\\n\\tensure that providers are using available tools to avoid duplication of services and ensure compliance with medical advice, and\\n\\n\\n\\n\\n\\nnot mark any of the health plan’s data with any statement indicating that the data is proprietary, confidential, protected, and/or the property of the Contractor.\\n\\n \\n\\n\\t2.3.3\\tThe Department will consider local networks for less than statewide plans if the networks are properly credentialed.\\n\\n\\t\\n\\n\\t2.3.4\\tThe Contractor must develop and maintain an on-line, real-time directory of participating providers of services, that must be available and easily accessible on the Contractor’s web site to all enrollees and administrators (see subsection 8.15).  Additionally, this on-line directory must be capable of being printed, in a printer-friendly format, by group administrators and enrollees. The Contractor shall have and execute a plan for communicating provider changes to affected enrollees. This should include the ability to identify providers outside of the state.\\n\\n\\t\\n\\n\\t\\tQUALIFICATIONS FOR OFFERORS\\n\\n\\n\\n\\t2.4.1\\tAll network-based plans shall demonstrate that sufficient access is available as demonstrated by the geo-access response in Attachment 2.  \\n\\n\\n\\n\\t\\tAll network-based plans shall annually produce and submit a HEDIS (or department approved substitute), including the standard Member Satisfaction Survey, in accordance with the current requirements.  This report must be submitted by August 15th for the prior plan year and shall include plan- and program-specific data. Please note, that currently, and likely going forward, the state employee, LODA, and most TLC plan years run from July 1 through June 30. Some TLC plan years run from October 1 through September 30. \\n\\n\\t\\n\\n\\t\\tAll network-based plans shall apply for NCQA certification before responding to this RFP.  If rejected, regardless of the reason, the plan(s) shall re-apply at the earliest time permitted by NCQA.\\n\\n\\n\\n\\t\\tTo be awarded a contract, all plans must demonstrate the capability to provide the claims and eligibility files in a format required by the Department.   Such demonstration will consist of submission and approval of a test file in the format provided to finalists.  The timing and other logistics involved with this process will be determined during the proposal evaluation and negotiations. \\n\\n\\t\\n\\n\\t\\tThe network for the statewide plans shall provide access to participating providers outside of the Commonwealth of Virginia where needed by enrollees.\\n\\n\\t\\n\\n\\t\\tThe Medical/Surgical Offeror(s) must also provide vision and hearing coverages as a buy-up to match current benefit design.\\n\\n\\n\\n2.5\\tCLAIMS PROCESSING QUALIFICATIONS FOR OFFERORS\\n\\n\\t\\n\\n\\t2.5.1\\tProcess all claims incurred during the life of this contract.\\n\\n\\t\\n\\n\\t2.5.2  \\tReceive, date, and control claims within 24 hours of the day received.\\n\\n\\t\\n\\n\\t2.5.3 \\tVerify eligibility of claimant and period of coverage for every claim processed.  Pay no claims for any participants who are in a claims-hold status. Pay no claims for TLC employees whose premiums are not currently paid. \\n\\n\\t\\n\\n\\t2.5.4\\tExamine the licensure and participation status of the provider of services.\\n\\n\\t\\n\\n\\t2.5.5 \\tDetermine whether or not the services are covered.\\n\\n\\t\\n\\n\\t2.5.6\\tDetermine that the services provided are medically necessary.  The Department requires that diagnostic codes and provider names are captured with the claim.\\n\\n\\t\\n\\n\\t2.5.7\\tCheck claims history and prevent duplicate payments or payments that exceed contract limits.\\n\\n\\t\\n\\n\\t2.5.8\\t Price the services. \\n\\n\\t\\n\\n\\t2.5.9\\tGenerate and mail a check, as required, and an explanation of benefits (EOB) or denial notice.  The forms of the EOB and denial notice are subject to the Department\\'s approval, however, the Department requires an electronic EOB in lieu of a paper EOB if requested by the participant.  Payments and denial notices must be mailed or generated within five business days of the date on which the claim was processed. \\n\\n\\t\\n\\n\\t2.5.10\\tDeliver a summary paid claim listing to the Department in a form acceptable to the Department every week along with an invoice.  Administrative costs are to be billed monthly. \\n\\n\\t\\n\\n\\t2.5.11\\tMaintain a bank account for paying claims. Reconcile the account and credit interest to the Department when interest on the float exceeds banking charges.  The amount of interest will be determined by mutual agreement between Offeror and the Department. \\n\\n\\t\\n\\n\\t\\tMaintain a history of all claims paid.  Not less than 18 months of claims history               prior to the current calendar year shall be maintained on line.\\n\\n\\t\\n\\n\\t\\tProvide Department’s Consultant an electronic file of claims that support the most recent bill – usually weekly.\\xa0 Format will be provided to the finalists to produce a test file as part of the Offeror selection process as described in 2.8.5. \\n\\n\\t\\n\\n\\t\\tProvide, on a schedule to be determined, an electronic claim file to a designated data warehouse.\\n\\n\\t\\n\\n\\t\\tThe successful Offeror(s) for the Medical/Surgical product for Components one, two and five will be responsible for developing a consolidated Summary of Benefit Coverage (SBC) for each available benefit option. This means that, should the Department select multiple Offerors to administer the PPO (for example, one Offeror for Medical/Surgical, Behavioral Health, and Vision/Hearing; another for Prescription Drug; and another for Dental), it will be the responsibility of the Medical/Surgical Offeror to work with all successful Offerors to gather and format information into a consolidated SBC for each option.\\n\\n\\n\\n\\t\\tThe Commonwealth of Virginia enacted legislation in April, 2012, that created the Virginia All-Payer Claims Database (APCD).\\xa0 The Medical/Surgical/Behavioral Health, Pharmacy, and Dental Offerors are required to submit claims in the specified format and at the specified frequency.\\n\\n\\t\\t\\n\\n2.6     \\tQUALIFICATIONS FOR OFFERORS REGARDING PLAN PARTICIPANT INQUIRIES\\n\\n\\n\\n\\t2.6.1 \\tRespond correctly and timely to inquiries received by telephone, mail, e-mail, or in person.\\n\\n\\n\\n2.7\\tBENEFITS ADMINISTRATION QUALIFICATIONS FOR OFFERORS\\n\\n\\n\\n\\tThe Offeror shall check the eligibility of claimants against the Department’s eligibility files before authorizing benefits.  . \\n\\n\\t\\n\\n\\tThe Offeror shall develop employee communication materials that fully and accurately describe, including any companion carve-out benefits:\\n\\n\\t\\n\\n\\tthe benefits of the program,\\n\\n\\t\\t\\n\\n\\thow the program works,\\n\\n\\t\\t\\n\\n\\twhere, how, and when additional information can be obtained,\\n\\n\\t\\t\\n\\n\\thow to access benefits,\\n\\n\\t\\t\\n\\n\\twhat to do in an emergency,\\n\\n\\t\\n\\n\\thow to appeal the determination of the Offeror with respect to a denial of benefits for any reason, \\n\\n\\t\\n\\n\\temployee assistance services available,\\n\\n\\t\\n\\n\\tsuch other information as would be required to meet the standards of a summary plan description as that term is defined in the Employee Retirement Income Security Act (ERISA), \\n\\n\\t\\n\\n\\tdevelop communications materials as needed to focus on specific program components as determined by the Department, and\\n\\n\\t\\n\\n\\tany communications required to comply with state or federal legislation.\\n\\n\\t\\t\\n\\n2.7.3\\tProvide a legal defense against all claims arising out of this contract.\\n\\n\\n\\n2.7.4  \\tHold enrollees and covered dependents harmless with respect to services covered under this contract when such services are furnished by participating providers.\\n\\n\\n\\n2.7.5\\tShould this RFP result in multiple Contractors, each Contractor shall coordinate as closely as possible with other  Contractors under which the employee is enrolled to integrate customer service, claims processing, disease/case management, data reporting and other services as deemed necessary by the Department.\\n\\n\\n\\n2.7.6\\tParticipate as requested in HR Conferences, benefits fairs and wellness activities coordinated under the CommonHealth program or through the Department.\\n\\n\\n\\n\\t2.8\\tGENERAL MANDATORY QUALIFICATIONS FOR OFFERORS (Please note that in addition to these general mandatory qualifications, there are also mandatory qualifications below in section 2.9 for specific products). The Offeror must have at least one client, with 25,000 employees, requiring similar services as those for which the Offeror is bidding in this RFP, and include it as a reference as directed in applicable Attachment 2 questionnaires. Public sector clients over 50,000 employees are preferred, and all should be included as references.  Additionally, the Offeror must submit transparent administrative cost detail by service component as requested in the Attachment 2 cost exhibits.\\n\\n\\n\\n\\t2.8.1\\tWhere applicable, the Offeror shall demonstrate that the access to participating providers available to employees of the Commonwealth is acceptable to the Department.  \\n\\n\\t\\n\\n\\t2.8.2\\tThe Contractor shall annually produce and submit an approved Member Satisfaction Survey. Additionally, a Contractor operating a network-based plan shall include in this survey a question(s) about network adequacy.  \\n\\n\\t\\n\\n\\t\\tThe5 selected Offeror must offer dedicated and staffed toll-free customer service telephone numbers at least three months before the effective date of the contract. \\n\\n\\t\\n\\n\\t\\tThe Contractor must provide dedicated websites available to state, TLC, and LODA participants that are available for use by April 1, 2019. Failure to meet the April deadline will result in liquidated damages as defined in Section 3.9. If the Offeror is unable to comply with this due date, then the Offeror may offer another solution if it is deemed acceptable by the Department. However, without exception, the Offeror must provide the dedicated websites by July 1, 2019. These websites must be maintained for the duration of the contract.\\n\\n\\t\\n\\n\\t\\tBefore issuing a contract, the Offeror must submit a paid claims test file containing at least 500 claims in a format that will be provided. The Department will evaluate the test file and may require additional submissions until the format is acceptable. PLEASE NOTE:  Standard vendor files might not be acceptable to fulfill this requirement. \\n\\n\\t\\n\\n\\t\\tProposals should state the Offeror’s willingness to accept these standards, and to install the necessary telephone, telecommunications and other systems to ensure adherence to the standards without cost to the Commonwealth. The Contractor must be able to provide the Department with documentation of its performance.\\n\\n\\t\\n\\n\\t\\tThe Offeror shall cooperate with other vendors and designated consultants in data integration activities, and administration of programs requiring multi-vendor participation applicable to specific program (state employee, TLC, and LODA) provisions, as directed by the Department. \\n\\n\\t\\n\\n\\t\\t\\tThe Contractor(s) shall work with the Department and other vendors in the creation and distribution of single, plan-specific ID cards. This means that, at a minimum, the single ID card must include all necessary information related to the Medical/Surgical, Prescription Drug, Behavioral Health, Dental, Vision and Hearing benefits. The common ID card should not include a Social Security number; it should provide the unique, personal ID number generated by the Department.  The selected Offeror for the Medical/Surgical product will be responsible for providing this single ID card.  \\n\\n\\t\\n\\n\\t\\tThe Offeror(s) for components one through four shall be prepared to work with the Department to select a specific set of low value services as determined by the Milliman Health Waste Calculator and design and implement a program to reduce or eliminate waste in these services. \\n\\n\\t\\n\\n\\t\\t\\tThe Contractor(s) shall prepare or assist in preparing reports as requested by the Department, and in the style and format indicated by the Department.  This may include regularly-scheduled reports, including annual reports, and ad hoc reports.\\n\\n\\t\\t\\n\\n\\t\\t\\t\\tThe Contractor shall collaborate with other Contractors to achieve goals established by the Department.\\n\\n\\t\\t\\n\\n\\tThe Contractor shall comply with the following Commonwealth Information Security Standards:\\n\\n\\n\\n\\t\\n\\n\\tHosted Environment Information Security Standard (SEC525-02) (08/11/2016)\\n\\n\\thttps://www.vita.virginia.gov/media/vitavirginiagov/it-governance/psgs/pdf/HostedEnvironmentInformationSecurityStandardSEC52501.pdf\\n\\n\\t\\n\\n\\tIT Risk Management Standard (SEC520-00.1) (12/08/2016)\\n\\n\\t\\thttps://www.vita.virginia.gov/media/vitavirginiagov/it-governance/psgs/pdf/IT_Risk_Management_Standard_SEC520.pdf \\n\\n\\t\\n\\n\\tIT Security Audit Standard (SEC502-02.3) (12/08/2016)\\n\\n\\t\\thttps://www.vita.virginia.gov/media/vitavirginiagov/it-governance/psgs/pdf/IT_Security_Audit_Standard_SEC502.pdf \\n\\n\\t\\n\\n\\tRemoval of Commonwealth Data from Electronic Media Standard (SEC514-04) (12/21/2015) \\n\\n\\t\\thttps://www.vita.virginia.gov/media/vitavirginiagov/it-governance/psgs/pdf/RemovalCOVDataElectMediaStandardSEC51404.pdf \\n\\n\\n\\nAll these security standards can also be found at the following location:\\n\\nhttps://www.vita.virginia.gov/it-governance/itrm-policies-standards/#securityPSGs\\n\\n\\n\\n\\n\\n\\t2.8.13. \\tIf necessary, Offerors shall be prepared for a short implementation period. Each Offeror shall detail how it would handle an implementation period of less than six months.\\n\\n\\n\\n\\t2.9.\\tSPECIFIC MANDATORY QUALIFICATIONS BY COMPONENT (See Section 1.1)\\n\\n\\n\\n\\t2.9.1\\tCOMPONENT NUMBER ONE (Statewide PPO and HDHP Medical/Surgical, Behavioral Health, Vision, and Hearing administrative services for the state employee, TLC and LODA plans) \\n\\n\\t\\n\\n\\t\\tThe benefits to be offered are referenced in Appendix 3 and the employee handbooks on the web sites identified in subsection 1.2.  These booklets describe the contract between the employee and the Department.  To the fullest extent applicable, all the definitions, general rules, descriptions of services, exclusions, basic provisions, definitions, and eligibility rules therein apply to all contracts issued pursuant to this RFP.  \\n\\n\\t\\n\\n\\tThe medical/surgical offeror may choose to subcontract other vendors to propose the required single offering for this component. \\n\\n\\t\\t\\n\\n\\tMEDICAL/SURGICAL PLAN  \\n\\n\\n\\n\\tThe Contractor must administer out-of-network benefits where applicable.\\n\\n\\t\\n\\n\\tThe Contractor shall provide disease management, maternity management and wellness programs as directed by the Department. The Offeror may also propose additional programs, for which the Department shall have sole discretion as to whether to implement.\\n\\n\\t\\n\\n\\t\\tIf requested, the Contractor shall provide an on-site lifestyle coach at Capitol Square Healthcare clinic.  All self-insured plan participants may utilize the services of this coach.    \\n\\n\\t\\n\\n\\tThe Offeror must demonstrate experience in delivering the following services and capabilities:\\n\\n\\t\\t\\n\\nrobust online health information, behavior change programs and support tools,\\n\\n\\n\\nability to efficiently deliver biometric testing via multiple methods such as on-site or through a lab partner, provider form, or home kits,\\n\\n\\n\\nability to electronically transmit lab screening results,\\n\\n\\n\\ncomprehensive, multi-format lifestyle and behavior change programs,\\n\\n\\n\\nmulti-format chronic condition management programs,\\n\\n\\xa0\\n\\nsystems integration with other vendor partners (internal and external),\\n\\n\\n\\nability to secure and integrate health data and information from/with multiple sources to drive program activity and identify opportunities, as required by the Department,\\n\\n\\n\\nwillingness and ability to establish custom integration processes with other vendors if requested,\\n\\n\\n\\nflexible, integrated incentive tracking tools and reporting,\\n\\n\\n\\nability to support development and operation of an internal wellness champions network for the Commonwealth, and\\n\\n\\n\\nwillingness to partner with Commonwealth\\'s internal and external resources/vendors.\\n\\n\\t\\n\\n\\t\\tThe Offeror shall propose and be prepared to implement and administer a reference-based pricing model for suitable services at the Department’s direction.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall be prepared to implement and administer a shared savings incentive program similar to ones described in the Department’s Report dated November 1, 2017, available at https://rga.lis.virginia.gov/Published/2017/RD447/PDF\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall be prepared to implement and administer a bundled pricing model for musculoskeletal services as described in Item 475, G.7 available at https://budget.lis.virginia.gov/item/2018/2/HB5001/Enrolled/1/475/\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall agree to provide support for the Department’s Continuity of Operations Plan by providing office space with wi-fi encrypted access for the Department’s Office of Health Benefits and Office of Contracts and Finance staff in the event that an emergency creates the need to evacuate the Monroe Building for an extended period of time. The estimated number of Department staff is seven (7). DHRM staff will utilize their own computers and cell phones.    \\n\\n\\t\\t\\n\\n\\t\\tThe current HDHP is Health Savings Account (HSA)-compliant. The Offeror shall be prepared to implement and administer an HSA should the Department direct it to do so at any time in the future.\\n\\n\\t\\tThe Offeror shall demonstrate the ability and be prepared to implement a process for direct billing for individual LODA non-participating employers and for combined direct billing for employers in the LODA trust fund. More information can be found in the Line of Duty Act (§ 9.1-400 of the Code of Virginia). If implemented, this requirement shall include a Monthly Income Report showing premium income received from each billed entity and in total, with an indication of entities in default. This report shall be prepared in MS Excel format and provided within the time frame directed by the Department.  \\n\\n\\t\\t\\n\\n\\t\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\t\\n\\nBEHAVIORAL HEALTH PLAN \\n\\n\\t\\n\\nThe Offeror must maintain a statewide or national network of sufficient numbers of providers, counselors and other trained professionals and have the ability to mobilize those providers and respond to a large-scale critical incident event such as the Virginia Tech tragedy of April 2007.\\n\\n\\n\\nThe Offeror must administer out-of-network benefits where applicable.\\n\\n\\n\\n\\tBehavioral Health Telephone Responses\\n\\n\\n\\n\\tThe Offeror must be able to meet the following standards for telephone services and access.  These standards shall be measured and reported monthly.\\n\\n\\t\\n\\n\\t\\tThere will be no busy signals for the crisis telephone line.  Any deviation from the 100% requirement is below standard.\\n\\n\\t\\n\\n\\tNo caller to the crisis telephone line will ever be put on hold or fail to reach a live individual within 10 seconds, regardless of day or time.\\n\\n\\t\\n\\n\\tFor claims administration, utilization review and other Offeror services (for example, basic non- crisis related functions), adequate toll-free telephone lines must be available for access by covered persons during normal business hours.  The following standards must be met each month:\\n\\n\\t\\n\\n\\tFewer than 2% of calls are abandoned.\\n\\n\\t\\n\\n\\tAverage waiting time is 30 seconds or less.\\n\\n\\n\\n\\t\\tBehavioral Health Processing Time\\n\\n\\t\\t\\n\\n\\t\\t\\tThese processing standards shall be measured and reported monthly.\\n\\n\\t\\n\\n\\t\\tThe Offeror shall authorize/deny inpatient treatment within 24 hours of the request.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall authorize/deny outpatient treatment within five working days of the request.\\n\\n\\t\\t\\n\\n\\tProposals should state the Offeror’s willingness to accept these standards, and to install the necessary telephone telecommunications and other systems to ensure adherence to the standards without cost to the Commonwealth. The Offeror must be able to provide the Department with documentation of its performance.\\n\\n\\t\\n\\n\\tIn addition to those found in this section and section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist. For behavioral health services, these standards shall supersede others in this RFP with which these standards conflict.\\n\\n\\t\\n\\n\\tBrief Reference\\tLiquidated Damage Award\\n\\n\\t\\n\\n\\tno busy signal on crisis line\\t$5,000 per occurrence of busy signal\\n\\n\\t \\n\\n\\t10 second answer on crisis line\\t$5,000 per occurrence of longer than 10 second wait\\n\\n\\t                                                   \\n\\n\\t2% abandoned calls  \\t$1,000 per each percent or fraction thereof above standard\\n\\n\\t\\n\\n\\t30 second average wait time\\t$1,000 for each second above standard\\n\\n\\t\\n\\n\\t24 hour inpatient authorization\\t$1,000 per occurrence\\n\\n\\t\\n\\n\\t5 day outpatient authorization\\t$1,000 per occurrence\\n\\n\\n\\n\\n\\nVISION PLAN\\n\\n\\n\\nThe Offeror shall provide vision benefits consistent with those currently offered, unless directed otherwise by the Department.\\n\\n\\n\\n\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\n\\n\\n\\tHEARING PLAN\\n\\n\\t\\n\\n\\t\\tThe Offeror shall provide hearing benefits consistent with those currently offered, unless directed otherwise by the Department.\\n\\n\\t\\n\\n\\t\\t\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\n\\n\\tCOMPONENT NUMBER TWO (Statewide CDHP Medical/Surgical, Behavioral Health, Vision, and Hearing administrative services for the state employee plan) \\n\\n\\t\\n\\n\\tThe benefits to be offered are referenced in Appendix 3 and the employee handbook on the web site identified in subsection 1.2.  This booklet describes the contract between the employee and the Department.  To the fullest extent applicable, all the definitions, general rules, descriptions of services, exclusions, basic provisions, definitions, and eligibility rules apply to all contracts issued pursuant to this RFP. \\n\\n\\t\\n\\n\\tThe medical/surgical offeror may choose to subcontract other vendors to propose the required single offering for this component. \\n\\n\\n\\n\\tMEDICAL/SURGICAL PLAN  \\n\\n\\n\\n\\tThe Offeror must administer out-of-network benefits.\\n\\n\\t\\n\\n\\tThe Offeror shall provide disease management, maternity management and wellness programs as directed by the Department,  The Offeror may also propose additional programs, for which the Department shall have sole discretion as to whether to implement. \\n\\n\\t\\n\\n\\tThe Offeror must demonstrate experience in delivering the following services and capabilities:\\n\\n\\t\\n\\nrobust online health information, behavior change programs and support tools,\\n\\n\\n\\nability to efficiently deliver biometric testing via multiple methods such as on-site or through a lab partner, provider form or home kits,\\n\\n\\n\\nability to electronically transmit lab screening results,\\n\\n\\n\\ncomprehensive, multi-format lifestyle and behavior change programs,\\n\\n\\n\\nmulti-format chronic condition management programs,\\n\\n\\xa0\\n\\nsystems integration with other vendor partners (internal and external),\\n\\n\\n\\nability to secure and integrate health data and information from/with multiple sources to drive program activity and identify opportunities, as required by the Department,\\n\\n\\n\\nwillingness to establish custom integration processes with other vendors if requested,\\n\\n\\n\\nflexible, integrated incentive tracking tools and reporting,\\n\\n\\n\\nability to support development and operation of an internal wellness champions network for the Commonwealth, and\\n\\n\\n\\nwillingness to partner with Commonwealth\\'s internal and external resources/vendors.\\n\\n\\t\\n\\n\\tThe Offeror shall propose and be prepared to implement and administer a reference-based pricing model for suitable services at the Department’s direction.\\n\\n\\t\\n\\nThe Offeror shall be prepared to implement and administer a shared savings incentive program similar to ones described in the Department’s Report dated November 1, 2017, available at https://rga.lis.virginia.gov/Published/2017/RD447/PDF\\n\\n\\n\\nThe Offeror shall be prepared to implement and administer a bundled pricing model for musculoskeletal services as described in Item 475, G.7 available at https://budget.lis.virginia.gov/item/2018/2/HB5001/Enrolled/1/475/\\n\\n\\t\\n\\n\\tThe current CDHP provides a Health Reimbursement Arrangement (HRA). The Offeror shall be prepared to administer the HRA or, at the direction of the Department, implement and administer an HSA in place of the HRA.\\n\\n\\t\\n\\n\\t\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\t\\n\\n\\n\\n\\n\\tBEHAVIORAL HEALTH PLAN \\n\\n\\t\\n\\nThe Offeror must maintain a statewide or national network of sufficient numbers of providers, counselors and other trained professionals and have the ability to mobilize those providers and respond to a large-scale critical incident event such as the Virginia Tech tragedy of April 2007.\\n\\n\\n\\nThe Offeror must administer out-of-network benefits where applicable.\\n\\n\\n\\n\\tBehavioral Health Telephone Responses\\n\\n\\n\\n\\tThe Offeror must be able to meet the following standards for telephone services and access.  These standards shall be measured and reported monthly.\\n\\n\\t\\n\\n\\t\\tThere will be no busy signals for the crisis telephone line.  Any deviation from the 100% requirement is below standard.\\n\\n\\t\\n\\n\\tNo caller to the crisis telephone line will ever be put on hold or fail to reach a live individual within 10 seconds, regardless of day or time.\\n\\n\\t\\n\\n\\tFor claims administration, utilization review and other Offeror services (for example, basic non- crisis related functions), adequate toll-free telephone lines must be available for access by covered persons during normal business hours.  The following standards must be met each month:\\n\\n\\t\\n\\n\\tFewer than 2% of calls are abandoned.\\n\\n\\t\\n\\n\\tAverage waiting time is 30 seconds or less.\\n\\n\\n\\n\\tBehavioral Health Processing Time\\n\\n\\t\\n\\n\\tThese processing standards shall be measured and reported monthly.\\n\\n\\t\\n\\n\\t\\tThe Offeror shall authorize/deny inpatient treatment within 24 hours of the request.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall authorize/deny outpatient treatment within five working days of the request.\\n\\n\\t\\t\\n\\nProposals should state the Offeror’s willingness to accept these standards, and to install the necessary telephone telecommunications and other systems to ensure adherence to the standards without cost to the Commonwealth. The Offeror must be able to provide the Department with documentation of its performance.\\n\\n\\n\\nIn addition to those found in this section and section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist. For behavioral health services, these standards shall supersede others in this RFP with which these standards conflict.\\n\\n\\t\\n\\n\\tBrief Reference\\tLiquidated Damage Award\\n\\n\\t\\n\\n\\tno busy signal on crisis line\\t$5,000 per occurrence of busy signal\\n\\n\\t \\n\\n\\t10 second answer on crisis line\\t$5,000 per occurrence of longer than 10 second wait\\n\\n\\t                                                   \\n\\n\\t2% abandoned calls  \\t$1,000 per each percent or fraction thereof above standard\\n\\n\\t\\n\\n\\t30 second average wait time\\t$1,000 for each second above standard\\n\\n\\t\\n\\n\\t24 hour inpatient authorization\\t$1,000 per occurrence\\n\\n\\t\\n\\n\\t5 day outpatient authorization\\t$1,000 per occurrence\\n\\n\\n\\n\\n\\nVISION PLAN\\n\\n\\n\\nThe Offeror shall provide vision benefits consistent with those currently offered, unless directed otherwise by the Department.\\n\\n\\n\\n\\t\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\n\\nHEARING PLAN\\n\\n\\n\\nThe Offeror shall provide hearing benefits consistent with those currently offered, unless directed otherwise by the Department.\\n\\n\\n\\n\\t\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\n\\n\\tCOMPONENT NUMBER THREE (Statewide PPO, HDHP, and CDHP Prescription Drug  Administrative Services for the state employee, TLC, and LODA plans).   \\n\\n\\t\\n\\nFor PPO, HDHP and CDHP plans, prescription drug coverage must be mandatory generic with an option for the member to buy a brand medication, paying the difference in cost between the brand and generic plus the brand copay or coinsurance. For the PPO and HDHP plans, as defined in the Member Handbooks, there are limits to the out-of-pocket costs for certain types of medications.   \\n\\n\\t\\n\\nPrescription drug coverage must include out-of-network option.\\n\\n\\t\\n\\n\\tThe plan must be a Statewide Prescription Drug Plan providing both retail and mail service.        \\n\\n\\t\\n\\n\\tThe Offeror must develop a statewide network of pharmacies which, by contract, agrees to submit claims for subscribers, agrees not to waive co-payments or coinsurance, and agrees to accept the Offeror\\'s allowance (along with any patient co-payments or coinsurance) as payment in full for covered services.  Network pharmacies and mail services vendors must include Lesser of Logic to include MAC, Usual and Customary pricing and standard co-payments and coinsurance. \\n\\n\\t\\n\\n\\tDevelop and maintain channels of communications with pharmacies adequate to maintain a high degree of participation and continuity in the network and ensure that pharmacies are familiar with the program\\'s requirements.\\n\\n\\t\\n\\n\\tDevelop and execute a plan for conducting field audits of 5% of participating pharmacies each year.\\n\\n\\t\\n\\n\\tContract with a pharmacy, licensed to do business in Virginia, to provide mail-order prescription drug services for maintenance and specialty drug prescriptions.\\n\\n\\t\\n\\n\\tDetermine whether or not the drugs are covered.  The system should have the capability to exclude certain drugs or classes of drugs and to administer all step therapy and prior authorization requirements as requested.\\n\\n\\t\\n\\n\\tCheck consolidated community pharmacy and mail order pharmacy claims history to determine that the claim does not duplicate in whole or in part a previously paid claim or exceed contract limits.\\n\\n\\t\\n\\n\\tPrice mail order and community claims according to the contract terms.  The system should have the ability to handle varying dispensing fees, multiple co-payments, coinsurance calculations, incentives for dispensing generic drugs and limits on payments for drugs which have generic equivalents.  COB processing is required.\\n\\n\\t\\n\\n\\tPharmacy claims may be adjudicated at the point of service. However, paper claims submitted by the member must be responded to by the Offeror with an Explanation of Benefits (EOB) explaining the payment. All properly completed manual claims must be processed within 30 calendar days, and 90% of all properly completed claims must be processed within 14 calendar days.\\n\\n\\t\\t\\n\\n\\t\\tConduct an ongoing program which reviews the utilization of services, patterns of prescribing, and the actual dispensing of legend drugs under the program.\\n\\n\\t\\n\\n\\tProvide 24 hour per day pharmacy coverage to respond to emergency calls by enrollees regarding their prescriptions.\\n\\n\\t\\n\\n\\tThe Offeror shall maintain a file of persons eligible for the prescription drug benefit.  This file shall be updated from files provided by the Department, or from files provided by the Department\\'s designee.\\n\\n\\t\\n\\n\\tThe Offeror shall maintain a toll free telephone emergency information line.  The line shall be staffed at all times, 24 hours per day including weekends and holidays, by qualified personnel who can provide information to covered individuals and family members, and referrals for emergencies, if necessary.  NOTE: Answering machines or tape-recorded messages are not acceptable.\\n\\n\\t\\n\\n\\tThe Offeror shall check patient history to determine the appropriateness of the prescription in terms of quantity, possible interactions, and any related quality issues. The Offeror shall also have the ability to identify potential abuse issues.\\n\\n\\t\\n\\n\\tThe Offeror shall employ a sufficient number of licensed pharmacists to fill prescriptions correctly and quickly, and to provide redundancy as a control over quality.  Prior to mailing, each prescription must be checked by a pharmacist other than the one who filled the prescription.  The Offeror and the Department will develop a mutually agreeable performance standard in this area.  \\n\\n\\t\\n\\n\\tThe Offeror shall contact the patient or physician, as appropriate, to secure generic or therapeutic substitutes or to advise of potentially harmful drug interactions.\\n\\n\\t\\n\\n\\tThe Offeror shall dispense quickly and accurately, providing the patient with all important information about the drug dispensed and a kit for the next prescription.\\n\\n\\t\\n\\n\\tThe Offeror shall maintain consolidated retail and mail order claims history files with each prescription interactions, and provide consolidated billing to the Department.\\n\\n\\t\\n\\n\\tThe Offeror shall review paid claims files to locate and investigate cases of potential fraud and abuse.  The Offeror is responsible for developing the criteria used to identify cases, for contacting the beneficiary, pharmacy or prescribing physician, and for all appropriate corrective actions, such as collecting erroneous payments and referring potential fraud cases to appropriate authorities.\\n\\n\\t\\n\\n\\tClaims must be paid correctly.  The claims processing system must have an extensive series of prepayment edits. Claims payments are subject to audit.  Erroneous payments must be corrected, overpayments recovered and problems with the claims processing system must be repaired when ordered.\\n\\n\\t\\n\\n\\tMail order prescriptions must be dispensed within three business days of receipt.\\n\\n\\t\\n\\nMail order drugs actually dispensed shall conform to the drugs actually prescribed in every respect. Substitution consistent with the FDA Orange Book is permitted.  (The data to be used to evaluate compliance with this standard includes all prescriptions for all programs filled at the facility which dispenses drugs covered under the State plan.)\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tThe facility must be open to announced and unannounced inspections by the Department and its agents.\\n\\n\\t\\n\\n\\tThe number of dispensing errors identified at the last quality control checkpoint must be recorded and reported immediately as requested.  Also, any dispensing error discovered (by whatever means) after the drug has been mailed must be reported upon discovery in a special incident report to the Department.\\n\\n\\t\\n\\n\\tProvide definitive replies to 98% of written correspondence within five business days, and to all written correspondence within seven business days.\\n\\n\\t\\n\\nPrescription Drug Telephone Access Timeliness\\n\\n\\t\\n\\n\\tLost calls should not exceed 3% of calls during any week in a calendar month.\\n\\n\\t\\n\\n\\tAverage telephone holding time should not exceed thirty seconds during any week in a calendar month.\\n\\n\\t\\n\\n\\tAccess to live Customer Service Representatives should always be viable alternative to IVR access.\\n\\n\\t\\t\\n\\n\\tThe Offeror shall administer and monitor a Medication Therapy Management program in a manner suitable to and approved by the Department. \\n\\n\\t\\n\\n\\tIn addition to those found in this section and section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist. \\n\\n\\t\\n\\nThe Offeror shall: 1) maintain policies and procedures for transparency in its pharmacy benefit administration programs; 2) transparently provide information to state employees through an explanation of benefits regarding the cost of drug reimbursement, dispensing fees, copayments, coinsurance, the amount paid to the dispensing pharmacy for the claim, and the amount charged for the claim by the pharmacy benefit manager to the state employee health benefits program; and 3) starting\\xa0 in the second year that the contract is in effect, provide an annual report by October 1 of each year to the Department of Human Resource Management, capturing data from\\xa0 the previous fiscal year. Specifically, this report shall include the aggregate difference in amounts between reimbursements made to pharmacies by the pharmacy benefit manager for claims covered by the state employee health benefits program and the amount charged for the claim by the pharmacy benefit manager to the state employee health benefits program, as well as an explanation for any difference.\\n\\n\\t\\n\\n\\tSPECIFIC PRESCRIPTION DRUG SCHEDULE OF LIQUIDATED DAMAGES\\n\\n\\t\\n\\n\\t\\tBrief Reference\\tLiquidated Damage Award\\n\\n\\t\\t\\n\\n\\t\\tprescription\\t$100 per day for each late prescription\\n\\n\\t\\ttimeliness\\n\\n\\t\\t\\n\\n\\t\\tprescription\\t$1,000 for every instance in which scripts are \\n\\n\\t\\tdispensing\\tnot checked by a pharmacist other than the dispenser\\n\\n\\t\\t\\n\\n\\tCOMPONENT NUMBER FOUR (Statewide PPO, HDHP, and CDHP Dental administrative services for the state employee, TLC, and LODA plans)\\n\\n\\n\\nIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process should the Offeror be selected as a finalist.\\n\\n\\n\\nCOMPONENT NUMBER FIVE (Fully-insured regional offerings for state employee and TLC plans)\\n\\n\\n\\nA fully-insured regional offering must contain all major elements of the other components (Medical/Surgical, Behavioral Health/EAP, Prescription Drugs, and Dental). It may also offer additional elements, such as Vision and Hearing. \\n\\n\\n\\nThe Offeror may choose to partner with other vendors to propose the required single offering for this component. \\n\\n\\n\\nCOMPONENT NUMBER SIX (Section 125 Flexible Spending Account administration for state employees)\\n\\n\\n\\n\\t\\tThe Offeror shall construct appropriate master files from enrollment information.  All enrollment records will be retrieved by the Contractor from the Department\\'s File Transfer Protocol (FTP) file. \\n\\n\\t\\t\\n\\n\\t\\tAfter enrollment, the Offeror shall provide participants notice of the reimbursement accounts and, if requested by the Department, provide their agencies with a verification of participation and the amount(s) elected.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall record and update changes in the master files as contribution information is received from the Department of Accounts (DOA) and the decentralized payroll systems.  Payrolls are processed and updates are expected semi-monthly.  If a scheduled contribution is not received for an individual account, the Offeror shall suspend that account’s activity, as appropriate, and request clarification from their agency.  The Offeror shall release the suspended account only after the correct contribution or related election change has been received.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall develop, print, and distribute to appropriate agencies any and all forms required for all tasks.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall receive and track requests for payment (claims) from participants.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall verify documentation and determine conformance of the claim to IRS regulations, DHRM regulations and instructions and DHRM\\'s plan documents.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall determine that the claim is valid with respect to the object of expense and the date of incurrence and the eligibility of the claimant.  All complete paper claims shall be processed and paid within 5 to 7 business days of receipt.  All documentation for debit card transactions shall be validated within 5 to 7 business days of receipt.\\n\\n\\t\\tThe Offeror shall maintain a claim payment accuracy that exceeds 99% for financial accuracy and 95% for non-financial processing accuracy of all claims processed in each measured period of the contract. \\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall screen claims prior to payment to uncover potentially duplicate claims.  Checking for duplicate claims must be an automated feature of the payment system.  The Offeror shall investigate and resolve potential duplicates.  The Offeror shall record and code claims in sufficient detail to permit required reporting and to avoid duplicate payments.\\n\\n\\n\\n\\t\\tThe Offeror shall produce payment checks and remittance statements for individuals that clearly explain the nature of the payment being made, plan year to date activity and account balances.  If a claim is being denied partially or in its entirety, the statement should clearly explain the basis of the denial and advise the claimant of how the claim may be perfected or to whom the denial may be appealed.\\n\\n\\t\\t\\n\\n\\t\\tWithin thirty days of the close of each calendar quarter, the Offeror shall provide each participant a statement disclosing the status of his/her account as of the end of the quarter.  \\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall provide participants on or about 60 days prior to the end of each plan year a statement of balances alerting participants of possible forfeitures. Mailing of the year end notice shall be no earlier than 70 days and no later than 50 days prior to the end of the plan year. \\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall process claims for at least 90 days (this is the run-out period) after the close of the plan year or the termination of the participant’s account, and upon request of the Department.  Except during the first and the last year of the contract, the Offeror shall process claims simultaneously for more than one plan year (that is, claims for the current plan year and claims for the 90 day run-out period of the previous plan year).  The Offeror shall control and report claim payments by plan year, regardless of plan year status.  As soon as practicable after the close of the first month of this 90 day run-out period, but no later than 15 days into the second month, the Offeror shall mail to participants, whose accounts have balances, a statement of the account.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall provide educational materials and standard election and revision forms for each Open Enrollment during the life of the contract if required by the Department.  The Offeror shall provide or arrange for the provision of educational materials, as required herein or agreed to by the Department, at sites throughout the State.  This task includes all design, typeset, printing, overprinting and distribution costs.  No additional costs shall be billed outside of this contract.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall prepare an administrative plan manual with provisions for updates, for the use of your processors, which is Commonwealth of Virginia plan specific, including as a minimum the reporting requirements, standards, processing procedures and policies.  The Offeror shall provide a draft of this manual to DHRM for approval at least 30 days prior to start of contract.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall provide staffing to ensure that no more than 3% of telephone calls may be abandoned in any week and the average time that callers are left on hold shall be less than 30 seconds. The Offeror shall deliver a monthly report detailing the number of calls received, abandoned, hold time and wait time for the previous month.\\n\\n\\n\\n\\t\\tThe Offeror shall ensure that correspondence from participants is date stamped on the day it is received, and is answered within 10 business days.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall provide all FSA participants with a website that permits on-line access to account balance, claim status and the ability to submit claims electronically as long as funds remain in the account(s). The Offeror’s website shall have a direct link to the Department’s web page. The Department prefers that Offeror’s submission also includes a mobile application that allows participants to check balances and submit claims.\\n\\n\\t\\t\\n\\n\\tThe Offeror shall provide all FSA participants the ability to request direct deposit for all claim payments and a debit card(s) via a web application, customer service center or form.  The Offeror shall provide FSA participants that elect the debit card with a monthly statement of activity.\\n\\n\\t\\t\\n\\n\\tThe Offeror shall enroll in the Commonwealth’s Electronic Data Interchange (EDI) program as all payments shall be made through EDI.  Additional information is available from the Department of Account’s website: \\n\\n\\thttps://www.doa.virginia.gov/onlineservices.shtml#edi \\n\\n\\t\\t\\n\\n\\t\\tNo later than one week prior to the start of the plan year for the new contract, the Commonwealth will provide a mutually acceptable amount as an initial deposit or pre-fund.  The purpose of this deposit is to cover claims paid out by the Offeror until the claims may be invoiced and reimbursed to the Offeror by the Commonwealth.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall invoice the Commonwealth for claims processed during the previous payment cycle as agreed upon by the Offeror and Commonwealth.  Invoices shall provide totals by account type and plan year, regardless of the status of the plan year.  Documentation providing details by employer, employee, account type and plan year for claims reimbursed per the invoice must also be provided.  Adjustments and/or claims paid for prior plan years must be identified separately and include employer, employee, account type and plan year details.\\n\\n\\t\\t\\n\\n\\t\\tThe Offeror shall safeguard check stock and control signature authority according to best GAAP equivalent business practices.  The Offeror shall account for all wasted, voided and manual checks.\\n\\n\\t\\t\\n\\n\\tThe Offeror shall provide the Commonwealth with a monthly accounting of all disbursements by account type and plan year.\\n\\n\\t\\t\\n\\n\\tThe Offeror shall provide forfeiture balances and listing of all stale-dated checks (identified by employer, employee, account type and plan year) within 45 days of the end of the run-out period at the end of each plan year.\\n\\n\\t\\n\\n\\tThe Offeror shall be responsible for completion of escheatment process according to policies and procedures established by the Treasurer of Virginia for all stale-dated checks and shall provide a report to the Department listing amounts sent to Treasury, identified by employee, account type and plan year.  Additional information regarding Commonwealth’s escheatment process may be found on Department of Treasury’s website:  http://www.trs.virginia.gov/UCP/Holder/default.aspx  \\n\\n\\t\\n\\n\\tThe Offeror shall process payments accurately and timely.  The Offeror shall advise the Department’s contract representative (subsection 8.7) within two working days whenever there are problems in the system which may create public inquiries, such as system breakdowns, missed payment cycles, or egregious errors.  The Offeror shall produce demand letters for collecting any amounts due from participants and process refunds for participants as required.\\n\\n\\t\\n\\n\\tThe Offeror shall provide a toll free number for the use of participants to obtain general information regarding the respective programs and specific information about account balances and transactions. This number shall be activated no later than 90 days prior to the effective date of the first plan year.  The Offeror shall operate a call tracking system which allows for the measurement of the number of calls abandoned, wait time, etc.  The Offeror shall answer inquiries, whether by phone or by mail, accurately, politely and timely.\\n\\n\\t\\n\\n\\tWithin 180 days of the end of each plan year, the Offeror shall have performed by an outside auditor, an audit of the plan year’s activity and provide the Department with the results.\\n\\n\\t\\n\\n\\tThe Offeror shall demonstrate the ability to administer a section 132 qualified Transportation Fringe Benefit Program to include mass transit and parking for the Commonwealth if requested.\\n\\n\\t\\n\\n\\tThe Offeror shall administer future salary reduction plans (such as 125 or 132) if requested by the Department.\\n\\n\\t\\n\\n\\tThe Offeror shall have the ability to accept a claim file(s) for out of pocket expenses associated with the health plan and auto-process these claims upon the participant’s request.\\n\\n\\t\\n\\n\\tThe Offeror shall have the ability to integrate and/or coordinate claim reimbursement with a Health Reimbursement Arrangement medical plan design.\\t\\n\\n\\n\\n\\tThe Offeror shall provide the report contents as agreed to and adhere strictly to the agreed upon reporting time frames. \\n\\n\\t\\n\\n\\tThe Offeror shall adhere strictly to the agreed upon implementation schedule.\\n\\n\\t\\n\\n\\tThe Offeror shall comply with Health Insurance Portability and Accountability Act of 1996 (HIPAA) regulations, including, but not limited to 45 CFR Section 160.103.\\n\\n\\n\\n\\tThe Offeror shall update the Master files within 3 business days upon the date of receipt of a processable eligibility file from the Commonwealth.  The Offeror shall post contributions to customer accounts within 3 business days upon the date of receipt of processable payroll files from the Commonwealth.\\n\\n\\t\\n\\n\\tThe Offeror shall deliver a monthly report within 30 days of the end of each calendar quarter providing the status of each individual account including contributions, payments, annual elections and account balances.  This information should also be summarized by employer for each plan.\\n\\n\\t\\n\\n\\tThe Offeror shall deliver separate comprehensive reports by January 15 of each year for the previous plan year’s operations for each account.  In addition to statistical information, these reports shall include an evaluation of the program with recommendations for improvement.\\n\\n\\t\\n\\n\\tIn addition to those found in section 3.9, the Offeror shall propose any additional plan specific standards of performance by which its performance shall be judged. Specific areas of interest are identified in the Attachment 2 questionnaire. Liquidated damages will be attached to these additional standards through the negotiation process, should the Offeror be selected as a finalist.\\n\\n\\t\\n\\n\\t\\n\\n\\t3.0\\tSTANDARDS OF PERFORMANCE\\n\\n\\n\\n\\t3.1  \\tGENERAL\\n\\n\\n\\nThe Offeror shall be solely responsible to the Department and liable for any delay or non-performance of any portion of the contract which results from this RFP, and for erroneous payments.  The Offeror shall not be responsible for delay or non-performance if the non-performance is caused by the failure of the Commonwealth, covered persons, or non-network providers to provide information necessary for the Offeror to meet its contractual obligations.  \\n\\n\\n\\nCertain performance obligations are of such importance that an Offeror’s failure to achieve the requirements found herein jeopardizes the value which the Department expected of the contract.  In acknowledgment of this, and in consideration of the extra expenses and other damages incurred by the Department, should the Offeror fail to fulfill specified contractual obligations, both parties agree that the Offeror shall pay to the Department the amount contained in the appropriate schedule of liquidated damages (see subsections 2.9 and 3.9) when the Offeror\\'s performance fails to meet the specified standards of performance.  \\n\\n\\n\\nIt is expressly agreed that, unless otherwise specified, the determination of liquidated damages, if any, shall be determined annually by comparing the system generated reports in Attachment 2 and 3 to the related Schedules submitted by the Offeror. \\n\\n\\n3.2\\tCLAIMS MUST BE PAID CORRECTLY\\n\\n\\n\\nThe goal is 100% accuracy.\\n\\n\\n\\nBelow Standard:  \\n\\n\\n\\nTotal payment error rate in excess of .8% of benefit payments, where total payment error rate is the dollar amount of erroneous payments, including payments to an incorrect payee (any reason) or paid in an incorrect amount (any overpayment plus any underpayment) or any other payment error (including both incorrect payee and incorrect amount), divided by the total dollar amount of claims paid during the audit period, OR\\n\\n\\n\\nTotal error rate in excess of 3% of claims processed, where total error rate is the number of claims with any kind of error (including payment errors) divided by the total number of claims processed during the audit period.\\n\\n\\n\\nCompliance with this standard shall be determined by internal audit, and be subject to verification by external audit.  Should the internal and external audits arrive at results which materially affect the amount of liquidated damages, the Offeror and the Department shall negotiate the actual amount of the damages.  If these parties cannot reach an agreement through negotiation, they shall jointly pay for an independent audit whose determination shall be binding on both parties.\\n\\n\\n\\nCOORDINATION OF BENEFITS SAVINGS\\n\\n\\n\\nProduce savings from coordination of benefits of at\\xa0least 1% of non-Medicare paid claims per reporting year for\\xa0the active employee-early retiree group.   HMOs are exempt from this requirement.\\n\\n\\n\\nCompliance with this standard shall be determined by audit as described in 3.2.\\n\\n\\n\\nOfferor(s) must respond to requests from other vendors who are monitoring Medicare primacy of active employees on behalf of the Department for purposes of recovering primary payments made in error and will maintain accurate coordination of benefits based on Medicare secondary payer guidelines and/or designation of Medicare primacy through eligibility file designations.  Medicare Secondary Payer Demands and Primary Payment Notices will be forwarded to the Department for processing. \\n\\n\\n\\n3.4\\tACCESS OF ELIGIBILITY FILES UPDATES\\n\\n\\n\\nThe Department will maintain current eligibility files for the state employee, TLC, and LODA programs.  The Department provides the Contractor with eligibility information in the HIPAA 834 Transaction File formats as described on the DHRM Website (http://web1.dhrm.virginia.gov/itech/documentation.html).  \\n\\n\\n\\nThe Contractor must connect to the Department’s secure FTP server for file transfers by one of the following protocols:  SFTP usng SSH2 on port 22; or HTTPS for manual retrieval.  It is expected that each Contractor will comply with the Department’s eligibility file processing schedule. \\n\\n\\n\\nTwo types of files are regularly provided:  \\n\\n\\n\\nThe Daily Change File is provided Tuesday through Saturday and is used to update the Contractor’s system.  \\n\\nThe Monthly Audit File is provided on the third of each month and is not used to update the Contractor’s system.  It is used to compare information in the Contractor’s system and to report discrepancies back to the Department.  Discrepancies are due to the Department no later than the 20th of the month in which the Audit File is received.  \\n\\n\\n\\n3.5\\tREPORTING\\n\\n\\n\\nReports containing the requested information shall be submitted timely and accurately.  The submission of a materially inaccurate report does not constitute timely submission for the purposes of this section.  NOTE: Timely reporting also includes the submission of accurate and readable weekly claims files, paid claims invoices, and monthly administration invoices. \\n\\n\\n\\nThe Department shall determine compliance with this standard by the date of receipt of reports.\\n\\n\\n\\nINVOICE PROCESSING\\n\\n\\n\\nThe Offeror must process 90% of TLC premium invoices within 3 business days of\\xa0receipt of payment and 100% of premium invoices within 5\\xa0business days of receipt. See Section 8.4.2 for additional details.\\n\\n\\n\\nCompliance with this standard shall be determined by audit as described in 3.2.\\n\\n\\n\\n\\t3.7\\tPREMIUM PROJECTIONS \\n\\n\\n\\nThe total guaranteed discount representing the Net Payment after Application of Your Reimbursement Method reported (in Attachment 2), will be compared with actual results no later than 90 days after the close of the fiscal year. Actual discount results that are within 2% (percentage points) of the total guaranteed discount will not invoke a penalty. If actual results are less than 98% of the total guaranteed discount, then 5% of the Offeror’s administrative fee shall be owed and due to the Department as liquidated damages.\\n\\n\\n\\n3.8\\tMEMBER SATISFACTION\\n\\n\\n\\nAt least 90% of the covered persons responding to the Offeror\\'s annual surveys (subsection 4.1.8) must rate their overall experience with the program as \"satisfactory\" or better.\\n\\n\\n\\n\\n3.9  \\tSCHEDULE OF LIQUIDATED DAMAGES – GENERAL\\n\\n\\n\\nThis schedule of liquidated damages is mutually agreed upon in view of the difficulty and the cost of measuring the actual damages incurred from complaints, lost productive time, intrusion into other business, etc., as a result of under-performance in the areas noted.\\n\\n\\n\\n        Brief Reference \\t\\t\\tLiquidated Damage Award \\n\\n\\n\\n99.2% of benefit $ paid correctly\\n\\n2% of administrative costs for failure to meet the standard\\n\\n\\n\\n97% of claims paid without error\\n\\n2% of administrative costs for failure to meet the standard\\n\\n\\n\\nEligibility Files not picked up as scheduled\\n\\n\\n\\n$1,000 first occurrence, $10,000 per occurrence thereafter\\n\\n\\n\\nCOB savings of 1%\\n\\n1% of administrative costs for failure to meet standard This is an old PG and less pertinent today with most families not electing double coverage. \\n\\n\\n\\nLate/Missing/Inaccurate Reports\\n\\n$1,000 per day, days 1-5;  $10,000 per day thereafter\\n\\n\\n\\nInvoice Processing\\n\\n$2,000 per invoice not meeting standard\\n\\n\\n\\nProvider Discount Attainment\\n\\n5% of administrative fees if actual discounts are less than 98% of the guaranteed target\\n\\nMember Satisfaction\\n\\n\\n\\n\\n\\nDedicated Website must be developed and implemented in accordance with contract\\n\\n\\n\\nDedicated customer service telephone services must be developed and implemented in accordance with contract\\n\\n$5,000 for each 1% or fraction thereof below standard\\n\\n\\n\\n$25,000 per month or prorated for a partial month\\n\\n\\n\\n\\n\\n $25,000 per month or prorated for a partial month\\n\\n\\n\\n\\n\\n\\n\\nAdditional performance standards specific to the Prescription Drug and Behavioral Health products may be found at Section 2.9. above. \\n\\n\\n\\n\\n\\n\\t4.0 \\tREPORTS AND DELIVERABLES\\n\\n\\n\\nGenerally, separate report sets are required for each separate group, including but not necessarily limited to (1) TLC local governments, (2) TLC school jurisdictions, (3) TLC in total, (4) the state employee active employee group, (5) the state employee early retiree group, (6) state program Extended Coverage/COBRA participants, (7) the state employee group in total, (8) LODA participating employers, (9) LODA non-participating employers, (10) LODA current employment, (11) LODA former employment, and (12) LODA in total. Attachments 2 and 3 also contain formats of some system-generated reports that will be used to assess Offeror performance and to determine the amount of liquidated damages due, if any.  Report formats are generally contained in Attachments 2 and 3.  Offerors are invited to suggest improvements or additional reports.\\n\\n\\n\\n\\t4.1\\tREPORTS AON SUPPLYING FOR RFP\\n\\n\\n\\n\\t4.1.1\\tRate and Administrative Expense Buildup Schedule\\n\\n\\t\\t\\t\\n\\n\\t\\tThis form, which may be found in Attachment 2, must be submitted.\\n\\n\\t\\n\\n\\t4.1.2\\tWeekly Claims Report\\n\\n\\t\\n\\n\\t\\tThe Weekly Claims Report is to be prepared in MS Excel format and E-mailed on the third business day after the close of the week.  The format is contained in Attachment 3.\\n\\n\\t\\n\\n\\t4.1.3\\tWeekly claims file\\n\\n\\t\\n\\n\\t\\tThe format for the file will be provided to the finalist.  \\n\\n\\t\\n\\n\\tAdministrative Fee Report\\n\\n\\t\\n\\n\\t\\tThis report is used by all ASO plans to invoice administrative costs on a monthly basis. The format is contained in Attachment 3.\\n\\n\\t\\n\\n\\t4.1.5  \\tQuarterly Service Report\\n\\n\\t\\n\\n\\t\\tThis report discloses Offeror’s results in meeting customer service and claims processing goals. The format can be found in Attachment 3.\\n\\n\\t\\n\\n\\t4.1.6\\tTLC Monthly Income Report\\n\\n\\t\\tThe Monthly Income Report shows the premium income received from each local employer by plan and in total, with an indication of employer groups in default. The report is to be prepared in MS Excel format (see Attachment 3) and E-mailed on the 20th day after the close of the month.\\n\\n\\t4.1.7\\tQuarterly Utilization Management Report\\n\\n\\t\\tThis report discloses the Offeror’s assessment of its utilization management activities, including admission review, concurrent review and case management. The specifications for this electronic file are found in Attachment 3.\\n\\n\\t\\n\\n\\t4.1.8\\tAnnual HEDIS\\n\\n\\t\\n\\n\\t\\tThe Offeror shall submit the latest appropriate version of the HEDIS (or Department-approved equivalent), including the standard Member Satisfaction Survey for the most recent calendar year, by July 1 or with the Offeror’s renewal, as appropriate.\\n\\n\\t\\n\\n\\t4.1.9 \\tAnnual Accounting and Renewal \\n\\n\\t\\n\\n\\t\\tOn or before September 15, or such date as determined by the Department, after the completion of 12 months’ operations under the contract, the Offeror shall submit specified IBNR lag triangle data in the required form to the Department Actuary.\\n\\n\\t\\n\\n\\t\\tOn or before September 15, or such date as determined by the Department, after the completion of 12 months’ operations under the contract, the Offeror shall submit a complete accounting of its operations for the fiscal year ended the last June 30th, and shall propose a rate, for the fiscal year beginning the next July 1.  The accounting and rate analysis should treat separately each major class of benefits, Medical/Surgical, Behavioral Health, Prescription Drug, and Dental.  \\n\\n\\t\\n\\n\\t\\tIn addition, the Medical/Surgical Offeror’s Annual Report shall contain:\\n\\n\\t\\n\\n\\tcosts by employee, spouse and dependents (separately for active employees, retirees, and extended coverage enrollees), \\n\\n\\ta list of the fifty highest cost cases (enrollees) with relevant detail on admissions, diagnoses, etc., \\n\\n\\tamounts paid to hospitals (including inpatient surgical per diem, inpatient acute medical per diem, inpatient acute obstetrical case rate, inpatient outlier minimum charge per case and inpatient outlier rate, and outpatient case rates for those procedures which comprise 50% of outpatient hospital reimbursement, or for the 25 procedures which have the highest total dollar impact together with an indication of the percentage of total outpatient reimbursement these 25 procedures represent),\\n\\n\\tshow the fifty professional providers of services receiving the largest payments, and \\n\\n\\t\\n\\n\\tclaims in excess of $150,000 for Medical/Surgical, Behavioral Health, and Prescription Drug if not previously reported. \\n\\n\\tThe Annual Report shall provide a frequency distribution of contracts, claims and dollars paid in total and by type of benefit. Offeror shall also include additional information at the Department’s direction and present the Annual Report in a format acceptable to the Department.\\n\\n\\t\\n\\n\\t\\tThe Offeror shall provide an annual mandated benefit report as required by § 2.2-2818(R)\\n\\n\\t\\n\\n\\t\\tMonthly Behavioral Health Services Report\\n\\n\\n\\n\\t\\tThis report discloses requests for services, pre-authorized amounts, and the impact of appeals. The format can be found in Attachment 3.\\n\\n\\t\\n\\n\\t4.1.12 \\tSuch other reports as may be necessary to document the performance of the Offeror and its adherence to the contracted standards.\\n\\n\\n\\n\\t 4.2  \\tOTHER DELIVERABLES\\n\\n\\n\\n\\t4.2.1\\tThe Offeror for each Component agrees to furnish and warrants that the administrative charge quoted includes all enrollment materials, benefits booklets, and brochures describing plan benefits, applications, notices, claims forms, checks, remittance advices, articles for employee publications, administrative manuals, provider networks, directories, forecasts, invoices, identification cards, criteria sets and such services and materials stated or implied anywhere in this RFP or the Contractor\\'s response thereto, including any correspondence associated with new programs as a result of provisions of this RFP.\\n\\n\\t\\n\\n\\tThe Component Number One Contractor shall assist the Department with the ongoing operations of the TLC program, and, where applicable, the LODA program, by providing direct support with, but not limited to, marketing; communications; underwriting; renewal and proposal preparation and delivery; group billing and collections; and distributing ASO membership to other subcontractors as needed.    \\n\\n\\n\\n\\n\\n\\t5.0\\t PROCUREMENT PROCEDURES \\n\\n\\n\\n\\t5.1\\tMETHOD OF AWARD\\n\\n\\n\\n\\t5.1.1\\tThe Department shall select two or more Offerors per product deemed to be fully qualified and best suited among those Offerors submitting proposals, unless the Department has made a determination in writing that only one Offeror is fully qualified, or that one Offeror is clearly more highly qualified than the others under consideration.  The selection of Offerors will be based on the evaluation factors included in this RFP.  Negotiations shall be conducted with the selected Offeror(s).  Price shall be considered when selecting finalists for negotiation, but shall not be the sole determining factor.\\n\\n\\t\\n\\n\\t5.1.2\\tAfter negotiations have been conducted with each selected Offeror, the Department shall select the Offeror, which, in its opinion, has made the best proposal.  The Department shall award the contract to that Offeror.  The Department may cancel this RFP, or reject proposals at any time prior to an award.  The Department is not required to furnish a statement of the reason why a particular Offeror was not deemed to have made the best proposal (Section 2.2-4359, Code of Virginia).\\n\\n\\t\\n\\n\\t5.1.3 \\tShould the Department determine in writing, and in its sole discretion, that only one Offeror is fully qualified, or that one Offeror is clearly more highly qualified than the others under consideration, a contract may be negotiated and awarded to that Offeror.\\n\\n\\t\\n\\n\\t5.1.4\\tThe contract will incorporate by reference all the requirements, terms and conditions of this RFP and the Contractor’s proposal, except as either or both may be amended through negotiation.  All statements and representations, written or verbal, relating to the award of this and renewal contracts must be construed to be consistent with the following submission instructions.\\n\\n\\t\\n\\n\\t5.2\\tSUBMISSION OF WRITTEN PROPOSALS\\n\\n\\n\\n\\t5.2.1\\tAll proposals must be in the form requested (See Section 6.0 and Attachment 2).  The data required on the schedules submitted in response to this RFP are subject to verification.  Material errors shall be a basis for rejecting such a proposal.  An Original, a Redacted Electronic Copy and twelve (12) hard/paper and twenty-five (25) electronic copies of the original, on separate CDs, shall be delivered in a sealed container, and labeled as a proposal, with the words \"Do Not Open\" and the type of benefit plan enclosed prominently displayed on the outside.  Proposals must be received no later than 2:00\\xa0p.m. on September 18, 2018, by:\\n\\nMr. Richard Whitfield\\n\\nDepartment of Human Resource Management\\n\\nJames Monroe Building, 13th Floor\\n\\n101 North 14th Street\\n\\nRichmond, Virginia  23219\\n\\n\\n\\nThe original proposal and hard copies should be bound in a loose-leaf notebook.  All documentation submitted with the proposal should be contained in that single volume.  (If necessary, additional notebooks may be submitted in clearly marked and referenced sequence.)  Offerors are required to submit a CD containing their response in MS Excel and Word format, as directed by the Attachment 2 schedules, along with each copy of the proposal.\\n\\n\\n\\n\\t5.2.2\\tOwnership of all data, materials and documentation originated and prepared for the Department pursuant to the RFP shall belong exclusively to the Department and be subject to public inspection in accordance with the Virginia Freedom of Information Act.  Trade secrets or proprietary information submitted by an Offeror shall not be subject to public disclosure under the Virginia Freedom of Information Act; however, the Offeror must invoke the protections of Section 2.2-4342 of the Code of Virginia, in writing, at the time the data or other material is submitted.  The written notice must specifically identify the data or materials to be protected and state the reasons why protection is necessary.  The proprietary or trade secret material submitted must be identified as required and must indicate only the specific words, figures, or paragraphs that constitute trade secrets or proprietary information. The Department, in its sole discretion, may not consider proposals with unduly broad requests for protection against disclosure.\\n\\n\\n\\n\\n\\t5.3\\tMODIFICATION OF PROPOSALS\\n\\n\\n\\nAny changes, amendments or modifications of an Offeror\\'s proposal prior to the deadline for receipt of proposals must be in writing and submitted in the same manner as the original proposals.  All modifications must be labeled conspicuously as a change, amendment, or modification of the previously submitted proposal.  Changes, amendments, or modifications of proposals will not be considered after the deadline for receipt of proposals, except when the Department requests modifications.\\n\\n5.4\\tORAL PRESENTATION\\n\\n\\n\\nOfferors who submit a proposal in response to this RFP may be required to give an oral presentation of their proposal to the Department.  This provides an opportunity for the Offeror to clarify or elaborate on the proposal.  This is a fact finding and explanation session only and does not include negotiation.  The Department will schedule the time and location of these presentations.  Oral presentations are an option of the Department and may or may not be conducted.\\n\\n\\n\\n5.5\\tINQUIRIES CONCERNING THE RFP\\n\\n\\n\\nAny communication concerning this RFP or any resulting contracts must be addressed in writing to:\\n\\nMr. Richard Whitfield\\n\\nDepartment of Human Resource Management\\n\\nJames Monroe Building, 13th Floor\\n\\n101 North 14th Street\\n\\nRichmond, Virginia  23219\\n\\n\\t\\t\\tE-mail:   richard.whitfield@dhrm.virginia.gov \\n\\n\\n\\n5.6\\tPUBLIC INSPECTION OF PROCUREMENT RECORDS\\n\\n\\n\\nProposals will be subject to public inspection only in accordance with Section 2.2-4342 of the Code of Virginia.\\n\\n\\n\\n5.7\\tCLARIFICATION OF PROPOSAL INFORMATION\\n\\n\\n\\nThe Department reserves the right to request verification, validation or clarification of any information contained in any of the proposals.  This clarification may include checking references and securing other data from outside sources, as well as from the Offeror.\\n\\n\\n\\n5.8\\tREFERENCE TO OTHER MATERIALS\\n\\n\\n\\nThe Offeror cannot compel the Department to consider any information except that which is contained in its proposal, or which is offered in response to a request from the Department.  The Offeror should rely solely on its proposal.  The Department, however, reserves the right, in its sole discretion, to take into consideration its prior experience with Offerors and information gained from other sources.\\n\\n\\n\\n\\n5.9\\tMANDATORY PRE-PROPOSAL CONFERENCE\\n\\n\\t\\n\\nA mandatory pre-proposal conference will be held at 10:30 a.m. August 20, 2018, in the James Monroe Building, Conference Rooms C, D and E, 1st Floor, 101 North 14th Street, Richmond, Virginia.  The purpose of this conference is to allow potential Offerors an opportunity to present questions and to obtain clarification relative to any facet of this procurement. \\n\\n\\n\\nAttendance at this conference is a prerequisite to submitting a proposal.  Offerors who intend to submit a proposal are required to attend.  Any changes resulting from this conference will be issued in a written addendum to the RFP.  Attendance at the conference will be documented by the representative’s signature on the attendance roster. Offerors should bring a copy of this RFP to the conference. Any vendor arriving to the conference rooms after 10:30 a.m. will not be admitted (a picture ID is required, and arrival 30 minutes in advance of the meeting is recommended to allow for parking).\\n\\n\\n\\nIt is requested that any known questions regarding the RFP be sent by e-mail to Richard Whitfield prior to date of conference to facilitate the conference.  See E-mail address in subsection 5.5. It is also requested that the entire RFP, including exhibits, appendices, and attachments (including the Attachment 2 schedules) be read prior to submitting questions.\\n\\n\\n\\n\\t5.10 \\tTIMETABLE\\n\\n\\n\\n\\tRFP Published\\tAugust 3, 2018\\n\\n\\tMandatory Pre-Proposal Conference\\tAugust 20, 2018\\n\\n\\tCut-off Date for RFP Questions/Inquiries\\tSeptember 1, 2018\\n\\n\\tProposals Due, 2:00 P.M.\\tSeptember 18, 2018\\n\\n\\tNotice of Intent to Award\\tDecember 11, 2018\\n\\n\\t\\n\\n\\t\\n\\n\\t6.0 \\tFORM OF RESPONSE AND CRITERIA\\n\\n\\n\\n\\t6.1 \\tGENERAL\\n\\n\\n\\nThe Department, at its discretion, may choose one Offeror to administer all or any combination of products. \\n\\n\\n\\nFully insured and less than statewide proposals for each area for which you are proposing must be bundled, which means Offerors must include pharmacy, and dental with their medical/surgical/behavioral health submission. \\n\\n\\n\\nOfferors must clearly identify the Component for which they are bidding. \\n\\n\\n\\nAttachment 2 contains instructions for obtaining the schedules required to complete a proposal.  Please review Attachment 2 carefully and complete all information that applies to the proposal.  \\n\\n\\n\\nProposal shall be in the form of a loose-leaf binder tabbed to point to each section below.  Before the first tab:\\n\\n\\n\\nPlace the executed RFP Cover Sheet followed by a statement defining those sections of your proposal which may not be released because they are proprietary.   \\n\\n\\t\\n\\nFollowing the executed Cover Sheet and statement of confidentiality, place a properly completed Proposal Checklist, which is found in Appendix 9.\\n\\n\\n\\nAn original proposal, an electronic redacted version, twenty-five (25) electronic copies and twelve (12) hard/paper copies are required.  The original shall contain a Cover Sheet bearing an original signature signed in BLUE ink and be labeled on the cover as “Original”.\\n\\n\\n\\n\\t\\tREDLINE RFP NOTING DEMURRALS (TAB 1)\\n\\n\\n\\nInclude a copy of the RFP. Using the Track Changes and Highlight Changes MS Word tools, annotate in redline any and all demurrals or deviations to the requirements of the RFP. Please note that demurrals are a rating criterion for this RFP. As such, Offerors who include demurrals should expressly describe the reasons for each demurral. You may enter any substantive comments on the RFP provisions, but please restrict such to issues that are necessary to clearly understand your proposal. Information required in the tabs below need NOT be repeated in this tab. Also, affirmations or confirmations of compliance to RFP requirements are unnecessary in this tab and are NOT to be included.\\n\\n\\t\\n\\n\\t\\tLEGALLY CORRECT DESCRIPTION OF BENEFITS (TAB 2)\\n\\n\\n\\n\\t\\tFor all plans within all components except Number Five, itemize any benefit changes to current plans. \\n\\n\\t\\n\\n\\t\\tBENEFITS BROCHURE (TAB 3)\\n\\n\\t\\n\\n\\tThe Offeror shall submit a model brochure containing supplemental information for employees to help them understand how the plan works.\\n\\n\\n\\n\\tThe brochure shall consist of the information required by the monthly service report (see subsection 4.1), and all of the following available or applicable to the type plan offered.\\n\\n\\t\\n\\n\\tthe plan’s NCQA certification status, \\n\\n\\t\\n\\n\\tselected HEDIS (or Department approved substitute) information on \\n\\n\\t\\n\\n\\tplan membership\\n\\n\\t\\t\\n\\n\\t\\teffectiveness of care\\n\\n\\t\\t\\n\\n\\t\\tPCP availability\\n\\n\\t\\t\\n\\n\\t\\tphysician turnover\\n\\n\\t\\tdisenrollment \\n\\n\\t\\t\\n\\n\\t\\trate trends\\n\\n\\t\\t\\n\\n\\thighlights from the HEDIS (or Department approved substitute) Member Satisfaction Survey, including\\n\\n\\t\\n\\n\\toverall satisfaction\\n\\n\\t\\n\\n\\toverall quality of care and services\\n\\n\\t\\n\\n\\taccess\\n\\n\\t\\n\\n\\trecommendation to family and friends\\n\\n\\t\\t\\n\\n\\ta brief summary of the report, which describes the plan’s adherence to the access standards, found in paragraph 2.3, subsection 1. \\n\\n\\t\\n\\n\\t\\ta brief discussion of the criteria used to admit institutional and professional providers into the network and the bases on which the plan pays the providers.  \\n\\n\\t\\n\\n\\t\\toptionally, the plan may include practice guidelines covering those outpatient procedures representing about one-half of outpatient professional costs.\\n\\n\\n\\n\\tTECHNICAL QUESTIONNAIRE\\n\\n\\n\\nAttachment 2 contains instructions for completing the Technical Questionnaire, which constitutes the technical proposal. Instructions will include guidance on which questionnaire(s) must be completed for each component.  Offerors should only complete sections related to the components for which they are submitting a proposal.   Attachment 2 schedules must be completed in accordance with the instructions contained in the Questionnaire.  In addition to the hard copy contained in the questionnaire, the electronic file must be provided with your response as requested in the Questionnaire.\\n\\n\\n\\n\\t\\tCOST PROPOSAL \\n\\n\\n\\nAttachment 2 contains the instructions for the schedules which, along with the Offeror’s latest certified audit report, constitute the cost proposal.  Include in this tab, a copy of the audited report for the most recently completed fiscal year and a hard copy of the schedules.  Also, the schedules must be submitted as directed in Attachment 2 instructions.\\n\\n\\n\\nThe attachment also contains instructions for the administrative cost schedules that provide the following cost proposal detail:\\n\\n\\n\\n\\tA firm, fixed price per contract month for the first contract year.  \\n\\n\\t\\n\\n\\tA firm, fixed price per contract month for the second contract year. This price may not be indexed to the price of the first contract year. \\n\\n\\t\\n\\n\\tA firm, fixed price per contract month for the third contract year. This price may not be indexed to the price of either the first or the second contract year. \\n\\n\\t\\n\\n\\tA guaranteed interest rate for funds in the operating account or an index which will constitute a minimum guarantee. (Offerors of fully insured plans are exempt from this subsection)\\n\\n\\t\\n\\n\\tA cost summary page\\n\\n\\t\\n\\nPlease note: The administrative cost schedule requires component cost information. The level of detail requested must be provided.\\n\\n\\t\\n\\nSMALL BUSINESS PARTICIPATION (TAB 6) \\n\\n\\n\\nComplete the information required on Exhibit TWO.\\n\\n\\n\\n\\t\\tCRITERIA \\n\\n\\n\\nProposals for each Component will be evaluated as listed below: \\n\\n\\n\\nComponent Number One--Statewide PPO and HDHP Medical/Surgical, Behavioral Health (to include EAP), Vision, and Hearing administrative services for the state employee, TLC, and LODA plans:\\n\\n\\n\\nThe total score available for Component Number One is 100 points. \\n\\n\\n\\nMedical/Surgical, Behavioral Health, and Vision/Hearing are each scored separately, and combined they are worth 80 points. Med/Surg is weighted at 80% of total score for these elements (this is calculated by multiplying the number of earned points by .8). Behavioral Health is weighted at 15% of total score (this is calculated by multiplying the number of earned points by .15), and Vision/Hearing is scored together and weighted at 5% of total score (this is calculated by multiplying the number of earned points by .05).\\n\\n\\n\\nOfferors must complete and submit a separate Exhibit Two for each Component offered, recognizing that there may be a different mix of small business participation for each Component. Small business participation is scored as a single total for the Offeror, and is worth 20 points. \\n\\n\\n\\nCOMPONENT NUMBER ONE\\n\\n\\n\\n\\n\\nMedical/Surgical\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nPlan benefit administration capability, flexibility and innovation\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\n\\n\\nBehavioral Health\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nVision and Hearing\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nSmall Business Participation\\n\\n20 Points\\n\\n\\n\\nComponent Number Two—Statewide CDHP Medical/Surgical, Behavioral Health (to include EAP), Vision, and Hearing administrative services for state employee CDHP:\\n\\n\\n\\nThe total score available for Component Number Two is 100 points. \\n\\n\\n\\nMedical/Surgical, Behavioral Health, and Vision/Hearing are each scored separately, and combined they are worth 80 points. Med/Surg is weighted at 80% of total score for these elements (this is calculated by multiplying the number of earned points by .8). Behavioral Health is weighted at 15% of total score (this is calculated by multiplying the number of earned points by .15), and Vision/Hearing is scored together and weighted at 5% of total score (this is calculated by multiplying the number of earned points by .05).\\n\\n\\n\\nOfferors must complete and submit a separate Exhibit Two for each Component offered, recognizing that there may be a different mix of small business participation for each Component. Small business participation is scored as a single total for the Offeror, and is worth 20 points. \\n\\n\\n\\nCOMPONENT NUMBER TWO\\n\\n\\n\\n\\n\\nMedical/Surgical\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nPlan benefit administration capability, flexibility and innovation\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\n\\n\\nBehavioral Health\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nVision and Hearing\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nSmall Business Participation\\n\\n20 Points\\n\\n\\n\\nComponent Number Three—Statewide PPO, HDHP, and CDHP Prescription Drug administrative services for the state employee, TLC, and LODA plans:\\n\\n\\n\\nOfferors must complete and submit a separate Exhibit Two for each Component offered, recognizing that there may be a different mix of small business participation for each Component. Small business participation is scored as a single total for the Offeror, and is worth 20 points. \\n\\n\\n\\nCOMPONENT NUMBER THREE\\n\\n\\n\\nPrescription Drug\\n\\nPoints\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n10\\n\\nDemurrals\\n\\n10\\n\\nAdministrative capability and flexibility\\n\\n20\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\nSmall Business Participation\\n\\n20\\n\\n\\n\\nComponent Number Four—Statewide PPO, HDHP, and CDHP Dental administrative services for the state employee, TLC, and LODA plans\\n\\n\\n\\n\\n\\nThe total score available for Component Number Four is 100 points. \\n\\n\\n\\nOfferors must complete and submit a separate Exhibit Two for each Component offered, recognizing that there may be a different mix of small business participation for each Component. Small business participation is scored as a single total for the Offeror, and is worth 20 points. \\n\\n\\n\\n\\n\\n\\nCOMPONENT NUMBER FOUR\\n\\n\\n\\nDental\\n\\nPoints\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative capability and flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\nSmall Business Participation\\n\\n20\\n\\n\\n\\nComponent Number Five—Fully-Insured regional plans for state employee and TLC plans\\n\\n\\n\\nThe total score available for Component Number Five is 100 points. \\n\\n\\n\\nMedical/Surgical, Behavioral Health, Vision/Hearing, Prescription Drug, and Dental are each scored separately, and combined they are worth 80 points. Med/Surg is weighted at 60% of total score for these elements (this is calculated by multiplying the number of earned points by .6). Behavioral Health is weighted at 8% of total score (this is calculated by multiplying the number of earned points by .08), Vision/Hearing is scored together and weighted at 2% of total score (this is calculated by multiplying the number of earned points by .02), Prescription Drug is weighted at 20% of total score (this is calculated by multiplying the number of earned points by .2), and Dental is weighted at 10% of total score (this is calculated by multiplying the number of earned points by .1).\\n\\n\\n\\nOfferors must complete and submit a separate Exhibit Two for each Component offered, recognizing that there may be a different mix of small business participation for each Component. Small business participation is scored as a single total for the Offeror, and is worth 20 points. \\n\\n\\n\\nCOMPONENT NUMBER FIVE\\n\\n\\n\\n\\n\\nMedical/Surgical\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nPlan benefit administration capability, flexibility and innovation\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\n\\n\\nBehavioral Health\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nVision and Hearing\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nPrescription Drug\\n\\nPoints (Before Weighting)\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nProvider network \\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nDental\\n\\nPoints (Before Weighting\\n\\nOfferor’s organization and financial stability\\n\\n10\\n\\nQualifications of staff\\n\\n5\\n\\nProvider network\\n\\n15\\n\\nDemurrals\\n\\n10\\n\\nAdministrative Capability and Flexibility\\n\\n15\\n\\nBenefit cost management and administrative cost\\n\\n25\\n\\n\\n\\n\\n\\nSmall Business Participation\\n\\n20 Points\\n\\n\\n\\nComponent Number Six—Section 125 Flexible Spending Accounts administration for state employees\\n\\n\\n\\nThe total score available for Component Number Six is 100 points. \\n\\n\\n\\nOfferors must complete and submit a separate Exhibit Two for each Component offered, recognizing that there may be a different mix of small business participation for each Component. Small business participation is scored as a single total for the Offeror, and is worth 20 points. \\n\\n\\n\\nCOMPONENT NUMBER SIX\\n\\n\\n\\nSection 125 Flexible Spending Accounts administration\\n\\nPoints\\n\\nOfferor’s organization and financial stability \\n\\n10\\n\\nQualifications of staff \\n\\n5\\n\\nAdministrative capability and flexibility\\n\\n25\\n\\nDemurrals\\n\\n10\\n\\nAdministrative cost\\n\\n20\\n\\nIntegration with health plan payers\\n\\n10\\n\\nSmall Business Participation\\n\\n20\\n\\n\\n\\n\\n\\n\\n\\t7.0     GENERAL TERMS AND CONDITIONS  \\n\\n\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tA.\\tVENDORS MANUAL:  This solicitation is subject to the provisions of the Commonwealth of Virginia Vendors Manual and any changes or revisions thereto, which are hereby incorporated into this contract in their entirety.  The procedure for filing contractual claims is in section 7.19 of the Vendors Manual.  A copy of the manual is normally available for review at the purchasing office and is accessible on the Internet at www.eva.virginia.gov under “Vendors Manual” on the vendors tab.  \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tB.\\tAPPLICABLE LAWS AND COURTS: This solicitation and any resulting contract shall be governed in all respects by the laws of the Commonwealth of Virginia, without regard to its choice of law provisions, and any litigation with respect thereto shall be brought in the circuit courts of the Commonwealth.  The agency and the contractor are encouraged to resolve any issues in controversy arising from the award of the contract or any contractual dispute using Alternative Dispute Resolution (ADR) procedures (Code of Virginia, § 2.2-4366).  ADR procedures are described in Chapter 9 of the Vendors Manual.  The contractor shall comply with all applicable federal, state and local laws, rules and regulations.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tC.\\tANTI-DISCRIMINATION: By submitting their proposals, offerors certify to the Commonwealth that they will conform to the provisions of the Federal Civil Rights Act of 1964, as amended, as well as the Virginia Fair Employment Contracting Act of 1975, as amended, where applicable, the Virginians With Disabilities Act, the Americans With Disabilities Act and § 2.2-4311 of the Virginia Public Procurement Act (VPPA).  If the award is made to a faith-based organization, the organization shall not discriminate against any recipient of goods, services, or disbursements made pursuant to the contract on the basis of the recipient\\'s religion, religious belief, refusal to participate in a religious practice, or on the basis of race, age, color, gender or national origin and shall be subject to the same rules as other organizations that contract with public bodies to account for the use of the funds provided; however, if the faith-based organization segregates public funds into separate accounts, only the accounts and programs funded with public funds shall be subject to audit by the public body. (Code of Virginia, § 2.2-4343.1E).\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tIn every contract over $10,000 the provisions in 1. and 2. below apply:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.\\tDuring the performance of this contract, the contractor agrees as follows:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ta.\\tThe contractor will not discriminate against any employee or applicant for employment because of race, religion, color, sex, national origin, age, disability, or any other basis prohibited by state law relating to discrimination in employment, except where there is a bona fide occupational qualification reasonably necessary to the normal operation of the contractor.  The contractor agrees to post in conspicuous places, available to employees and applicants for employment, notices setting forth the provisions of this nondiscrimination clause.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tb.\\tThe contractor, in all solicitations or advertisements for employees placed by or on behalf of the contractor, will state that such contractor is an equal opportunity employer.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tc.\\tNotices, advertisements and solicitations placed in accordance with federal law, rule or regulation shall be deemed sufficient for the purpose of meeting the requirements of this section.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\td.\\tThe requirements of these provisions 1. and 2. are a material part of the contract.  If the Contractor violates one of these provisions, the Commonwealth may terminate the affected part of this contract for breach, or at its option, the whole contract.  Violation of one of these provisions may also result in debarment from State contracting regardless of whether the specific contract is terminated.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\te.\\tIn accordance with Executive Order 61 (2017), a prohibition on discrimination by the contractor, in its employment practices, subcontracting practices, and delivery of goods or services, on the basis of race, sex, color, national origin, religion, sexual orientation, gender identity, age, political affiliation, disability, or veteran status, is hereby incorporated in this contract.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t2.\\tThe contractor will include the provisions of 1. above in every subcontract or purchase order over $10,000, so that the provisions will be binding upon each subcontractor or vendor.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tD.\\tETHICS IN PUBLIC CONTRACTING: By submitting their proposals, offerors certify that their proposals are made without collusion or fraud and that they have not offered or received any kickbacks or inducements from any other offeror, supplier, manufacturer or subcontractor in connection with their proposal, and that they have not conferred on any public employee having official responsibility for this procurement transaction any payment, loan, subscription, advance, deposit of money, services or anything of more than nominal value, present or promised, unless consideration of substantially equal or greater value was exchanged.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tE.\\tIMMIGRATION REFORM AND CONTROL ACT OF 1986: Applicable for all contracts over $10,000: \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBy entering into a written contract with the Commonwealth of Virginia, the Contractor certifies that the Contractor does not, and shall not during the performance of the contract for goods and services in the Commonwealth, knowingly employ an unauthorized alien as defined in the federal Immigration Reform and Control Act of 1986.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tF.\\tDEBARMENT STATUS: By participating in this procurement, the vendor certifies that they are not currently debarred by the Commonwealth of Virginia from submitting a response for the type of goods and/or services covered by this solicitation. Vendor further certifies that they are not debarred from filling any order or accepting any resulting order, or that they are an agent of any person or entity that is currently debarred by the Commonwealth of Virginia.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tIf a vendor is created or used for the purpose of circumventing a debarment decision against another vendor, the non-debarred vendor will be debarred for the same time period as the debarred vendor.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tG.\\tANTITRUST: By entering into a contract, the contractor conveys, sells, assigns, and transfers to the Commonwealth of Virginia all rights, title and interest in and to all causes of action it may now have or hereafter acquire under the antitrust laws of the United States and the Commonwealth of Virginia, relating to the particular goods or services purchased or acquired by the Commonwealth of Virginia under said contract.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tH.\\tMANDATORY USE OF STATE FORM AND TERMS AND CONDITIONS FOR IFBs AND RFPs  \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.\\tFailure to submit a proposal on the official state form provided for that purpose may be a cause for rejection of the proposal.  Modification of or additions to the General Terms and Conditions of the solicitation may be cause for rejection of the proposal; however, the Commonwealth reserves the right to decide, on a case by case basis, in its sole discretion, whether to reject such a proposal.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tI.\\tCLARIFICATION OF TERMS: If any prospective offeror has questions about the specifications or other solicitation documents, the prospective offeror should contact the buyer whose name appears on the face of the solicitation no later than five working days before the due date.  Any revisions to the solicitation will be made only by addendum issued by the buyer.\\n\\n\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tJ.\\tPAYMENT:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.\\tTo Prime Contractor:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ta.\\tInvoices for items ordered, delivered and accepted shall be submitted by the contractor directly to the \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tpayment address shown on the purchase order/contract.  All invoices shall show the state contract number and/or purchase order number; social security number (for individual contractors) or the federal employer identification number (for proprietorships, partnerships, and corporations). \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tb.\\tAny payment terms requiring payment in less than 30 days will be regarded as requiring payment 30 days after invoice or delivery, whichever occurs last.  This shall not affect offers of discounts for payment in less than 30 days, however.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tc.\\tAll goods or services provided under this contract or purchase order, that are to be paid for with public funds, shall be billed by the contractor at the contract price, regardless of which public agency is being billed.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\td.\\tThe following shall be deemed to be the date of payment: the date of postmark in all cases where payment is made by mail, or when offset proceedings have been instituted as authorized under the Virginia Debt Collection Act.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\te.\\tUnreasonable Charges.  Under certain emergency procurements and for most time and material purchases, final job costs cannot be accurately determined at the time orders are placed.  In such cases, contractors should be put on notice that final payment in full is contingent on a determination of reasonableness with respect to all invoiced charges.  Charges which appear to be unreasonable will be resolved in accordance with Code of Virginia, § 2.2-4363 and -4364.  Upon determining that invoiced charges are not reasonable, the Commonwealth shall notify the contractor of defects or improprieties in invoices within fifteen (15) days as required in Code of Virginia, § 2.2-4351.,.  The provisions of this section do not relieve an agency of its prompt payment obligations with respect to those charges which are not in dispute (Code of Virginia, § 2.2-4363).\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t2.\\tTo Subcontractors:  \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ta.\\tWithin seven (7) days of the contractor’s receipt of payment from the Commonwealth, a contractor awarded a contract under this solicitation is hereby obligated:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(1)\\tTo pay the subcontractor(s) for the proportionate share of the payment received for work performed by the subcontractor(s) under the contract; or\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(2)\\tTo notify the agency and the subcontractor(s), in writing, of the contractor’s intention to withhold payment and the reason.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tb.\\tThe contractor is obligated to pay the subcontractor(s) interest at the rate of one percent per month (unless otherwise provided under the terms of the contract) on all amounts owed by the contractor that remain unpaid seven (7) days following receipt of payment from the Commonwealth, except for amounts withheld as stated in (2) above.  The date of mailing of any payment by U. S. Mail is deemed to be payment to the addressee.  These provisions apply to each sub-tier contractor performing under the primary contract.  A contractor’s obligation to pay an interest charge to a subcontractor may not be construed to be an obligation of the Commonwealth.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n3.\\tEach prime contractor who wins an award in which provision of a SWaM procurement plan is a condition to the award, shall deliver to the contracting agency or institution, on or before request for final payment, evidence and certification of compliance (subject only to insubstantial shortfalls and to shortfalls arising from subcontractor default) with the Small Business Participation procurement plan.  Final payment under the contract in question may be withheld until such certification is delivered and, if necessary, confirmed by the agency or institution, or other appropriate penalties may be assessed in lieu of withholding such payment.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n4.\\tThe Commonwealth of Virginia encourages contractors and subcontractors to accept electronic and credit card payments.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tK.\\t\\tPRECEDENCE OF TERMS: The following General Terms and Conditions VENDORS MANUAL, APPLICABLE LAWS AND COURTS, ANTI-DISCRIMINATION, ETHICS IN PUBLIC CONTRACTING, IMMIGRATION REFORM AND CONTROL ACT OF 1986, DEBARMENT STATUS, ANTITRUST, MANDATORY USE OF STATE FORM AND TERMS AND CONDITIONS, CLARIFICATION OF TERMS, PAYMENT shall apply in all instances.  In the event there is a conflict between any of the other General Terms and Conditions and any Special Terms and Conditions in this solicitation, the Special Terms and Conditions shall apply.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tL.\\tQUALIFICATIONS OF OFFERORS: The Commonwealth may make such reasonable investigations as deemed proper and necessary to determine the ability of the offeror to perform the services/furnish the goods and the offeror shall furnish to the Commonwealth all such information and data for this purpose as may be requested.  The Commonwealth reserves the right to inspect offeror’s physical facilities prior to award to satisfy questions regarding the offeror’s capabilities.  The Commonwealth further reserves the right to reject any proposal if the evidence submitted by, or investigations of, such offeror fails to satisfy the Commonwealth that such offeror is properly qualified to carry out the obligations of the contract and to provide the services and/or furnish the goods contemplated therein.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tM.\\tTESTING AND INSPECTION: The Commonwealth reserves the right to conduct any test/inspection it may deem advisable to assure goods and services conform to the specifications.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tN.\\tASSIGNMENT OF CONTRACT: A contract shall not be assignable by the contractor in whole or in part without the written consent of the Commonwealth.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tO.\\tCHANGES TO THE CONTRACT: Changes can be made to the contract in any of the following ways:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1.\\tThe parties may agree in writing to modify the terms, conditions, or scope of the contract.\\xa0 Any additional goods or services to be provided shall be of a sort that is ancillary to the contract goods or services, or within the same broad product or service categories as were included in the contract award.\\xa0 Any increase or decrease in the price of the contract resulting from such modification shall be agreed to by the parties as a part of their written agreement to modify the scope of the contract.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t2.\\tThe Purchasing Agency may order changes within the general scope of the contract at any time by written notice to the contractor.  Changes within the scope of the contract include, but are not limited to, things such as services to be performed, the method of packing or shipment, and the place of delivery or installation.  The contractor shall comply with the notice upon receipt, unless the contractor intends to claim an adjustment to compensation, schedule, or other contractual impact that would be caused by complying with such notice, in which case the contractor shall, in writing, promptly notify the Purchasing Agency of the adjustment to be sought, and before proceeding to comply with the notice, shall await the Purchasing Agency\\'s written decision affirming, modifying, or revoking the prior written notice.\\xa0 If the Purchasing Agency decides to issue a notice that requires an adjustment to compensation, the contractor shall be compensated for any additional costs incurred as the result of such order and shall give the Purchasing Agency a credit for any savings.  Said compensation shall be determined by one of the following methods:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ta.\\tBy mutual agreement between the parties in writing; or\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tb.\\tBy agreeing upon a unit price or using a unit price set forth in the contract, if the work to be done can be expressed in units, and the contractor accounts for the number of units of work performed, subject to the Purchasing Agency’s right to audit the contractor’s records and/or to determine the correct number of units independently; or\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tc.\\tBy ordering the contractor to proceed with the work and keep a record of all costs incurred and savings realized.  A markup for overhead and profit may be allowed if provided by the contract.  The same markup shall be used for determining a decrease in price as the result of savings realized.  The contractor shall present the Purchasing Agency with all vouchers and records of expenses incurred and savings realized.  The Purchasing Agency shall have the right to audit the records of the contractor as it deems necessary to determine costs or savings.  Any claim for an adjustment in price under this provision must be asserted by written notice to the Purchasing Agency within thirty (30) days from the date of receipt of the written order from the Purchasing Agency.  If the parties fail to agree on an amount of adjustment, the question of an increase or decrease in the contract price or time for performance shall be resolved in accordance with the procedures for resolving disputes provided by the Disputes Clause of this contract or, if there is none, in accordance with the disputes provisions of the Commonwealth of Virginia Vendors Manual.  Neither the  existence of a claim nor a dispute resolution process, litigation or any other provision of this contract shall excuse the contractor from promptly complying with the changes ordered by the Purchasing Agency or with the performance of the contract generally.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tP.\\tDEFAULT: In case of failure to deliver goods or services in accordance with the contract terms and conditions, the Commonwealth, after due oral or written notice, may procure them from other sources and hold the contractor responsible for any resulting additional purchase and administrative costs.  This remedy shall be in addition to any other remedies which the Commonwealth may have.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tQ.\\tTAXES: (NOT APPLICABLE) Sales to the Commonwealth of Virginia are normally exempt from State sales tax.  State sales and use tax certificates of exemption, Form ST-12, will be issued upon request.  Deliveries against this contract shall usually be free of Federal excise and transportation taxes.  The Commonwealth’s excise tax exemption registration number is 54-73-0076K.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tIf sales or deliveries against the contract are not exempt, the contractor shall be responsible for the payment of such taxes unless the tax law specifically imposes the tax upon the buying entity and prohibits the contractor from offering a tax-included price. \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tR.\\tUSE OF BRAND NAMES: (NOT APPLICABLE) Unless otherwise provided in this solicitation, the name of a certain brand, make or manufacturer does not restrict offeror to the specific brand, make or manufacturer named, but conveys the general style, type, character, and quality of the article desired.  Any article which the public body, in its sole discretion, determines to be the equivalent of that specified, considering quality, workmanship, economy of operation, and suitability for the purpose intended, shall be accepted.  The offeror is responsible to clearly and specifically identify the product being offered and to provide sufficient descriptive literature, catalog cuts and technical detail to enable the Commonwealth to determine if the product offered meets the requirements of the solicitation.  This is required even if offering the exact brand, make or manufacturer specified.  Normally in competitive sealed bidding only the information furnished with the bid will be considered in the evaluation.  Failure to furnish adequate data for evaluation purposes may result in declaring a bid nonresponsive.  Unless the offeror clearly indicates in its proposal that the product offered is an equivalent product, such proposal will be considered to offer the brand name product referenced in the solicitation.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tS.\\tTRANSPORTATION AND PACKAGING: (NOT APPLICABLE) By submitting their proposals, all offerors certify and warrant that the price offered for FOB destination includes only the actual freight rate costs at the lowest and best rate and is based upon the actual weight of the goods to be shipped.  Except as otherwise specified herein, standard commercial packaging, packing and shipping containers shall be used.  All shipping containers shall be legibly marked or labeled on the outside with purchase order number, commodity description, and quantity.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tT.\\tINSURANCE: By signing and submitting a proposal under this solicitation, the offeror certifies that if awarded the contract, it will have the following insurance coverage at the time the contract is awarded.  For construction contracts, if any subcontractors are involved, the subcontractor will have workers’ compensation insurance in accordance with §§ 2.2-4332 and 65.2-800 et seq. of the Code of Virginia.  The offeror further certifies that the contractor and any subcontractors will maintain these insurance coverage during the entire term of the contract and that all insurance coverage will be provided by insurance companies authorized to sell insurance in Virginia by the Virginia State Corporation Commission.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t MINIMUM INSURANCE COVERAGES AND LIMITS:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tWorkers’ Compensation - Statutory requirements and benefits.  Coverage is compulsory for employers of three or more employees, to include the employer. Contractors who fail to notify the Commonwealth of increases in the number of employees that change their workers’ compensation requirements under the Code of Virginia during the course of the contract shall be in noncompliance with the contract.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t2.\\tEmployer’s Liability - $100,000.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCommercial General Liability - $1,000,000 per occurrence and $2,000,000 in the aggregate.  Commercial General Liability is to include bodily injury and property damage, personal injury and advertising injury, products and completed operations coverage.  The Commonwealth of Virginia must be named as an additional insured and so endorsed on the policy.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAutomobile Liability - $1,000,000 combined single limit. (Required only if a motor vehicle not owned by the Commonwealth is to be used in the contract.\\xa0 Contractor must assure that the required coverage is maintained by the Contractor (or third party owner of such motor vehicle.)\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\tProfession/Service\\tLimits\\n\\n\\t\\t\\t\\tAccounting\\t$1,000,000 per occurrence, $3,000,000 aggregate\\n\\n\\t\\t\\t\\tArchitecture\\t$2,000,000 per occurrence, $6,000,000 aggregate\\n\\n\\t\\t\\t\\tAsbestos Design, Inspection \\n\\n\\t\\t\\t\\tor Abatement Contractors\\t$1,000,000 per occurrence, $3,000,000 aggregate\\n\\n\\t\\t\\t\\tHealth Care Practitioner (to include \\n\\n\\t\\t\\t\\t\\tDentists, Licensed Dental\\n\\n\\t\\t\\t\\t\\tHygienists, Optometrists, \\n\\n\\t\\t\\t\\t\\tRegistered or Licensed \\n\\n\\t\\t\\t\\t\\tPractical Nurses, Pharmacists, \\n\\n\\t\\t\\t\\t\\tPhysicians, Podiatrists, \\n\\n\\t\\t\\t\\t\\tChiropractors, Physical \\n\\n\\t\\t\\t\\t\\tTherapists, Physical Therapist \\n\\n\\t\\t\\t\\t\\tAssistants, Clinical Psychologists,\\n\\n\\t\\t\\t\\t\\tClinical Social Workers, \\n\\n\\t\\t\\t\\t\\tProfessional Counselors, \\n\\n\\t\\t\\t\\t\\tHospitals, or Health Maintenance \\n\\n\\t\\t\\t\\t\\tOrganizations.) \\t$2,150,000 per occurrence, $4,250,000 aggregate \\n\\n\\t\\t\\t\\t(Limits increase each July 1 through fiscal year 2031per Code of Virginia § 8.01-581.15.)\\n\\n\\t\\t\\t\\tInsurance/Risk Management\\t$1,000,000 per occurrence, $3,000,000 aggregate\\n\\n\\t\\t\\t\\tLandscape/Architecture\\t\\t$1,000,000 per occurrence, $1,000,000 aggregate\\n\\n\\t\\t\\t\\t\\tLegal\\t$1,000,000 per occurrence, $5,000,000 aggregate\\n\\n\\t\\t\\t\\t\\tProfessional Engineer\\t$2,000,000 per occurrence, $6,000,000 aggregate\\n\\n\\t\\t\\t\\t\\tSurveying\\t$1,000,000 per occurrence, $1,000,000 aggregate\\n\\n\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\tU. \\tANNOUNCEMENT OF AWARD:  Upon the award or the announcement of the decision to award a contract as a result of this solicitation, the purchasing agency will publicly post such notice on the DGS/DPS eVA VBO (www.eva.virginia.gov) for a minimum of 10 days.\\n\\n\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tV.\\tDRUG-FREE WORKPLACE: Applicable for all contracts over $10,000: \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tDuring the performance of this contract, the contractor agrees to (i) provide a drug-free workplace for the contractor\\'s employees; (ii) post in conspicuous places, available to employees and applicants for employment, a statement notifying employees that the unlawful manufacture, sale, distribution, dispensation, possession, or use of a controlled substance or marijuana is prohibited in the contractor\\'s workplace and specifying the actions that will be taken against employees for violations of such prohibition; (iii) state in all solicitations or advertisements for employees placed by or on behalf of the contractor that the contractor maintains a drug-free workplace; and (iv) include the provisions of the foregoing clauses in every subcontract or purchase order of over $10,000, so that the provisions will be binding upon each subcontractor or vendor. \\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFor the purposes of this section, “drug-free workplace” means a site for the performance of work done in connection with a specific contract awarded to a contractor, the employees of whom are prohibited from engaging in the unlawful manufacture, sale, distribution, dispensation, possession or use of any controlled substance or marijuana during the performance of the contract.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tW.\\tNONDISCRIMINATION OF CONTRACTORS:  A offeror, or contractor shall not be discriminated against in the solicitation or award of this contract because of race, religion, color, sex, national origin, age, disability, faith-based organizational status, any other basis prohibited by state law relating to discrimination in employment or because the offeror employs ex-offenders unless the state agency, department or institution has made a written determination that employing ex-offenders on the specific contract is not in its best interest.  If the award of this contract is made to a faith-based organization and an individual, who applies for or receives goods, services, or disbursements provided pursuant to this contract objects to the religious character of the faith-based organization from which the individual receives or would receive the goods, services, or disbursements, the public body shall offer the individual, within a reasonable period of time after the date of his objection, access to equivalent goods, services, or disbursements from an alternative provider.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\tX.\\teVA BUSINESS-TO-GOVERNMENT VENDOR REGISTRATION, CONTRACTS, AND ORDERS: The eVA Internet electronic procurement solution, web site portal www.eVA.virginia.gov, streamlines and automates government purchasing activities in the Commonwealth.  The eVA portal is the gateway for vendors to conduct business with state agencies and public bodies.  All vendors desiring to provide goods and/or services to the Commonwealth shall participate in the eVA Internet e-procurement solution by completing the free eVA Vendor Registration.  All offerors must register in eVA and pay the Vendor Transaction Fees specified below; failure to register will result in the proposal being rejected.\\n\\n\\t\\t\\n\\nVendor transaction fees are determined by the date the original purchase order is issued and the current fees are as follows:\\n\\n\\t\\t\\n\\n\\t\\tFor orders issued July 1, 2014, and after, the Vendor Transaction Fee is:\\n\\n\\t\\t\\n\\n\\t\\tDSBSD-certified Small Businesses: 1%, capped at $500 per order.\\n\\n\\t\\tBusinesses that are not DSBSD-certified Small Businesses: 1%, capped at $1,500 per order.\\n\\n\\t\\t\\n\\n\\t\\tRefer to Special Term and Condition “eVA Orders and Contracts” to identify the number of purchase orders that will be issued as a result of this solicitation/contract with the eVA transaction fee specified above assessed for each order.\\n\\n\\t\\t\\n\\n\\t\\tFor orders issued prior to July 1, 2014, the vendor transaction fees can be found at www.eVA.virginia.gov.\\n\\n\\t\\t\\n\\n\\t\\tThe specified vendor transaction fee will be invoiced, by the Commonwealth of Virginia Department of General Services, typically within 60 days of the order issue date.  Any adjustments (increases/decreases) will be handled through purchase order changes.\\n\\n\\t\\n\\nY.\\tAVAILABILITY OF FUNDS:  It is understood and agreed between the parties herein that the agency shall be bound hereunder only to the extent that the legislature has appropriated funds that are legally available or may hereafter become legally available for the purpose of this agreement.\\n\\n\\n\\n\\tZ.\\tSET-ASIDES IN ACCORDANCE WITH THE SMALL BUSINESS ENHANCEMENT AWARD PRIORITY:   This solicitation is set-aside for award priority to DSBSD-certified micro businesses or small businesses when designated as “Micro Business Set-Aside Award Priority” or “Small Business Set-Aside Award Priority” accordingly in the solicitation.  DSBSD-certified micro businesses or small businesses also include DSBSD-certified women-owned and minority-owned businesses when they have received the DSBSD small business certification.  For purposes of award, offerors shall be deemed micro businesses or small businesses if and only if they are certified as such by DSBSD on the due date for receipt of proposals.\\n\\n\\t\\t\\n\\nAA. \\tBID PRICE CURRENCY:  Unless stated otherwise in the solicitation, offerors shall state offer prices in US dollars.\\n\\n\\n\\nBB.\\tAUTHORIZATION TO CONDUCT BUSINESS IN THE COMMONWEALTH:  A contractor organized as a stock or nonstock corporation, limited liability company, business trust, or limited partnership or registered as a registered limited liability partnership shall be authorized to transact business in the Commonwealth as a domestic or foreign business entity if so required by Title 13.1 or Title 50 of the Code of Virginia or as otherwise required by law. Any business entity described above that enters into a contract with a public body pursuant to the Virginia Public Procurement Act shall not allow its existence to lapse or its certificate of authority or registration to transact business in the Commonwealth, if so required under Title 13.1 or Title 50, to be revoked or cancelled at any time during the term of the contract. A public body may void any contract with a business entity if the business entity fails to remain in compliance with the provisions of this section.\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t8.0  \\tSPECIAL TERMS AND CONDITIONS\\n\\n\\n\\n\\t8.1\\tCOST LIMITS\\n\\n\\t\\t\\n\\n\\tThe Contractor is responsible for all the costs of implementing and administering the program.  The Department is responsible for ensuring that the Contractor receives payment of all fees that are established pursuant to the contract which results from this RFP.  Any cost incurred by the Contractor to address the tasks and responsibilities identified in this RFP which exceeds the contractually established fees is the risk of the Contractor.\\n\\n\\t\\n\\n\\t\\tRENEWAL OF CONTRACT\\n\\n\\t\\n\\n\\t\\tThe term of this contract is three years with four one-year renewal options.  For the one-year renewal options, the contract may renew annually subject to the following.\\n\\n\\n\\n\\t8.2.1\\tThe Contractor shall advise the Department in writing no later than 2:00 PM on the last business day before September 16 that the insurer is willing to renew the contract on the same terms and conditions as currently in force or as modified pursuant to a request from the Department. This advice shall be in the form of a proposal which meets the requirements of Section 6, except that the submission of tabs 1 and 2 are necessary only to the extent that there are changes from the original proposal.  Selected tab 5 detail is required with each renewal. \\n\\n\\t\\n\\n\\t8.2.2\\tAll Contractors require a finding by the Department that the Contractor’s performance has been satisfactory.  Such findings are within the sole discretion of the Department but will be based on materially important issues such as the plan’s accreditation status (if applicable), employee satisfaction, and the amount of liquidated damages due the Department because of failure of the Contractor to meet standards. \\t\\t\\n\\n\\t\\n\\n\\tIf the Commonwealth elects to exercise the option to renew the contract for an additional one-year period, the contract price for the additional one year shall not exceed the contract price of the original increased/decreased by more than the percentage increase/decrease of the services category of the CPI-W section of the Consumer Price Index of the United States Bureau of Labor Statistics for the latest twelve months for which statistics are available.\\n\\n\\t\\n\\n\\tIf during any subsequent renewal periods, the Commonwealth elects to exercise the option to renew the contract, the contract price for the subsequent renewal period shall not exceed the contract price of the previous renewal period increased/decreased by more than the percentage increased/decreased of the services category of the CPI-W section of the Consumer Price Index of the United States Bureau of labor Statistics for the latest twelve months for which statistics are available.\\n\\n\\t\\n\\n\\t\\t8.3 \\tTERMINATION, SUSPENSION AND CANCELLATION OF CONTRACT\\n\\n\\t\\n\\n\\t\\tEither party may terminate this contract for its sole convenience effective July 1 of any year by delivery of written notice at least nine months prior to the effective date of cancellation, that is, by the previous September 1.  Some school groups in the TLC program have plan years ending on September 30th.  Therefore, it is agreed that for any Contractor having enrollment in one or more of these school groups, the termination of this contract as applied to the particular school group will be effective September 30 following the July 1 termination date of the contract.\\n\\n\\t\\n\\n\\t\\tIf the Department determines, in its sole discretion, that limiting additional enrollment would enhance the administration of this contract, the Department may limit enrollment or suspend entirely new enrollments by a written order to the Contractor.\\n\\n\\t\\n\\n\\t\\tFurthermore, in the event of emergency requirements or significant changes in the Contractor’s financial or organizational status which could not have reasonably been foreseen, the Department reserves the right to cancel and terminate this contract, in part or in whole without penalty, upon 60 days written notice to the Contractor.  \\n\\n\\t\\n\\n\\t\\tAny contract cancellation notice shall not relieve the Contractor of the obligation to deliver and/or perform on all outstanding orders issued prior to the effective date of cancellation.\\n\\n\\t\\n\\n\\t\\t8.4     PAYMENTS AND INTEREST\\n\\n\\n\\n\\t\\tState Employee, TLC, and LODA Health Benefit Plans - The Department will send or make available (through the internet) to each Contractor an electronic file of changes in eligible enrollees and eligible dependents in a form to be mutually agreed upon on a daily or other basis as may be mutually agreeable. Contractor agrees that the Department shall be the only official source for any eligibility file maintained by the Contractor for any claims payment made by the Contractor, unless the Department agrees to changes in writing.  \\n\\n\\t\\n\\n\\t\\tIf the plan is an insured plan, the Department will provide a premium payment sufficient to pay for the coverage of persons determined to be eligible by the Department. This payment will be made to the Contractor no later than the tenth business day of the month for which coverage is effective.  The premium payment will reflect appropriate retroactive adjustments.  The Contractor will pay claims or provide services only for persons determined to be eligible by the Department.\\n\\n\\t\\t\\t\\n\\n\\t\\t\\tThe Local Choice Program – Bills for premiums shall be submitted by the Medical/Surgical Contractor to each local employer that has one or more active employees enrolled in the plan.  Billing shall be based on information provided through the daily file generated by the Department’s eligibility system in a form acceptable to the Department.  The bill shall be provided by the tenth day of the month prior to the month for which the premium is being billed.  The Medical/Surgical Contractor shall receive payments, which are due by the first of the coverage month with a ten-day grace period per the Administrative Code of Virginia.  If the timely premium is not received, the Medical/Surgical Contractor shall advise the Department to facilitate notification to the local employer of the consequences of non-payment. \\n\\n\\t\\n\\n\\tThe Medical/Surgical Contractor shall generate a bill directly to local employers’ retirees and/or survivors and/or COBRA participants as indicated on the eligibility file per the request of the local employer to the Department.  Due dates for premium payments will be at the direction of the Department or as required by law, and the Contractor will notify the Department and/or local employer when timely payment has not been made by the direct bill recipient.  \\n\\n\\t\\n\\n\\tThe Medical/Surgical Contractor shall adjust, in the next billing cycle, group and direct premium billing to reflect any retroactive change in enrollment reported on the daily eligibility file.  New or updated ID cards resulting from any membership change or addition shall be generated promptly upon receipt of the eligibility file.  The plan will not pay claims or provide services starting with the first of the month after the unpaid coverage month when group premiums are not paid by the twentieth day of the coverage month, as directed by the Department.  The Contractor will suspend claim payments on the tenth day of the month for which any direct bill participant has failed to pay the premium payment.  Payment of claims will resume when the direct bill premium is paid within the grace period.  If premium is not paid within the grace period, coverage will be terminated.\\n\\n\\t\\t\\n\\n\\t\\tNote:  The Medical/Surgical Contractor will provide the ability for large (as determined by the Department) TLC groups to pay their monthly premiums utilizing wire transfers or other electronic transfers of funds.\\n\\n\\t\\n\\n\\t8.4.3\\tEach Contractor will bill the Department for claims payments on a weekly basis and for administrative costs on a monthly basis.  The Department will pay, subject to verification, the Contractor for services rendered.  The form of the bills and the schedule of payments shall be acceptable to both parties.  The plan will pay claims or provide services only for persons determined to be eligible by the Department.\\n\\n\\t\\n\\n\\t8.4.4   \\tThe standard form of payment utilized by the Commonwealth is by EDI (See Appendix 9 for description).  Unless a different method is agreed upon through negotiations, each Contractor must complete the EDI agreements required by the Department of Accounts.\\n\\n\\t\\n\\n\\t8.4.5   \\tRetroactive Adjustments\\n\\n\\t\\n\\n\\t\\tWhere the Department discovers an error in enrollment for which the Contractor has no responsibility, Contractor agrees to correct such an error retroactively up to a period of eighteen months from the date on which the error is discovered. \\n\\n\\n\\n\\t\\tCOBRA Eligibles and Direct Bill Retirees\\n\\n\\t\\n\\n\\t           \\tFor state employee and TLC groups, the Medical/Surgical Contractor agrees to bill certain Extended Coverage (COBRA) enrollees and certain retiree group participants designated by the Department for premiums.  Plans shall submit a listing of any retirees who have failed to pay their monthly premium within 15 calendar days or COBRA participants who failed to pay their monthly premium within five business days of the end of the month for which premium has not been paid, identified by alternate ID number.  Billing format will comply with Department policy and/or governing law. \\n\\n\\t \\n\\n\\t\\tThe ASO Medical/Surgical plan shall report those collections on the Monthly Income Report. \\n\\n\\t\\t\\n\\n\\t8.4.7 \\tSettlement and Payment of Liquidated Damages\\n\\n\\t\\n\\n\\t\\tThere shall be an annual settlement between the Contractor and the Department on or before November 30th, unless both parties agree to an extension. The settlement agreement shall provide for the final settlement of contract expenses, including liquidated damages.  It is mutually agreed that liquidated damages, if any, shall be determined by reference to claims incurred for the fiscal year in settlement and paid through the 30th of September following the close of that year.  Amounts owed to either party shall be paid within 30 days of settlement.  Late payments by either party are subject to interest at 1% per month on the unpaid balance, such that interest is due and payable on the 31st day following the date of settlement for the 30 days the balance would have remained unpaid.  The settlement agreement shall specify the last business day on which timely payment may be made.\\n\\n\\t\\n\\n\\t8.4.8 \\tInterest\\n\\n\\t\\t\\t\\n\\n\\t\\tA Contractor shall pay the Department interest on all funds held by the Contractor for the Department, including check float.  The Department will bargain in good faith with respect to the total structure of the financial arrangements such that the Contractor and the Department are both protected against the untimely payment of amounts due, including weekly claims reimbursements.\\n\\n\\t\\n\\n\\t\\n\\n\\n\\n\\n\\t8.5 \\tSERVICES \\n\\n\\t\\n\\n\\tThe Contractor shall deliver only those services actually ordered by the Department. The Department will accept and pay only for those services which have been fully rendered.  The Contractor shall invoice the Department each month for services provided during the prior month.  Payment will be made by the Department within 30 days of receipt of an approved invoice by the Commonwealth’s EDI payment method.  \\n\\n\\t\\n\\n\\tThe Offerors shall propose premiums in accordance with the following provisions. \\n\\n\\n\\n\\tInsured plans shall establish total premiums in accordance with their own procedures.  Each year, the insured plan(s) shall provide the total premium schedule to the Department by October 15th of the year prior to the plan year for which the premium schedule applies. The Department shall determine the employer and employee contribution amounts.  Notice of any change in premiums shall be accomplished using the Premium Buildup form referenced in subsection 8.4.1.\\n\\n\\n\\n\\tThe Department retains the right to establish premiums for each ASO plan.  In establishing such premiums, the Department will consider the Contractor’s proposal, the relative age and gender of the covered population, the administrative costs of the Department, the relative efficiency of the plan’s provider networks, the prices the plan pays for services, the plan’s administrative costs, and such other factors as may be relevant.\\n\\n\\n\\n\\tSurcharges\\n\\n\\n\\n\\tAll plans shall participate in the costs of the Department in the administration of the state employee health benefits program, the TLC program, and the LODA health benefits plans and in the costs of activities which benefit the insureds of the plans, including but not limited to annual enrollment and CommonHealth, the Department’s work site health promotion program.  All rate projections should include a surcharge of 2% to recognize these costs. \\n\\n\\n\\n\\tFor the state employee group plans, all insured plans will be paid 98% of the agreed upon premium.  \\n\\n\\n\\n\\tFor the TLC program, all insured plans will be required to reimburse the Department, no less frequently than each quarter, at 2% of the total monthly premium paid by each group having enrollees under the insured plan. This 2% calculation should be included in any rate buildup.  \\n\\n\\t\\n\\n\\t8.6     AUDITS \\n\\n\\t\\n\\nSome standards of performance under this contract shall be measured by audits.  Results of claims audits shall be extrapolated to the universe of claims being audited, and the Contractor’s performance with respect to the universe of claims shall be deemed to be the same as the Contractor’s performance on the sample of claims, provided that the audit sample was randomly drawn and statistically valid (+/-3% error rate at 95% confidence level).\\n\\n\\n\\nThe Contractor shall assist the Department and the Department’s auditors, who may be employees of the Department, employees of other Contractors, or agents of the Department, in the conduct of audits.  This assistance shall include the provision of secure, quiet office space, including furnishings and telephones needed by the auditors.\\n\\n\\n\\nThe Contractor agrees to retain all books, records, and other documents relative to the contract which results from this RFP for five (5) years after final payment, or until the conclusion of any audit by the Commonwealth, whichever is sooner.  The Department, its authorized agents, and State Auditors, shall have full access to, and the right to examine, any of the Contractor’s materials relevant to the contract which results from this RFP.\\n\\n\\n\\n8.7   \\tCONTRACT REPRESENTATIVES \\n\\n\\t\\n\\n\\tBoth the Department and the Contractor shall appoint a contract representative who shall ensure that the provisions of this contract are adhered to.  \\n\\n\\t\\n\\n\\tThe Contractor shall provide the full name and address of their contract representative including telephone and fax number.  In the event of a change in contract representatives, an official written notice shall be provided within 15 days of the change.\\n\\n\\t\\n\\n\\t\\tThe Department reserves the right, after adequate notice, to require the Contractor to replace any members of the State, TLC, Or LODA health plans’ dedicated account team.\\n\\n\\n\\n\\t8.8    \\tCERTIFIED CORPORATE ANNUAL REPORTS \\n\\n\\t\\n\\nWithin 120 days of the close of its fiscal year, the Contractor shall furnish to the Department an annual report of its consolidated operations. This report shall be certified by an independent auditor. In the event that the Contractor provides some part(s) of the total component through subcontractor(s), reports from all partners of the Contractor are required.\\n\\n\\t\\n\\n8.9    \\tCONFIDENTIALITY OF INFORMATION\\n\\n\\t\\n\\nThe Contractor shall treat all information utilized in its performance of the contract as confidential, personal information.  The Contractor shall handle all confidential information in accordance with the Virginia Privacy Protection Act, Virginia Code Section 2.1-377 et seq and with the privacy and security provisions of the Health Insurance Portability and Accountability ACT (HIPAA).  All files, computer data bases and other records developed or maintained pursuant to the execution of the contract are the property of the Department, and shall be delivered to the Department upon demand.  The Contractor merely serves as the custodian of the files, and acts as agent for the Department in the payment for services and the performance of other assigned tasks, including assisting the Department with requests under the Virginia Freedom of Information Act. \\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t8.10   \\tCOMMISSIONS AND BROKERAGE FEES\\n\\n\\t\\n\\n\\tThe Contractor agrees that, in the performance of this contract, no payments shall be made to brokers or sales persons who are not employees of the Contractor.\\n\\n\\n\\n\\t8.11    SEVERABILITY \\n\\n\\t\\n\\n\\tIn the event any portion of the contract shall be determined by a court of competent jurisdiction to be invalid or unenforceable, such provision shall be deemed void and the remainder of the contract shall continue in full force and effect.\\n\\n\\t\\n\\n\\t8.12\\tELIGIBILITY\\n\\n\\t\\n\\n\\tThe Department shall determine who is eligible for the Health Benefits programs.\\n\\n\\n\\n\\t8.13 \\tEMPLOYER CONTRIBUTIONS TOWARDS PREMIUMS\\n\\n\\t\\n\\n\\tThe Department shall set the employer contribution for all state health benefits plans. TLC employers set their own employer contribution levels within specified parameters.  \\n\\n\\t\\n\\n\\t\\tFORCE MAJEURE \\n\\n\\t\\n\\nNeither party shall be deemed to be in default of any of its obligations hereunder, if, and so long as, it is prevented from performing such obligations by an act of war, hostile foreign action, nuclear explosion, earthquake, hurricane, tornado, or other catastrophic natural event or act of God.\\n\\n\\t\\n\\n\\t8.15\\tINTERNET SITE\\n\\n\\t\\n\\n\\tContractor agrees to maintain dedicated Internet sites devoted to enrollees covered under the employee health benefits program, TLC, and LODA.  As a minimum, the sites shall contain the following.\\n\\n\\n\\n\\t\\tif applicable, a link to the Contractor’s current provider directory with a capability to locate providers by geographic locations and type of practice\\n\\n\\t\\n\\n\\t8.15.2\\tthe data specified in subsection 6.3.\\n\\n\\t\\t\\n\\n\\t8.15.3\\tan outline of coverage\\n\\n\\t\\n\\n\\t8.15.4\\tother information about the plan\\n\\n\\t\\n\\n\\t8.15.5 \\tsecure access to claims information by participants\\n\\n\\t\\n\\n\\t8.15.6 \\taccess to comparison cost and quality information for participating providers.\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t8.16\\tSUBCONTRACTING \\n\\n\\t\\n\\n\\tThe Contractor is fully responsible for all work performed under the contract.  The Contractor may not assign, transfer, or subcontract any interest in the contract, without prior written approval of the Department.  The Contractor shall require all subcontractors to comply with all provisions of this RFP.  The Contractor will be held liable for contract compliance for all duties and functions whether performed by the Contractor or any subcontractor.\\n\\n\\t\\tDISPUTES \\n\\n\\n\\nIn accordance with section 2.2-4363 of the Code of Virginia, disputes arising out of the contract, whether for money or other relief, may be submitted by the Contractor for consideration by the Department.  Disputes must be submitted in writing, with all necessary data and information, to the Director of the Department of Human Resource Management at the James Monroe Building, 12th Floor, 101 North 14th Street, Richmond, Virginia 23219.  Disputes will not be considered if submitted later than sixty (60) days after the final payment is made by the Department under the contract.  Further, no claim may be submitted unless written notice of the Contractor’s intention to file the dispute has been submitted at the time of the occurrence or at the beginning of the work upon which the dispute is based.  The Department shall render a final written decision regarding the dispute not more than ninety (90) days after the dispute is submitted, unless the parties agree to an extension of time.  If the Department does not render its decision within 90 days, the Contractor’s sole remedy will be to institute legal action, pursuant to section 2.2-436411-70 of the Code of Virginia.  The Contractor shall not be granted relief as a result of any delay in the Department’s decision.  During the time that the parties are attempting to resolve any dispute, each party shall proceed diligently to perform its duties.\\n\\n\\n\\n\\t\\tCONTRACTOR AFFILIATION \\n\\n\\t\\n\\n\\t\\tIf an affiliate (as defined below in this subsection) of the Contractor takes any action which, if taken by the Contractor, would constitute a breach of the contract, the action taken by the affiliate shall be deemed a breach by the Contractor.  “Affiliate” shall mean a “parent,” subsidiary or other company controlling, controlled by, or in common control with the Contractor, sub Contractor or agents of the Contractor.\\n\\n\\t\\n\\n\\t\\t8.19\\tTRANSFER OF FILES\\n\\n\\t\\n\\n\\t\\tIf for any reason the Department decides to no longer contract with the Contractor, the Contractor agrees to transfer to the party designated by the Department, at no cost, all data, records, computer files, other files, and materials of any sort that were maintained for the Commonwealth.  The Contractor agrees to assist the Department in understanding, using, and transferring all files and records, including those maintained in computer language.\\n\\n\\n\\n\\t\\t8.20\\tINDEMNIFICATION \\n\\n\\t\\n\\n\\t\\tContractor agrees to indemnify the Commonwealth of Virginia, its officers, agents, and employees for any loss, liability, cost, or reasonable settlement cost incurred as a result of any claims, damages and actions of any kind or nature, whether at law or in equity, arising from or caused by the use of any materials, goods, or equipment of any kind or nature furnished by the contractor/any services of any kind or nature furnished by the contractor, provided that such liability is not attributable to the sole negligence of the using agency or to failure of the using agency to use the materials, goods, or equipment in the manner already and permanently described by the contractor on the materials, goods or equipment delivered.\\n\\n\\n\\n\\t8.21\\tANNUAL ENROLLMENT \\n\\n\\t\\n\\n\\t\\tThe Department will provide employees an annual opportunity to change health benefits plans or types of membership.  Contractor agrees to follow all instructions of the Department with respect to the conduct of the annual enrollment, including and especially the form and content of information supplied to eligible persons.  The Contractor agrees to supply to agencies and TLC employers sufficient quantities of benefits booklets and brochures, both written and electronically, for the orderly conduct of annual enrollment activities. Annual enrollment expenses are the responsibility of the Contractor and are to be absorbed in its administrative costs.  There will be no special recognition of annual enrollment expenses without the prior agreement of the Department.  The LODA plans do not have an annual enrollment opportunity, but the Contractor must supply Summary Plan Descriptions (SPD), as required, for new enrollees and to all participants upon changes to an existing SPD.\\n\\n\\t\\n\\n\\t\\tThe Department’s Benefits Eligibility System (BES) collects, validates, and distributes eligibility and enrollment data to data partners that comply with state and federal regulations for the state employee, TLC, and LODA programs.  BES is the central, sole-source of all eligibility and enrollment information for Contractors, including those enrollees whose premium is direct billed.  The Department has implemented a personal identification number for participants, which is not a Social Security Number.  The ID card should include the unique ID number created by the Department. \\n\\n\\t\\n\\n\\t8.22\\tHIPAA PRIVACY BUSINESS ASSOCIATES AGREEMENT\\n\\n\\t\\n\\n\\t\\tThe Contractor agrees to be bound by the HIPAA Privacy Business Associates Agreement. This agreement must be executed prior to any contract award. See Exhibit ONE.\\n\\n\\t\\n\\n\\t8.23\\tCHANGES IN PARTICIPATING PROVIDERS \\n\\n\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tThe Plan shall require, among other things, that the provider will abide by the provisions of the agreement with the Plan for a full contract year with respect to State and TLC employees, except for such changes as retirement, abandonment of practice, etc. This provision does not apply to staff/group type HMOs.  Note well that the end of the contract year for many, but not most, TLC groups is September 30, not June 30.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t8.24   MAILINGS AND NOTICES\\n\\n\\t\\n\\n\\t\\tThe Medical/Surgical Contractor agrees to notify retiree group participants and extended coverage enrollees annually in a form acceptable to the Department of changes, regardless of under which product they fall, in premiums and benefits or other contract amendments in a form acceptable to the Department. All notices shall be mailed first class. Each Contractor agrees to supply group administrators with all necessary forms and supplies.\\n\\n\\n\\n\\t\\tContractor will strictly limit the content and form of mailings and notices, other than claims related transactions, to the benefits booklet and brochure cited in subsections 6.2 and 6.3 and an approved cover letter, unless otherwise approved by the Department.  Benefits booklets and brochures shall be printed in black ink on plain white paper, grade number 3, 50 pound offset, without any illustrations except graphs to illustrate HEDIS data.  Under no circumstances will any communication of the Contractor, written or verbal, compare its cost, benefits, or performance with that of another plan in the employee health benefits program without express permission from the Department.  The logo of the Department and the title of the document shall be the most prominent features on the first page of each document. \\n\\n\\t\\n\\n\\t\\tContractor will never generate mass correspondence without review and approval of the Department. \\n\\n\\t\\n\\n\\t8.25\\tIDENTITY THEFT: \\n\\n\\t\\n\\n\\t\\tThe Contractor assures that any and all personal information and data obtained as a result of performing contractual duties associated with this contract shall be held in strict confidence. Such information shall not be divulged without written permission from the individual and this Agency.\\n\\n\\n\\nAll personal information whether electronic or hard copy shall be stored in a manner that will prevent intrusion by unauthorized persons.\\n\\n\\t\\n\\nAll intrusions or suspicion of intrusion into secured files containing personal information shall be reported to the Agency within 24 hours of detection.\\n\\n\\t\\n\\nAll remedies suggested by the Contractor shall be approved by the Agency prior to being implemented.\\n\\n\\n\\n\\t8.26 \\tEVA ORDERS AND CONTRACTS\\n\\n\\t\\n\\n\\t\\tIt is anticipated that the contract will result in multiple purchase orders (i.e., one for each delivery requirement) with the eVA transaction fee specified below assessed for each order.\\n\\n\\t\\n\\n\\t\\t\\t\\tVendors desiring to provide goods and/or services to the Commonwealth shall participate in the eVA Internet e-procurement solution and agree to comply with the following:  If this solicitation is for a term contract, failure to provide an electronic catalog (price list) or index page catalog for items awarded will be just cause for the Commonwealth to reject your bid/offer or terminate this contract for default.  The format of this electronic catalog shall conform to the eVA Catalog Interchange Format (CIF) Specification that can be accessed and downloaded from www.eVA.virginia.gov.  Contractors should email Catalog or Index Page information to eVA-catalog-manager@dgs.virginia.gov.\\n\\n\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t8.27 \\tSUBMISSION OF SMALL BUSINESS SUBCONTRACTING PLAN, EVIDENCE OF COMPLIANCE WITH SMALL BUSINESS SUBCONTRACTING PLAN, AND SUBCONTRACTOR REPORTING : \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tA.\\tSubmission of Small Business Subcontracting Plan: \\tIt is the statewide goal of the Commonwealth that 42% of its purchases be made from small businesses certified by DSBSD. This includes discretionary spending in prime contracts and subcontracts.  All bidders/offerors are required to submit a Small Business Subcontracting Plan.   The contractor is encouraged to offer such subcontracting opportunities to DSBSD-certified small businesses.  This shall include DSBSD-certified women-owned and minority-owned businesses and businesses with DSBSD service disabled veteran-owned status when they have also received DSBSD small business certification. Where it is not practicable for any portion of the goods/services to be subcontracted to other suppliers, the bidder/offeror shall note such on the Small Business Subcontracting Plan.  No bidder/offeror or subcontractor shall be considered a small business unless certified as such by the Department of Small Business and Supplier Diversity (DSBSD) by the due date for receipt of bids or proposals.  \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tB.\\tEvidence of Compliance with Small Business Subcontracting Plan:  Each prime contractor who wins an award in which provision of a small business subcontracting plan is a condition of the award, shall deliver to the contracting agency or institution timely reports substantiating compliance in accordance with the small business subcontracting plan.  If a variance exists, the contractor shall provide a written explanation.   A subcontractor shall be considered a Small Business for purposes of a contract if and only if the subcontractor holds a certification as such by the DSBSD.  Payment(s) may be withheld until the purchasing agency confirms that the contractor has certified compliance with the contractor’s submitted Small Business Subcontracting Plan or is in receipt of a written explanation of the variance.  The agency or institution reserves the right to pursue other appropriate remedies for non-compliance to include, but not be limited to, termination for default.\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tC.\\t\\tPrime Contractor Subcontractor Reporting:  \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t1. \\tEach prime contractor who wins an award greater than $100,000, shall deliver to the contracting agency or institution on a _____ (insert monthly, quarterly, or other frequency) ______ basis, information on use of subcontractors that are DSBSD-certified businesses or Employment Services Organizations (ESOs).  The contractor agrees to furnish the purchasing office at a minimum the following information:  name of firm, phone number, total dollar amount subcontracted, category type (Businesses that are DSBSD-certified small, women-owned, minority-owned, businesses with DSBSD Service Disabled Veteran-owned status, or Employment Services Organization) and type of product/service provided, at the frequency required. \\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t2.  \\tIn addition each prime contractor who wins an award greater than $200,000 shall deliver to the contracting agency or institution on a _____ (insert monthly, quarterly, or other frequency) ______ basis, information on use of subcontractors that are not DSBSD-certified businesses or Employment Services Organizations.  The contractor agrees to furnish the purchasing office at a minimum the following information:  name of firm, phone number, total dollar amount subcontracted and type of product/service provided, at the frequency required. \\n\\n\\t\\n\\n\\n\\n\\nExhibit One\\n\\n\\n\\nOffice of State Health Benefits Programs\\n\\n\\t of the \\n\\n\\tDepartment of Human Resource Management\\n\\n\\t\\n\\n\\tH I PAA Privacy\\n\\n\\tBusiness Associate\\n\\n\\tAgreement\\n\\n\\tWith\\n\\n\\t(Insert Company Name)\\n\\n\\t\\n\\n\\t\\n\\n\\tEffective Date:\\n\\n\\t(Insert Date)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGroup Health Plan’s Business Associate Agreement\\n\\n\\n\\nThis agreement (“Agreement”) is effective as of July 1, 2019 and is made among ___________________ (“Business Associate”), and the Commonwealth of Virginia Group Health Plan and The Local Choice Plan , administered by the Office of Health Benefits Programs (“Plans”) for the Department of Human Resource Management. \\n\\nWITNESSETH AS FOLLOWS:\\n\\nWHEREAS, the Commonwealth of Virginia has established and maintains the Plans as programs that provide health care coverage for employees, former employees, and eligible dependents of employees pursuant to § 2.2-2818 and employees of local governments, local officers, teachers, and retirees, and the eligible dependents of such employees, officers, teachers, and retirees pursuant to § 2.2-1204,  of the Code of Virginia.  The Plans meet the definition of a “health plan” under the Health Insurance Portability and Accountability Act of 1996 and it’s implementing regulations (45 C.F.R. Parts 160-64);\\n\\n WHEREAS, the Plans have retained Business Associate to provide certain administrative services with respect to the Plans which are described and set forth in a separate Administrative Services Agreement among those parties procured under RFP numbered OHB19-01 (“ASO Agreement”) which is in effect on the effective date of this Agreement, as amended or replaced from time to time;\\n\\nWHEREAS, the parties to this Agreement desire to establish the terms under which Business Associate may use or disclose Protected Health Information (as defined herein) such that the Plans may comply with applicable requirements of the Health Insurance Portability and Accountability Act of 1996 and its implementing regulations (45 C.F.R. Parts 160-64) (“HIPAA Privacy Regulations”);\\n\\nNOW, THEREFORE, in consideration of these premises and the mutual promises and agreements hereinafter set forth, the Plans, and Business Associate hereby agree as follows:\\n\\nDefinitions\\n\\nCatch-all definition:\\n\\nThe following terms used in this Agreement shall have the same meaning as those terms in the HIPAA Rules: Breach, Data Aggregation, Designated Record Set, Disclosure, Health Care Operations, Individual, Minimum Necessary, Notice of Privacy Practices, Protected Health Information, Required By Law, Secretary, Security Incident, Subcontractor, Unsecured Protected Health Information, and Use.\\n\\nSpecific definitions:\\n\\n(a) Business Associate. \\xa0“Business Associate” shall generally have the same meaning as the term “business associate” at 45 CFR 160.103, and in reference to the party to this agreement, shall mean                “NAME OF Benefits Administrator”_______.\\n\\n(b) Covered Entity. \\xa0“Covered Entity” shall generally have the same meaning as the term “covered entity” at 45 CFR 160.103, and in reference to the party to this agreement, shall mean The Department of Human Resource Management, Office of Health Benefits.\\n\\n(c) Covered Person. “Covered Person” shall mean the covered employee and the covered employees’ legal spouse and/or dependent children as specified in the plan document.\\n\\n(d) HIPAA Rules.\\xa0 “HIPAA Rules” shall mean the Privacy, Security, Breach Notification, and Enforcement Rules at 45 CFR Part 160 and Part 164.\\n\\n\\n\\nI. Obligations and Activities of Business Associate\\n\\n\\n\\nBusiness Associate agrees to:\\n\\n\\n\\n(a) \\tNot use or disclose protected health information other than as permitted or required by the Agreement or as required by law;\\n\\n\\n\\n(b) \\tUse appropriate safeguards, and comply with Subpart C of 45 CFR Part 164 with respect to electronic protected health information, to prevent use or disclosure of protected health information other than as provided for by the Agreement;\\n\\n\\n\\n(c) \\tReport to covered entity any use or disclosure of protected health information not provided for by the Agreement of which it becomes aware, including breaches of unsecured protected health information as required at 45 CFR 164.410, and any security incident of which it becomes aware;\\n\\n\\n\\n1.  \\tBusiness Associate will promptly report to the Plans within five (5) business days of any use or disclosure of Covered Person’s Protected Health Information (whether by itself or by its subcontractors) not permitted by this Agreement.\\n\\n\\t(d) \\tIn accordance with 45 CFR 164.502(e)(1)(ii) and 164.308(b)(2), if applicable, ensure that any subcontractors that create, receive, maintain, or transmit protected health information on behalf of the business associate agree to the same restrictions, conditions, and requirements that apply through this Agreement and to the business associate with respect to such information;\\n\\n\\t\\n\\n\\t(e) \\tMake available protected health information in a designated record set to the “covered entity” as necessary to satisfy covered entity’s obligations under 45 CFR 164.524;\\n\\n\\t\\n\\nNon-HIPAA requests\\n\\nBusiness Associate will continue to respond to Covered Person’s routine requests for access to their Protected Health Information as part of Business Associate’s normal customer service functions, if those requests do not qualify as a formal HIPAA request.  In order to be deemed a “formal HIPAA request” the Covered Person must submit the request directly to the appropriate Plan, and follow all of the procedural requirements set forth in the appropriate Plan’s Privacy Notice.  All requests submitted directly to the Business Associate will be handled as a non-HIPAA request.\\n\\n\\n\\nHIPAA requests\\n\\nBusiness Associate will assist the Plans in responding to Covered Person’s formal HIPAA requests by performing the following functions:\\n\\nUpon receipt of written notice (includes faxed and emailed notice) from either of the Plans, Business Associate will make available for inspection and obtaining copies by the Plans, or at the Plans’ direction by the Covered Person (or the Covered Person’s personal representative), any Protected Health Information about the Covered Person created or received for or from the Plans, in Business Associate’s custody or control, so that the Plans may meet  their access obligations under 45 Code of Federal Regulations § 164.524 within 10 calendar days.\\n\\nBusiness Associate will not respond directly to Covered Person’s formal HIPAA requests.  Business Associate will refer the Covered Person to the proper plan that is either The Local Choice or the Commonwealth of Virginia Group Health Plan, so that the respective Plan can coordinate and prepare a timely response to the Covered Person.\\n\\n\\n\\n\\t(f) \\tMake any amendment(s) to protected health information in a designated record set as directed or agreed to by the covered entity pursuant to 45 CFR 164.526, or take other measures as necessary to satisfy covered entity’s obligations under 45 CFR 164.526;\\n\\n\\t\\n\\n\\tNon-HIPAA requests\\n\\n\\t\\tBusiness Associate will continue to respond to Covered Person’s routine requests to amend their Protected Health Information as part of Business Associate’s normal customer service functions, if those requests do not qualify as a formal HIPAA request.  In order to be deemed a “formal HIPAA request” the Covered Person must submit the request directly to the Plan for which they are enrolled, and follow all of the procedural requirements set forth in the Plan’s Privacy Notice. All requests submitted directly to the Business Associate will be handled as a non-HIPAA request. \\n\\n\\t\\n\\n\\tHIPAA requests\\n\\n\\t\\tBusiness Associate will assist the Plans in responding to Covered Person’s formal HIPAA requests by performing the following functions:\\n\\n\\t\\tUpon receipt of written notice (includes faxed and e-mailed notice) from the Plans, Business Associate will amend any portion of the Protected Health Information created or received for or from the Plans in Business Associate’s custody or control, so that the Plans may meet  their amendment obligations under 45 Code of Federal Regulations § 164.526 within 10 calendar days.\\n\\n\\t\\tBusiness Associate will not respond directly to Covered Person’s formal HIPAA requests.  Business Associate will refer the Covered Person to the Plans so that the Plans can coordinate and prepare a timely response to the Covered Person.\\n\\n\\t\\n\\n\\t(g) \\tMaintain and make available the information required to provide an accounting of disclosures to the “covered entity” as necessary to satisfy covered entity’s obligations under 45 CFR 164.528;\\n\\n\\t\\n\\nNon-HIPAA requests\\n\\nBusiness Associate will continue to respond to Covered Person’s routine requests for an accounting of disclosures of their Protected Health Information as part of Business Associate’s normal customer service functions, if those requests do not qualify as a formal HIPAA request.  In order to be deemed a “formal HIPAA request” the Covered Person must submit the request directly to the Plan for which they are enrolled, and follow all of the procedural requirements set forth in the Plan’s Privacy Notice. All requests submitted directly to the Business Associate will be handled as a non-HIPAA request.\\n\\n\\n\\nHIPAA requests\\n\\nBusiness Associate will assist the Plans in responding to Covered Person’s formal HIPAA requests by performing the following functions:\\n\\nSo the Plans may meet their disclosure accounting obligations under 45 Code of Federal Regulations § 164.528, Business Associate will do the following:\\n\\n\\n\\na.\\tDisclosure Tracking\\n\\nBusiness Associate will record each disclosure that Business Associate makes of Covered Person’s Protected Health Information, which is not excepted from disclosure accounting under Section I.g.2.b. of this agreement.\\n\\nThe information about each disclosure that Business Associate must record (“Disclosure Information”) is (a) the disclosure date, (b) the name and (if known) address of the person or entity to whom Business Associate made the disclosure, (c) a brief description of the Protected Health Information disclosed, and (d) a brief statement of the purpose of the disclosure or a copy of any written request for disclosure under 45 Code of Federal Regulations §164.502(a)(2)(ii) or §164.512.\\n\\nFor repetitive disclosures of Covered Person’s Protected Health Information that Business Associate makes for a single purpose to the same person or entity, Business Associate may record (a) the Disclosure Information for the first of these repetitive disclosures, (b) the frequency, periodicity or number of these repetitive disclosures, and (c) the date of the last of these repetitive disclosures.\\n\\n\\n\\nb.\\tExceptions from Disclosure Tracking\\n\\nBusiness Associate will not be required to record Disclosure Information or otherwise account for disclosures of Covered Person’s Protected Health Information (a) for Treatment, Payment or Health Care Operations, (b) to the Covered Person who is the subject of the Protected Health Information, to that Covered Person’s personal representative, or to another person or entity authorized by the Covered Person (c) to persons involved in that Covered Person’s health care or payment for health care as provided by 45 Code of Federal Regulations § 164.510, (d) for notification for disaster relief purposes as provided by 45 Code of Federal Regulations § 164.510, (e) for national security or intelligence purposes, (f) to law enforcement officials or correctional institutions regarding inmates, (g) that are incidental to a use or disclosure that is permitted by this Agreement or the ASO Agreement,  (h) as part of a limited data set in accordance with 45 Code of Federal Regulations § 164.514(e), or (i) that occurred prior to the Plan’s compliance date.\\n\\n\\n\\nc.\\tDisclosure Tracking Time Periods\\n\\nBusiness Associate will have available for the Plans the Disclosure Information required by Section I.g.2.a above for the six (6) years immediately preceding the date of the  Plans’ request for the Disclosure Information. \\n\\n\\n\\nd.\\tProvision of Disclosure Accounting\\n\\nUpon receipt of written notice (includes faxed and e-mailed notice) from the Plans, Business Associate will make available to the Plans, or at the  Plans’ direction to the Covered Person (or the Covered Person’s personal representative), the Disclosure Information regarding the Covered Person, so the Plans may meet their disclosure accounting obligations under 45 Code of Federal Regulations § 164.528 within 10 calendar days.\\n\\nBusiness Associate will not respond directly to Covered Person’s formal HIPAA requests for an accounting of disclosures.  Business Associate will refer the Covered Person to the Plans so that the Plans can coordinate and prepare a timely accounting to the Covered Person.\\n\\n\\n\\n(h)\\xa0 To the extent the business associate is to carry out one or more of covered entity\\'s obligation(s) under Subpart E of 45 CFR Part 164, comply with the requirements of Subpart E that apply to the covered entity in the performance of such obligation(s); and\\n\\n\\n\\n(i) \\tMake its internal practices, books, and records available to the Secretary for purposes of determining compliance with the HIPAA Rules.\\n\\n\\n\\n\\xa0II. Permitted Uses and Disclosures by Business Associate\\n\\n\\n\\n\\t(a) \\tBusiness associate may only use or disclose protected health information “as necessary to perform the services set forth in ASO Agreement.”\\n\\n\\t\\n\\nProtected Health Information Use\\n\\nBusiness Associate may use Covered Person’s Protected Health Information as necessary for Business Associate to perform Data Aggregation services, and to create De-identified Information, Summary Health Information and/or Limited Data Sets.\\n\\n\\n\\nProtected Health Information Disclosure\\n\\nBusiness Associate may disclose, in conformance with the HIPAA Privacy Regulations, Covered Person’s Protected Health Information to make disclosures of De-identified Information, Limited Data Set Information, and Summary Health Information, and to make Incidental Disclosures.\\n\\n\\n\\n\\t(b) \\tBusiness associate may use or disclose protected health information as required by law.\\n\\n\\t\\n\\n\\t(c) \\tBusiness associate agrees to make uses and disclosures and requests for protected health information consistent with covered entity’s minimum necessary policies and procedures that accomplish the intended purpose.\\n\\n\\t\\n\\n\\t (d) \\tBusiness associate may not use or disclose protected health information in a manner that would violate Subpart E of 45 CFR Part 164 if done by covered entity except for the specific uses and disclosures set forth below. \\n\\n\\t\\n\\n\\t(e) \\tBusiness associate may use protected health information for the proper management and administration of the business associate or to carry out the legal responsibilities of the business associate.\\n\\n\\t\\n\\n\\t(f) \\tBusiness associate may disclose protected health information for the proper management and administration of business associate or to carry out the legal responsibilities of the business associate, provided the disclosures are required by law, or business associate obtains reasonable assurances from the person to whom the information is disclosed that the information will remain confidential and used or further disclosed only as required by law or for the purposes for which it was disclosed to the person, and the person notifies business associate of any instances of which it is aware in which the confidentiality of the information has been breached.\\n\\n\\t\\n\\n\\t(g) \\tBusiness associate may provide data aggregation services relating to the health care operations of the covered entity.\\n\\n\\n\\nIII. Provisions for Covered Entity to Inform Business Associate of Privacy Practices and Restrictions\\n\\n\\n\\n(a) \\tCovered entity shall notify business associate of any limitation(s) in the notice of privacy practices of covered entity under 45 CFR 164.520, to the extent that such limitation may affect business associate’s use or disclosure of protected health information.\\n\\n\\n\\n(b) \\tCovered entity shall notify business associate of any changes in, or revocation of, the permission by an individual to use or disclose his or her protected health information, to the extent that such changes may affect business associate’s use or disclosure of protected health information.\\n\\n\\n\\n(c) \\tCovered entity shall notify business associate of any restriction on the use or disclosure of protected health information that covered entity has agreed to or is required to abide by under 45 CFR 164.522, to the extent that such restriction may affect business associate’s use or disclosure of protected health information.\\n\\nIV. Permissible Requests by Covered Entity\\n\\n\\n\\nCovered entity shall not request business associate to use or disclose protected health information in any manner that would not be permissible under Subpart E of 45 CFR Part 164 if done by covered entity except if the business associate will use or disclose protected health information for, and the agreement includes provisions for, data aggregation or management and administration and legal responsibilities of the business associate. \\n\\n\\n\\nV. Term and Termination\\n\\n\\n\\n\\t(a) \\tTerm. The Term of this Agreement shall be effective as of July 1, 2019, and shall terminate upon termination of the ASO Agreement or on the date covered entity terminates for cause as authorized in paragraph (b) of this Section, whichever is sooner.\\xa0\\n\\n\\t\\n\\n\\t(b) \\tTermination for Cause. Business associate authorizes termination of this Agreement by covered entity, if covered entity determines business associate has violated a material term of the Agreement and business associate has not cured the breach or ended the violation within the time specified by covered entity.\\xa0 \\n\\n\\t\\n\\n\\t(c) \\tObligations of Business Associate Upon Termination.\\n\\n\\t\\tUpon termination of this Agreement for any reason, business associate, with respect to protected health information received from covered entity, or created, maintained, or received by business associate on behalf of covered entity, shall in accordance with its record retention policy that it applies to similar records, except to the extent a longer period of time is specified by the ASO Agreement or the law:\\n\\n\\n\\n\\t\\tRetain only that protected health information which is necessary for business associate to continue its proper management and administration or to carry out its legal responsibilities;\\n\\n\\n\\n\\t\\tReturn to covered entity or destroy the remaining protected health information that the business associate still maintains in any form;\\n\\n\\n\\n\\t\\tContinue to use appropriate safeguards and comply with Subpart C of 45 CFR Part 164 with respect to electronic protected health information to prevent use or disclosure of the protected health information, other than as provided for in this Section, for as long as business associate retains the protected health information;\\n\\n\\n\\n\\t\\tNot use or disclose the protected health information retained by business associate other than for the purposes for which such protected health information was retained and subject to the same conditions set out at Section II.f which applied prior to termination; and\\n\\n\\n\\n\\t\\tReturn to covered entity or destroy the protected health information retained by business associate when it is no longer needed by business associate for its proper management and administration or to carry out its legal responsibilities.\\n\\n\\n\\n\\t\\tTransmit the protected health information to another business associate of the covered entity at termination, and obtain or ensure the destruction of protected health information created, received, or maintained by subcontractors.\\n\\n\\n\\n\\t(d) \\tSurvival.\\xa0 The obligations of business associate under this Section shall survive the termination of this Agreement.\\n\\n\\n\\nVI. Miscellaneous \\n\\n\\n\\n\\t(a)  \\tRegulatory References. A reference in this Agreement to a section in the HIPAA Rules means the section as in effect or as amended.\\n\\n\\t\\n\\n\\t(b)  \\tAmendment. The Parties agree to take such action as is necessary to amend this Agreement from time to time as is necessary for compliance with the requirements of the HIPAA Rules and any other applicable law.\\n\\n\\t\\n\\n\\t(c)  \\tInterpretation. Any ambiguity in this Agreement shall be interpreted to permit compliance with the HIPAA Rules.\\n\\n\\t\\n\\n\\t(d) \\tBusiness Associate is an independent contractor and not an agent of either of the Plans for any purpose under this agreement.\\n\\n\\t\\n\\n\\t\\t(e) \\tNotice.  Any notice or reporting required under this Agreement to be given to Business Associate shall be made in writing.\\n\\n\\t\\t\\n\\n\\n\\n\\n\\n\\n\\nIN WITNESS WHEREOF, Plans and Business Associate have caused this Business Associate Agreement to be executed by duly authorized officers.\\n\\n\\n\\n\\n\\n\\n\\nCommonwealth of Virginia Group\\t\\t\\tBusiness Associate\\n\\nHealth Plan and the Local Choice Plan\\n\\n\\n\\nBy:\\n\\n\\n\\n\\n\\nBy:\\n\\n\\n\\nName:\\n\\n\\n\\n\\n\\nName:\\n\\n\\n\\nTitle:\\n\\n\\n\\n\\n\\nTitle:\\n\\n\\n\\nAddress:\\n\\n\\n\\n\\n\\nAddress:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPhone:\\n\\n\\n\\n\\n\\nPhone:\\n\\n\\n\\nDate:\\n\\n\\n\\n\\n\\nDate:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nExhibit Two\\n\\n\\n\\nSmall Business Subcontracting Plan\\n\\n\\t\\t\\t\\t\\t\\t\\n\\nIt is the goal of the Commonwealth that over 42% of its purchases be made from small businesses.  All potential offerors are required to submit a Small Business Subcontracting Plan.  \\n\\n\\n\\nSmall Business:  \"Small business (including micro)” means a business which holds a certification as such by the Virginia Department of Small Business and Supplier Diversity (DSBSD) on the due date for proposals.    This shall also include DSBSD-certified women- and minority-owned businesses when they also hold a DSBSD certification as a small business on the proposal due date.  Currently, DSBSD offers small business certification and micro business designation to firms that qualify. \\n\\n\\n\\nCertification applications are available through DSBSD online at www.DSBSD.virginia.gov (Customer Service).\\n\\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\nOfferor Name: _____________________________________________  \\n\\n\\n\\nPreparer Name: ______________________________  \\tDate: _________________\\n\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tInstructions\\n\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\tA.\\tIf you are certified by the DSBSD as a micro/small business, complete only Section A of this form.  This includes DSBSD-certified women-owned and minority-owned businesses when they have also received DSBSD small business certification.\\n\\n\\t\\t\\t\\t\\n\\nB.  If you are not a DSBSD-certified small business, complete Section B of this form.  For the proposal to be considered and the offeror to be declared responsive, the offeror shall identify the portions of the contract that will be subcontracted to DSBSD-certified small business for the initial contract period in relation to the offeror’s total price for the initial contract period. in Section B.   \\n\\n\\n\\nSection A \\n\\n\\n\\n\\t\\tIf your firm is certified by the DSBSD provide your certification number and the date of certification. \\n\\n    \\n\\n\\n\\n\\t\\t\\tCertification number:________\\t      Certification Date:___________________\\n\\n\\t\\t\\t\\t\\t\\t\\nSection B\\n\\n\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\tPopulate the table below to show your firm\\'s plans for utilization of DSBSD-certified small businesses in the performance of this contract for the initial contract period in relation to the offeror’s total price for the initial contract period.  Certified small businesses include but are not limited to DSBSD-certified women-owned and minority-owned businesses that have also received the DSBSD small business certification.  Include plans to utilize small businesses as part of joint ventures, partnerships, subcontractors, suppliers, etc.  It is important to note that these proposed participation will be incorporated into the subsequent contract and will be a requirement of the contract.  Failure to obtain the proposed participation percentages may result in breach of the contract.  \\n\\n\\t\\t\\t\\t\\t\\xa0\\n\\n\\tB.  \\tPlans for Utilization of DSBSD-Certified Small Businesses for this Procurement \\n\\n\\n\\nMicro/Small Business Name & Address\\n\\n\\n\\nDSBSD Certificate #  \\n\\nStatus if Micro/Small Business is also: Women (W), Minority (M)\\n\\nContact Person, Telephone & Email\\n\\nType of Goods and/or Services\\n\\nPlanned  Involvement During Initial Period of the Contract\\n\\nPlanned Contract  Dollars During Initial Period of the Contract\\n\\n($ or %)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTotals $\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nList of Appendices\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\tAppendix 1\\tCurrent Standard Contract \\n\\n\\t\\n\\n\\tAppendix 2\\tSelected Enrollment, Costs, Workload, Demographics and Utilization for State Employees \\n\\n\\t\\n\\n\\tAppendix 3\\tWebsite Links for Benefit Program Descriptions and Current Forms \\n\\n\\t\\n\\n\\tAppendix 4\\tEnrollment for TLC and LODA \\n\\n\\t\\n\\n\\tAppendix 5\\tState Employee Eligibility, Enrollment and Billing \\n\\n\\t\\n\\n\\tAppendix 6\\tTLC Program Administration \\n\\n\\t\\n\\n\\tAppendix 7\\tEDI Website Link \\n\\n\\t\\n\\n\\tAppendix 8\\tFlexible Benefits Program Description \\n\\n\\t\\n\\n\\tAppendix 9 \\tProposal Checklist \\n\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\nAppendix 1\\n\\n\\n\\nDEPARTMENT OF HUMAN RESOURCE MANAGEMENT\\n\\nSTANDARD CONTRACT\\n\\n\\n\\n\\t\\n\\n\\tCOMMONWEALTH OF VIRGINIA\\n\\n\\tDepartment of Human Resource Management\\n\\n\\t\\tSTANDARD CONTRACT\\n\\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\\tContract Number:  OHB 19-01\\n\\n\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThis contract entered into this ___ day of ________ 2019, by_______________________ hereinafter called the “Contractor” and Commonwealth of Virginia, Department of Human Resource Management, Commonwealth of Virginia Campaign hereafter called the “Purchasing Agency.”\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tWITNESSETH that the Contractor and the Purchasing Agency, in consideration of the mutual covenants, promises and agreements herein contained, agree as follows:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tSCOPE OF CONTRACT:  The Contractor shall provide the goods/services to the Purchasing Agency as set forth in the Contract Documents.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPERIOD OF PERFORMANCE: From July 1, 2019 through June 30, 2024.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThe contract documents shall consist of:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(1)\\tThis signed form;\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(2)\\tThe following portions of the Request for Proposal dated ________________:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(a)\\tThe Statement of Needs,\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(b)\\tThe General Terms and Conditions,\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(c)\\tThe Special Terms and Conditions together with any negotiated modifications of those Special Conditions;\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAttachment ______, Date  ____________________\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAttachment ______, Date  ____________________\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(3)\\tThe Contractor’s Proposal dated ____________________ and the following negotiated modifications to the Proposal, all of which documents are incorporated herein.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tIN WITNESS WHEREOF, the parties have caused this Contract to be duly executed intending to be bound thereby.\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCONTRACTOR:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPURCHASING AGENCY:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBy:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tBy:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tTitle:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\nTitle:\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\nNote: This public body does not discriminate against faith-based organizations in accordance with the Code of Virginia, § 2.2-4343.1 or against a bidder or offeror because of race, religion, color, sex, national origin, age, disability, or any other basis prohibited by state law relating to discrimination in employment.\\n\\nAppendix 2\\n\\n\\n\\n\\n\\nSelected Enrollment, Cost, Workload, Demographic and Utilization for State Employees\\n\\n\\n\\nFiles containing claims, enrollment data and the Attachment 2 schedules you will need to prepare and submit a proposal are available in electronic form. To obtain these files, please send email to Brian Dwyer (brian.dwyer@aon.com) with copy to Richard Whitfield (richard.whitfield@dhrm.virginia.gov) requesting credentials and instructions necessary to download the files from a secure site.\\n\\n\\n\\nPlease note, these files are proprietary and available only to vendors of the services requested by this RFP\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\n\\n\\nAppendix 3\\n\\n\\n\\n\\n\\nSummary Description of All Plans Offered to COVA and TLC Groups\\n\\n\\n\\nPlease access the web addresses shown for summary benefit descriptions of plans\\n\\n\\n\\n\\t\\tState Program Plans:\\n\\n\\t\\n\\n\\t\\thttps://www.dhrm.virginia.gov/healthcoverage/planhandbooks    \\n\\n\\t\\n\\n\\t\\n\\n\\t\\tLODA Health Benefits Plans\\n\\n\\t\\n\\n\\t\\thttps://www.dhrm.virginia.gov/healthcoverage/loda-health-benefits\\n\\n\\t\\n\\n\\t\\tThe Local Choice Plans\\n\\n\\n\\n\\thttp://www.thelocalchoice.virginia.gov/planinfo/employeeplans2018_2019.html\\n\\n\\thttp://www.thelocalchoice.virginia.gov/planinfo/regionalplans2018_2019.html\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\n\\n\\n\\tAppendix 4     \\n\\n\\tLODA Enrollment\\n\\n\\tJune 2018\\n\\n\\t\\n\\n\\tLODA Plan 1\\n\\n\\tLODA Plan 2\\n\\n\\tParticipants\\n\\n\\t1,061\\n\\n\\t6\\n\\n\\tFamily Members\\n\\n\\t1,378\\n\\n\\t12\\n\\n\\tTOTAL LIVES\\n\\n\\t2,439\\n\\n\\t18\\n\\n\\t\\n\\n\\tTLC Enrollment\\n\\n\\tJune 2018\\n\\n\\t\\n\\n\\n\\nHDHP*\\n\\nKey Adv*\\n\\nKey Adv 250*\\n\\nKey Adv 500*\\n\\nKey Adv 1000*\\n\\nTOTAL\\n\\nParticipants\\n\\n1,329\\n\\n5,248\\n\\n13,472\\n\\n11,652\\n\\n7,336\\n\\n39,037\\n\\nFamily Members\\n\\n1,201\\n\\n11,925\\n\\n8,404\\n\\n5,205\\n\\n4,467\\n\\n31,202\\n\\nTOTAL\\n\\n2,530\\n\\n17,173\\n\\n21,876\\n\\n16,857\\n\\n11,803\\n\\n70,239\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n*includes plans with diagnostic/preventive and comprehensive dental\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix 5\\n\\n\\n\\nState Employee Eligibility, Enrollment and Administrative Billing\\n\\nA.\\tBenefits Eligibility System\\n\\n\\n\\nThe Department collects, validates, and distributes data from a central, sole source eligibility and enrollment database known as the Benefits Eligibility System (BES).  BES is updated in a real-time environment and contains the official records for State participants and their covered dependents.  Participants include all employees, retirees, long-term disability participants, and survivors eligible for coverage.  Participants and their dependents are identified by a personal identification number which is not the participant’s social security number.  Contractors are required to use the personal identification number assigned by the Department on their databases.\\n\\n\\n\\nEnrollment and changes in enrollment are currently accomplished by paper form.  The Department’s web-based online enrollment system, EmployeeDirect, is currently unavailable, but an online system is planned for the future.     Changes through the online system will be automatically updated real time in BES.  \\n\\n\\n\\nThe Department uses data in BES to provide the Contractor with eligibility information in the HIPAA 834 Transaction File format as described on the DHRM Website (http://web1.dhrm.virginia.gov/itech/itdocs.htm).  Two types of files are provided:  \\n\\n\\n\\nThere is a Daily Change File provided Tuesday through Saturday which includes maintenance transactions that add or terminate coverage and must be processed by the Contractor within one business day.  Change transactions are provided as term/add pairs.  \\n\\n\\n\\nThe Monthly Audit File is provided on the third of each month and contains the state program, TLC and LODA full, active membership as of the first day of that month.  It is used only for comparison of information between the Department’s database and the Contractor’s database.  The Contractor is expected to report discrepancies found in the comparison to the Department no later than the 20th of that month.\\n\\n\\n\\nThe Contractor must connect to the Department’s secure FTP server for file transfers by one of the following protocols:  SFTP using SSH2 on port 22; or HTTPS for manual retrieval.  It is expected that Contractors update their databases on a regular basis to provide for accurate claims processing and ID card distribution.\\n\\n\\n\\nB.\\tClaims and Administrative Billing for Self-Funded Plans \\n\\n\\n\\nThe services billed under the self-funded plans fall into two categories. These are billing for claims payments and billing for administrative fees (Section 4.0) as records accumulated, and invoiced in total to the Department on a weekly basis. The Department’s staff reviews the invoice when it is received and processed for payment.  The Contractor(s) will be reimbursed for payment within three business days through an electronic transfer of funds.  The Contractor(s) is responsible to set up their payment and bank information through the Department of Accounts Electronic Data Interchange (EDI) web site. http://www.doa.virginia.gov/General_Accounting/EDI/EDI_Main.cfm\\n\\n\\n\\nThe billing documentation will at a minimum consist of: a cover invoice which provides the net claim dollars to be paid broken between the state program,  the TLC program, and the LODA program, including support documentation for each program that provides the claims dollars paid for each benefit category during the period covered by the invoice and year to date. This procedure will be finalized with each Contractor as part of the negotiation process and the cycle may be varied based upon compelling reasons, such as claim volume and dollars.\\n\\n\\n\\nThe administrative expenses are invoiced monthly to the Department by each Contractor by the 15th of the following month. In this process, the Department will review the invoice and authorize reimbursement through the EDI process. Again, the billing documentation will consist of a cover invoice providing the administrative dollars in total for each program with a summary for all programs, and documentation which supports the summary invoice. This support will at minimum consist of a breakdown by each program of billing units by price per unit, shown for the current period and year to date. The number of billing units for each employer under the TLC program will also be required. The monthly administrative invoice may also be used as the financial transfer document for miscellaneous non-claim items that are either due from or to the Department when supported by clear documentation. This procedure will also be finalized during final negotiations.\\n\\n\\n\\nC.\\tBilling for Fully-Insured Plans \\n\\n\\n\\nThe Department makes monthly premium payments to all fully-insured carriers by a self- billing procedure based on the BES records as of the first day of each month of coverage. The self-billing process is run on the fifth working day of each month of coverage based on all first-day eligibles and takes into consideration any retroactive changes. The self-billing file includes all eligibles for a Contractor shown by agency and premiums due. The file is transferred electronically to the carrier and at the same time generates the request for payment. An EDI transfer around the 10th working day of each month makes payments. (See Appendix 7 for a description of the Commonwealth\\'s EDI payment system and forms required to be completed)\\n\\n\\t\\n\\n\\n\\n\\nAppendix 6\\n\\n\\n\\n\\n\\nTHE LOCAL CHOICE (TLC) PROGRAM ADMINISTRATION \\n\\n\\n\\n\\n\\nAdoption by Local Governmental Employer Groups\\n\\n\\n\\nTLC was established by the General Assembly of Virginia to provide an optional source of health insurance benefits to local government entities within Virginia. The program operates under regulations established by the Commonwealth of Virginia. The regulations require completion of a formal application by a prospective group, and the Medical/Surgical underwriters and Department’s actuaries provide the applicant with monthly premiums for each of the plans which are available to the group based on the size of the group and, if applicable, the area of the state. A group may join the program at the beginning of any month, but all groups renew with a July 1 effective date (except for some school groups that may choose an October 1 renewal date). A prospective group joins the program by completing a legal adoption agreement and submitting a document containing the plan choices that they will offer to their employees. The choice of the plans is an employer decision within the parameters established by TLC based on group size, and their employees may only choose from the plans selected. Contractors will provide them with the material needed to conduct the initial enrollment.\\n\\n\\n\\nEnrollment by employees of TLC Member Groups\\n\\n\\n\\nEach member group conducts an open enrollment process prior to the start of a new plan year. For the July renewing groups, open enrollment is normally held during the month of April or May.  October renewing school groups hold open enrollment during August or September.  Open enrollments will be for a 30-day period.  Standard enrollment/waiver forms are provided to the groups by the program, along with summary information on plans offered. An on-line enrollment process is planned for the future.  Each Contractor is required to provide a toll-free customer service line to provide information about their plan and to receive orders for plan specific materials for use by  the group\\'s benefit administrator.\\n\\n\\n\\nEach member group defines their eligible employees based on TLC policy and applicable law. A group is required to complete the enrollment process and provide enrollment information to the Department for input to the eligibility system.  The Department will implement an on-line enrollment process in the future.  The Department will provide eligibility information by file to the Contractor.  The Contractor(s) must establish their membership for claims processing, issue identification cards, and provide eligibility information to any subcontractors.    \\n\\n\\n\\n\\n\\t\\tMembership Files and Group Billing\\n\\n\\n\\nSection 8.4.2 of this RFP describes the group and direct billing process for TLC self-funded plan(s).  \\n\\n\\n\\nFor self-funded plans, the premiums collected during any month are transferred to the Department by the 5th working day of the following month. The premiums submitted should be shown by group and coverage period with total dollars by plan. For fully- insured plans, the carrier retains the premiums except for reimbursement to the Department of the 2% administrative fee as described in Section 8.5.3.  \\n\\n\\n\\n\\t\\tRenewal Process\\n\\n\\n\\nEach year, TLC groups go through a formal renewal process in which they are provided the full menu of plans available in their area with the premiums for the upcoming plan year. The number of plans that can be offered by each group is based on group size.  The renewal process starts on September 15th prior to the upcoming July 1 or October 1 effective date when groups are provided with premiums for the next plan year. The communications development for the upcoming year begins immediately with the involvement of all Contractors. The paid claims data for the self-funded plans is pulled through December 31 for each group and is rated along with current demographics and the costs of the pooled products. The Contractors’ underwriters proceed to develop rates by group for each self-funded and fully-insured plan and print a complete proposal of available plans. Proposals are assembled and delivered to member groups by February 28. The groups then have until April 1, or July 1, as applicable to their renewal date, to either renew or withdraw from the program. Renewing groups conduct open enrollment as described above. and are responsible for getting enrollment changes to the Department by June 1 or early September , depending on the renewal date.  Groups withdrawing from the program must be evaluated for any adverse experience adjustment.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\n\\n\\nAppendix 7\\n\\n\\n\\n\\n\\n\\n\\nElectronic Data Exchange (EDI)\\n\\n\\n\\nAll payments to Contractors will be made by EDI. The Financial Handbook and forms to be completed are found at the Web location below:\\n\\n\\n\\n\\n\\nhttp://www.doa.virginia.gov/General_Accounting/EDI/tradingpartnerguide.pdf\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\n\\n\\nAppendix 8  \\n\\n\\n\\nFlexible Benefit Plan\\n\\n\\n\\n\\n\\nThe Department’s Flexible Benefits Plan, which runs on a fiscal year basis, consists of three parts: (1) premium conversion, (2) dependent care flexible spending account and (3) health flexible spending account.  The services procured under this RFP pertain only to the two flexible spending accounts.  There are approximately 100,000 eligible employees. The current enrollment in the health flexible spending account is 16,363 with an average annual election of $1,347.00 and 2,142 in the dependent care flexible spending account with an average annual election of $3,746.00.  \\n\\n\\n\\nCurrently, employees of over 200 state agencies, located across the state are eligible to participate in the Department’s flexible spending accounts.  Included in the state agencies are seven institutions of higher education and three authorities that have decentralized their payroll activities from the central payroll system.  As a result, an Offeror submitting a proposal must have the capability of processing enrollee contribution information from multiple sources.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppendix 9\\n\\n\\t\\tProposal Checklist \\n\\n\\n\\nComplete the form below in full, sign in blue ink the completion certification at the bottom of the form, and enclose it following the Cover Sheet as directed in RFP Section 6.  \\n\\n\\n\\nOfferor:  \\n\\n\\n\\n1.a.\\tIndicate the plan design you have proposed by checking the appropriate blocks:\\n\\n\\n\\nRFP Components\\n\\n\\n\\nComponent 1: Statewide PPO and HDHP Medical/Surgical; Behavioral Health w/ EAP; Vision; and Hearing Services for the State Employee, TLC, and LODA Plans\\n\\nComponent 2: Statewide CDHP Medical/Surgical; Behavioral Health w/ EAP; Vision; and Hearing Services for the State Employee Program\\n\\nComponent 3: \\n\\nStatewide PPO, HDHP, and CDHP Prescription Drug Services for the State Employee, TLC, and LODA Plans\\n\\nComponent 4: Statewide PPO, HDHP, and CDHP Dental Services for the State Employee, TLC, and LODA Plans.\\n\\n\\n\\nComponent 5: Fully-insured Regional Plans for State Employee and TLC Programs\\n\\nComponent 6: Section 125 Flexible Spending Account Administration for State Employees\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t1.b.\\tIf you have proposed a Fully-Insured Regional HMO or PPO with less than statewide coverage, check the block below to affirm that you have attached a copy of your HMO license, and/or, for either network configuration, a document showing the cities/counties comprising your service area.\\n\\n\\t\\t\\n\\n\\t\\t\\t(1)\\tHMO license showing service area enclosed with Tab 1.\\t\\n\\n\\t\\n\\n\\t\\t\\t(2)\\tQuoting under exception provision as briefly described below.\\t\\n\\n\\t\\t\\n\\n\\t2.\\tIf you have proposed any network-based plans, affirm, by checking the appropriate blocks below, that you meet the Mandatory Minimum Qualifications stated in this RFP.  Your affirmation will also declare your intent to submit appropriate documentation as may be required to demonstrate these qualifications are met throughout the contract period.\\n\\n\\t\\n\\nMandatory Qualifications for Contractors\\n\\n\\tMeet Standard ()\\n\\n\\tMeet GeoAccess standard as specified in Section 2, or\\n\\n\\t\\n\\n\\t\\tb.\\tWill apply for Certificate as specified in Section 2.4.1 \\n\\n\\t\\n\\n\\t\\tc.\\tWill submit HEDIS report annually \\n\\n\\t\\n\\n\\t\\td.\\tWill submit specified member satisfaction results annually \\n\\n\\t\\n\\n\\t\\tBenefits Exceptions Description\\n\\n\\t\\n\\n\\t\\tf.\\tComply with Section 2.4.4. area coverage requirement (or claim test file requirement) \\n\\n\\t\\n\\n\\t\\tg.\\tComply with toll-free service requirement\\n\\n\\t\\n\\n\\t3.\\tAffirm below that you are in agreement with the Standards of Performance specified in RFP Section 3, including the Schedule of Liquidated damages, and will provide the requested documentation and claims tapes substantiating your performance and will meet the claims file test requirements if you are a finalist.\\n\\n\\t\\n\\nStatement\\n\\n\\tAgreement  ()\\n\\n\\t\\t\\t\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t4.\\tAffirm, by checking the appropriate block below, that you have completed and submitted all of the following required proposal components as required in RFP Section 6.  \\n\\n\\t\\n\\nProposal Item\\n\\n\\tCompleted and Submitted ()\\n\\n\\t\\ta.\\tCover Sheet, original signed in blue ink\\n\\n\\t\\n\\n\\t\\tb.\\tThis Proposal Checklist and Questionnaire, original signed in blue ink\\n\\n\\t\\n\\n\\t\\tc.\\tRedacted version of submission excluding Confidential/Proprietary information \\n\\n\\t\\n\\n\\t\\td.\\tRedlined version of the RFP showing all demurrals\\n\\n\\t\\n\\n\\t\\te.    Benefits exceptions description \\n\\n\\t\\n\\n\\t\\tf.    Benefits brochure\\n\\n\\t\\n\\n\\t\\tg.   All questionnaires and exhibits as required in formats specified in Attachment 2\\n\\n\\t\\n\\n\\t\\ti.    Small Business Participation detail (Exhibit Two)\\n\\n\\t\\n\\n\\t\\n\\n\\t5.      \\tVirginia State Corporation Commission (SCC) registration information. The Offeror: \\n\\n\\tis a corporation or other business entity with the following SCC identification number: ____________ -OR-\\n\\n\\tis not a corporation, limited liability company, limited partnership, registered limited liability partnership, or business trust -OR-\\n\\n\\tis an out-of-state business entity that does not regularly and continuously maintain as part of its ordinary and customary business any employees, agents, offices, facilities, or inventories in Virginia (not counting any employees or agents in Virginia who merely solicit orders that require acceptance outside Virginia before they become contracts, and not counting any incidental presence of the offeror in Virginia that is needed in order to assemble, maintain, and repair goods in accordance with the contracts by which such goods were sold and shipped into Virginia from offeror’s out-of-state location) -OR-\\n\\n\\tis an out-of-state business entity that is including with this proposal an opinion of legal counsel which accurately and completely discloses the undersigned offeror’s current contacts with Virginia and describes why those contacts do not constitute the transaction of business in Virginia within the meaning of § 13.1-757 or other similar provisions in Titles 13.1 or 50 of the Code of Virginia.\\n\\n\\t\\n\\n\\t**NOTE** Check the following box if you have not completed any of the foregoing options but currently have pending before the SCC an application for authority to transact business in the Commonwealth of Virginia and wish to be considered for a waiver to allow you to submit the SCC identification number after the due date for proposals (the Commonwealth reserves the right to determine in its sole discretion whether to allow such waiver): \\n\\nLIST OF ATTACHMENTS\\n\\n\\n\\n\\n\\nBenefit/Program Description\\n\\n\\n\\nTechnical/Cost Questionnaire/Schedules/Claim & Eligibility Data\\n\\n\\n\\nReport Formats\\n\\n\\n\\n\\n\\n\\n\\n\\nAttachment One  \\n\\n\\n\\nBenefit/Program Description\\n\\n\\n\\nhttp://www.dhrm.virginia.gov/hbenefits/employeestoc.html\\n\\n\\n\\nhttp://www.thelocalchoice.virginia.gov\\n\\n\\n\\nhttp://www.dhrm.virginia.gov/healthcoverage/loda-health-benefits\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAttachment Two\\n\\n\\n\\nTechnical/Cost Questionnaire/Schedules/Claim & Eligibility Data \\n\\n\\n\\nFiles containing claims, enrollment data and the Attachment 2 schedules that you will need to prepare and submit a proposal are available in electronic form. To obtain these files, please send an email to Brian Dwyer (brian.dwyer@aon.com) with a copy to Richard Whitfield (richard.whitfield@dhrm.virginia.gov). \\n\\n\\n\\nPlease note that these files are proprietary and available only to vendors of the services requested by this RFP.\\n\\n\\n\\n\\n\\n\\nAttachment Three\\n\\n\\n\\nReport Formats\\n\\n\\n\\n\\n\\nReport Formats\\n\\n\\n\\nThe general form and contents of each contractor submitted report are outlined below. It is not the intent of the Department to require special designed reports if the Contractor has a standard report format that will satisfy the Department’s needs for oversight of the various programs.  However, the Department reserves the right to require a special report design if the standard reports are not satisfactory, in the Department’s opinion. Offeror shall submit a sample report with the final format and  details to be determined during the negotiation process. The primary reports are addressed below, however the Offeror should submit a sample of any requested report, whether identified below or not.\\n\\n\\n\\n\\t1. \\tWeekly Claims Report\\n\\n\\t\\n\\n\\tA.\\tCover Letter – Each report provides Cover Charges broken by the following categories: \\n\\n\\t\\n\\n\\t(a) \\tState Employees, and Early Retirees  \\n\\n\\t(b) \\tTLC Government, \\n\\n\\t(c) \\tTLC Schools, and\\n\\n\\t(d)\\tLODA Participating and Non-Participating Employers \\n\\n\\t(e) \\tTotal column. Each column will reflect any applicable discounts on a separate line and show net charges by category. This will serve as the Contractor’s invoice and the total of the net charges will be the amount due the Contractor.\\n\\n\\t\\t\\n\\n\\tB. \\tAn Excel spread sheet for each category will provide a detail of covered charges broken by type of charge. Examples can be: Inpatient, Hospital, Vision, EAP, Mail order, etc. The spreadsheet shall provide a Plan Year-To-Date column followed by month-to-date column and a separate column for each week of the current month.\\n\\n\\n\\n\\t2. \\tAdministrative Fee Report – Monthly Invoice\\n\\n\\t\\n\\n\\tA. \\tCover Letter – Each report provides Monthly Administrative Fees broken by the following categories: \\n\\n\\t\\n\\n\\t(a) \\tState Employees, and Early Retirees, \\n\\n\\t(b) \\tTLC Government, \\n\\n\\t(c) \\tTLC Schools, and \\n\\n\\t(d) \\tTotal column.  Any pre-approved charges or credits will be shown under each category and added to or subtracted from the categories fees. This will serve as the Contractor’s invoice and the total of the net charges will be the amount due the Contractor.\\n\\n\\t\\n\\n\\n\\tB. \\tSupport Documentation\\n\\n\\t\\n\\n\\t(a) \\tEnrollment summary – For each category in A, the enrollment by plan within that category will be provided along with the applicable fee singularly and in total for all units within the plan.  A Y-T-D column shall also be provided for each enrollment line.  The total for all units within a category shall be the amount invoiced in A above.\\n\\n\\t\\n\\n\\t\\tNOTE: THE DEPARTMENT WILL AUDIT EACH MONTH’S REPORTED ENROLLMENT BY THE CONTRACTOR. A VARIANCE IN EXCESS OF 0.5% (1/2 OF A PERCENT) FROM THE ENROLLMENT SHOWN ON BES MAY RESULT IN A DELAY IN PAYMENT OF THE INVOICE UNTIL THE DISCREPENCY IS RESOLVED. SEE LIQUIDATED DAMAGES  SCHEDULE AS PERTAINS TO ELIGIBILITY FILES NOT PICKED UP TIMELY.\\n\\n\\t\\n\\n\\t(b) \\tPre Approved Charges or Credits – A schedule of any charges or credits will be included by category as provided in A above.  Support documentation for such charges/credits must be provided.\\n\\n\\n\\n\\t3. \\tTLC Monthly Income Report \\n\\n\\t\\n\\n\\t\\tThis report pertains to the ASO contractor for medical/surgical benefits only (See subsection 4.1.6 in this RFP). The report shows the premium income received from each local employer by plan and in total, with an indication of employer groups in default. The report is to be prepared in MS Excel format and E-mailed on the 8th day after the close of the month.\\n\\n\\t\\n\\n\\t4. \\tMonthly Service Report\\n\\n\\t\\n\\n\\t\\tThis report shall be in Excel format and submitted electronically to the Department within 15 days of the end of each month. The report shall be contained one page, if possible, and address all Standards of Performance, Section 3.0, except for the annual premium projections due by September 15th.  The first column on the spreadsheet shall identify the items being reported and have headings with specific detail line items. Examples of headings, if applicable, would be: network, participants, customer service call statistics, claims activities, cycle time, inventory, accuracy rates, COB savings, claims dollars paid (by plan and by enrollee), EAP services, and pharmacy scripts by tier. Additional columns should show standards, if applicable, YTD statistics, and most recent two quarters of activity broken by month. \\n\\n\\t\\n\\n\\t5. \\tMonthly/Quarterly Utilization Management Report\\n\\n\\t\\n\\n\\tThis report shall be in Excel format and submitted electronically to the Department within 15 days of the end of each month/quarter. The report shall be contained one page, if possible. The purpose of this report is to disclose the Contractor’s assessment of its utilization management activities, including admission review, concurrent review and case management.  The first column on the spreadsheet shall identify the items being reported and have headings with specific detail line items. The additional columns should show the activity YTD, the current month, average of the past 3 months, and average of past 12 months.  In addition to the utilization report described above, the Contractor shall submit support reports that allow the Department to monitor utilization by the specific product covered. \\n\\n\\n\\n\\n\\t\\n\\n\\tExamples of, but not limited to, such reports are:\\n\\n\\t\\n\\n\\t(a) \\tMedical and Behavioral Health - Large inpatient claims expected to exceed $100,000 with amount paid to date and expected total \\n\\n\\t(b) \\tPharmacy – Top 10 drugs processed by quantity and dollar volume.\\n\\n\\t(c) \\tEAP – Services requested and provided by type.\\n\\n\\t(d) \\tInterventions provided – Type and quantity for disease and pharmacy management.\\n\\n\\t\\t\\n\\n\\t\\n\\n\\n\\n95\\n\\n\\t\\t\\t\\t\\t\\t \\n\\n\\t\\t\\t\\t\\n\\n68\\n\\n\\n\\n94\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t                 \\n\\n\\t\\t\\t\\t\\n\\n1\\n\\n\\n\\n66')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T17:50:49.829533Z",
     "start_time": "2025-11-17T17:50:49.281045Z"
    }
   },
   "source": [
    "## Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     ))\n",
    "                     )\n",
    "loader.load()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\n\\nExamples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\n\\nComparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\n\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\n\\nCategorization of human memory.\\n\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\n\\nComparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\n\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\n\\nA picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\n\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\n\\nIllustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\n\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\n\\nPseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\n\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T06:30:22.387760Z",
     "start_time": "2025-11-18T06:30:21.403742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://www.google.com/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "                     ))\n",
    "                     )\n",
    "loader.load()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.google.com/'}, page_content='')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T17:50:57.154314Z",
     "start_time": "2025-11-17T17:50:56.419229Z"
    }
   },
   "source": [
    "## Arxiv\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query=\"1706.03762\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "docs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T17:51:08.981337Z",
     "start_time": "2025-11-17T17:51:02.787833Z"
    }
   },
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "docs = WikipediaLoader(query=\"Generative AI\", load_max_docs=2).load()\n",
    "len(docs)\n",
    "print(docs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n=== Generative AI porn \"communities\" ===\\nPopular sites like civitai.com allow users to create, upload, and download fine-tuned versions of open source models of SDXL, Flux, and that are specifically designed for generating various pornographic scenes or effects.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nGenerative AI have extensively been used to produce pornography images and videos of non-consenting individuals. 404 M'), Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, Mistral AI, DeepSeek, Baidu and Yandex.\\nGenerative AI is used across many industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. The production of generative AI systems requires large scale data centers using specialized chips which require a lot of electricity for processing and water for cooling. \\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.  The material and energy intensity of the AI systems has raised concerns about the environmental impact of AI, especially in light of the challenges created by the energy transition.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, Mistral AI, DeepSeek, Baidu and Yandex.\\nGenerative AI is used across many industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. The production of generative AI systems requires large scale data centers using specialized chips which require a lot of electricity for processing and water for cooling. \\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.  The material and energy intensity of the AI systems has raised concerns about the environmental impact of AI, especially in light of the challenges created by the energy transition.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe origins of algorithmically generated media can be traced to the development of the Markov chain, which has been used to model natural language since the early 20th century. Russian mathematician Andrey Markov introduced the concept in 1906, including an analysis of vowel and consonant patterns in Eugeny Onegin. Once trained on a text corpus, a Markov chain can generate probabilistic text.\\nBy the early 1970s, artists began using computers to extend generative techniques beyond Markov models. Harold Cohen developed and exhibited works produced by AARON, a pioneering computer program designed to autonomously create paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nMachine learning uses both discriminative models and generative models to predict data. Beginning in the late 2000s, the introduction of deep learning technology led to improvements in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images, such as DeepDream.\\nIn 2017, the Transformer network enabled adva')]\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T17:51:16.724821Z",
     "start_time": "2025-11-17T17:51:16.708822Z"
    }
   },
   "source": [
    "docs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'Generative AI pornography', 'summary': 'Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.', 'source': 'https://en.wikipedia.org/wiki/Generative_AI_pornography'}, page_content='Generative AI pornography or simply AI pornography is a digitally created pornography produced through generative artificial intelligence (AI) technologies. Unlike traditional pornography, which involves real actors and cameras, this content is synthesized entirely by AI algorithms. These algorithms, including generative adversarial networks (GANs) and text-to-image models, generate lifelike images, videos, or animations from textual descriptions or datasets.\\n\\n\\n== History ==\\nThe use of generative AI in the adult industry began in the late 2010s, initially focusing on AI-generated art, music, and visual content. This trend accelerated in 2022 with Stability AI\\'s release of Stable Diffusion (SD), an open-source text-to-image model that enables users to generate images, including NSFW content, from text prompts using the LAION-Aesthetics subset of the LAION-5B dataset. Despite Stability AI\\'s warnings against sexual imagery, SD\\'s public release led to dedicated communities exploring both artistic and explicit content, sparking ethical debates over open-access AI and its use in adult media. By 2020, AI tools had advanced to generate highly realistic adult content, amplifying calls for regulation.\\n\\n\\n=== AI-generated influencers ===\\nOne application of generative AI technology is the creation of AI-generated influencers on platforms such as OnlyFans and Instagram. These AI personas interact with users in ways that can mimic real human engagement, offering an entirely synthetic but convincing experience. While popular among niche audiences, these virtual influencers have prompted discussions about authenticity, consent, and the blurring line between human and AI-generated content, especially in adult entertainment.\\n\\n\\n=== The growth of AI porn sites ===\\nBy 2023, websites dedicated to AI-generated adult content had gained traction, catering to audiences seeking customizable experiences. These platforms allow users to create or view AI-generated pornography tailored to their preferences. These platforms enable users to create or view AI-generated adult content appealing to different preferences through prompts and tags, customizing body type, facial features, and art styles. Tags further refine the output, creating niche and diverse content. Many sites feature extensive image libraries and continuous content feeds, combining personalization with discovery and enhancing user engagement. AI porn sites, therefore, attract those seeking unique or niche experiences, sparking debates on creativity and the ethical boundaries of AI in adult media.\\n\\n\\n=== Generative AI porn \"communities\" ===\\nPopular sites like civitai.com allow users to create, upload, and download fine-tuned versions of open source models of SDXL, Flux, and that are specifically designed for generating various pornographic scenes or effects.\\n\\n\\n== Ethical concerns and misuse ==\\nThe growth of generative AI pornography has also attracted some cause for criticism. AI technology can be exploited to create non-consensual pornographic material, posing risks similar to those seen with deepfake revenge porn and AI-generated NCII (Non-Consensual Intimate Image). A 2023 analysis found that 98% of deepfake videos online are pornographic, with 99% of the victims being women. Some famous celebrities victims of deepfake include Scarlett Johansson, Taylor Swift, and Maisie Williams.\\nOpenAI is exploring whether NSFW content, such as erotica, can be responsibly generated in age-appropriate contexts while maintaining its ban on deepfakes. This proposal has attracted criticism from child safety campaigners who argue it undermines OpenAI\\'s mission to develop \"safe and beneficial\" AI. Additionally, the Internet Watch Foundation has raised concerns about AI being used to generate sexual abuse content involving children.\\n\\n\\n=== AI-generated non-consensual intimate imagery (AI Undress) ===\\nGenerative AI have extensively been used to produce pornography images and videos of non-consenting individuals. 404 M'),\n",
       " Document(metadata={'title': 'Generative artificial intelligence', 'summary': 'Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, Mistral AI, DeepSeek, Baidu and Yandex.\\nGenerative AI is used across many industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. The production of generative AI systems requires large scale data centers using specialized chips which require a lot of electricity for processing and water for cooling. \\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.  The material and energy intensity of the AI systems has raised concerns about the environmental impact of AI, especially in light of the challenges created by the energy transition.', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (Generative AI, GenAI, or GAI) is a subfield of artificial intelligence that uses generative models to produce text, images, videos, audio, software code or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.\\nGenerative AI tools have become more common since the AI boom in the 2020s. This boom was made possible by improvements in transformer-based deep neural networks, particularly large language models (LLMs). Major tools include chatbots such as ChatGPT, Copilot, Gemini, Claude, Grok, and DeepSeek; text-to-image models such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video models such as Veo and Sora. Technology companies developing generative AI include OpenAI, xAI, Anthropic, Meta AI, Microsoft, Google, Mistral AI, DeepSeek, Baidu and Yandex.\\nGenerative AI is used across many industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. The production of generative AI systems requires large scale data centers using specialized chips which require a lot of electricity for processing and water for cooling. \\nGenerative AI has raised many ethical questions and governance challenges as it can be used for cybercrime, or to deceive or manipulate people through fake news or deepfakes. Even if used ethically, it may lead to mass replacement of human jobs. The tools themselves have been criticized as violating intellectual property laws, since they are trained on copyrighted works.  The material and energy intensity of the AI systems has raised concerns about the environmental impact of AI, especially in light of the challenges created by the energy transition.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nThe origins of algorithmically generated media can be traced to the development of the Markov chain, which has been used to model natural language since the early 20th century. Russian mathematician Andrey Markov introduced the concept in 1906, including an analysis of vowel and consonant patterns in Eugeny Onegin. Once trained on a text corpus, a Markov chain can generate probabilistic text.\\nBy the early 1970s, artists began using computers to extend generative techniques beyond Markov models. Harold Cohen developed and exhibited works produced by AARON, a pioneering computer program designed to autonomously create paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural networks (2014–2019) ===\\n\\nMachine learning uses both discriminative models and generative models to predict data. Beginning in the late 2000s, the introduction of deep learning technology led to improvements in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models due to the difficulty of generative modeling.\\nIn 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images, such as DeepDream.\\nIn 2017, the Transformer network enabled adva')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
